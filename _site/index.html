<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Rodrigo Araújo</title>
	
	<meta name="author" content="Rodrigo Araújo">

	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

	
	<meta name="description" content="I'm a Computer Scientist and Software Engineer, I enjoy Artificial Intelligence, Programming, Software Engineering."/>
	
	
	<meta name="keywords" content="Software, Engineering, Computer Science, Science, Artificial Intelligence, Machine Learning, Programming"/>
	

	<!-- Le styles -->
	<link href="/assets/resources/bootstrap/css/bootstrap.min.css" rel="stylesheet">
	<link href="/assets/resources/font-awesome/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/resources/syntax/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">

	<!-- Le fav and touch icons -->
	<!-- Update these with your own images
	<link rel="shortcut icon" href="images/favicon.ico">
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
	-->

	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
	<link rel="stylesheet" href="/assets/css/styles/hybrid.css">
	<script src="/assets/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/digorithm">
				<i class="fa fa-github"></i>
			</a>
			
			
			<a type="button" class="navbar-toggle nav-link" href="http://twitter.com/digorithm">
				<i class="fa fa-twitter"></i>
			</a>
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:rod.dearaujo@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
			<a class="navbar-brand" href="/">
				<img src="http://www.gravatar.com/avatar/?s=35" class="img-circle" />
				
			</a>
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-3 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/content/images/images/pic.jpg" width="180" height="180" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/">Rodrigo Araújo</a>
    </h3>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
</header>


<div id="bio" class="text-center">
	Stories, Thoughts and Ideas on Computer Science, Mathematics and Technology
</div>

<div class="pages">
	<ul>
		<li><a href="/about">Who I am</a></li>
		<li>
			<div id="workexpand"><a href="/work">The work I do</a>
			<div id="subitem">
			<ul>
				<li>
					<a href="/work/#research">Research</a>
				</li>
				<li>
					<a href="/work/#industry">Industry</a>
				</li>
				<li>
					<a href="/work/#pub">Publications</a>
				</li>
				<li>
					<a href="/work/#talk">Talks</a>
				</li>
				<li>
					<a href="/work/#releases">Releases</a>
				</li>
				<li>
					<a href="/work/#ta">Teaching</a>
				</li>
			</ul>
			</div>
			</div>
			
			
		</li>
		<li><a href="/archives">Archives</a></li>
		<li><a href="/readings">Recommended Readings</a></li>
	</ul>
</div>
<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/digorithm">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="https://twitter.com/digorithm">
				<i class="fa fa-twitter fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:rod.dearaujo@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
	</ul>
	<ul id="contact-list-secondary" class="list-unstyled list-inline">
		
		
		<li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->
<script>

	$(function () {

	var pathname = window.location.pathname;
	console.log(pathname);
	if (pathname == "/work/"){
		$("#subitem").fadeIn();
	} else {
		$("#subitem").fadeOut();
	}
});

$(function() {
  $('a[href*=#]:not([href=#])').click(function() {
    if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'') && location.hostname == this.hostname) {
      var target = $(this.hash);
      target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
      if (target.length) {
        $('html,body').animate({
          scrollTop: target.offset().top
        }, 1000);
        return false;
      }
    }
  });
});
</script>

	</div>

	<div class="col-sm-9 col-sm-offset-3">
		


<article class="home">

  <h2>
    <a href="/general/setup/demo/2015/07/02/Case-Study:-Sentiment-Analysis-on-Movie-Reviews.html">Case Study: Sentiment Analysis On Movie Reviews</a>
  </h2>

  <div class="post-date" style="margin-bottom:15px; margin-top:-15px;">
    
    July
    2nd,
    
    2015
  </div>


  <div>
    
    <p>Well, this case was a fun one, sentiment analysis is a sub type of NLP <em>(natural language processing)</em> in which you have to train a model to analyze a text, and classify it was a negative sentiment or a positive sentiment. In this case, I wanted to apply it on movies reviews, to see if a review is a negative one or a positive one. So, basically, teaching this kind of sentiment to the machine.</p>

<p>There’s a lot of challenges in doing this kind of NLP, experience has told me that when the text is big, it is usually hard to predict things about it… and most of the times, movie reviews are big chunk of text.</p>

<p>That’s the case where I needed to learn and use a few <em>extremely</em> useful things:</p>

<ol>
  <li>Pipeline</li>
  <li>Grid Search</li>
</ol>

<p>These two techniques made total difference in my results. Let’s go through the concept of these two:</p>

<h3 id="pipeline">Pipeline</h3>

<p>In a <a href="http://rodrigoaraujo.me/general/setup/demo/2015/06/30/A-Generic-Architecture-for-Text-Classification.html" target="_blank">previous post</a> I talked about a generic Architecture or Flow to achieve basic text classification tasks, so, Pipeline is a way to automate all those tasks, so, instead of explicitly and separately select features, extract features, train the learning algorithms, you can do all of these things inside the pipeline, and, the amazing scikit learn API gives you an awesome Pipeline interface, pretty similar to a basic estimator’s interface, so you can call methods such as <em>fit, fit_transform, predict</em>, etc…</p>

<p>So you can do even more inside a pipeline, such as new methods for feature extraction/selection, FeatureUnion, use other estimators to select better features for your estimator <em>(do you even neural netception? LOL).</em></p>

<p>Here’s an example of a simple pipeline that does PCA to reduce the dimension of the features and creates a SVC:</p>

<pre><code>    from sklearn.pipeline import Pipeline
    from sklearn.svm import SVC
    from sklearn.decomposition import PCA
    estimators = [('reduce_dim', PCA()), ('svm', SVC())]
    clf = Pipeline(estimators)
</code></pre>

<h3 id="grid-search">Grid Search</h3>

<p>This was the answer for many hours of pure pain tweaking gazillions of parameters to improve the algorithm’s performance. Grid Search is a way to automate parameters changes, so it can run many times with many different parameters… and choose the best for you, magical, right?</p>

<p>And, once again, the Grid Search object has an interface similar to the basic estimator’s interface, so you can call all the same methods. But, when you create the object grid search, you gotta pass a estimator as parameter… and here’s when things get interesting:</p>

<p>You can pass a pipeline to the grid search. <strong>so much win!</strong></p>

<div class="imgcenter">
	<img src="http://cdn.meme.am/instances2/500x/581044.jpg" />
</div>

<p> </p>

<p>But, the truth is: this operation is <em>extremely</em> expensive, and it’s recommended to use it only on the few first tries, where you don’t know exactly the best parameters values for the task.</p>

<p>Another interesting thing is, after the model inside the grid search is trained, methods like <em>predict</em> are computed using the best features. If we look inside the SKlearn’s grid search’s code, we can see how easily and elegant it’s done:</p>

<pre>
<code class="python hljs">
[...]
scores = list()
grid_scores = list()
for grid_start in range(0, n_fits, n_folds):
    n_test_samples = 0
    score = 0
    all_scores = []
    for this_score, this_n_test_samples, _, parameters in \
            out[grid_start:grid_start + n_folds]:
        all_scores.append(this_score)
        if self.iid:
            this_score *= this_n_test_samples
            n_test_samples += this_n_test_samples
        score += this_score
    if self.iid:
        score /= float(n_test_samples)
    else:
        score /= float(n_folds)
    scores.append((score, parameters))
    # TODO: shall we also store the test_fold_sizes?
    grid_scores.append(_CVScoreTuple(
        parameters,
        score,
        np.array(all_scores)))
# Store the computed scores
self.grid_scores_ = grid_scores

# Find the best parameters by comparing on the mean validation score:
# note that `sorted` is deterministic in the way it breaks ties
best = sorted(grid_scores, key=lambda x: x.mean_validation_score,
              reverse=True)[0]
self.best_params_ = best.parameters
self.best_score_ = best.mean_validation_score
</code>
</pre>

<p>As you can see, in the end it keeps the best parameters and scores, so it can be used when it needs to predict or output its <em>predict_proba</em> or <em>decision_function</em></p>

<h3 id="correctly-mixing-pipeline-grid-search-and-cross-validation-correctly-the-hidden-wizardry">Correctly mixing Pipeline, Grid Search and Cross Validation correctly: the hidden wizardry</h3>

<p>It’s tricky to mix all these things up, the most dangerous mistake that everybody does sometime is: <strong>using the same dataset to the grid search AND the cross validation</strong></p>

<p>So, here’s a nice technique to avoid this: Split the initial dataset into Development Dataset, which will be used to train the algorithm and to execute the grid search, and the Validation Dataset, which will be passed to the cross_val_score and internally splitted again to avoid bias (CV parameter can configure this).</p>

<p>So, in the end, you train the grid search with a subset of the dataset, and validate it with another subset, avoiding many kind of undesired effects.</p>

<h3 id="the-movie-review-problem">The Movie Review Problem</h3>

<p>Ok, so now let’s head to the real problem. The dataset can be downloaded with:</p>

<pre><code>wget http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz
tar xzf review_polarity.tar.gz
</code></pre>

<p>What I did to solve it was: I used grid search to search a few parameters, such as <em>TfidfVectorizer’s max_features</em>, that in the end I kept with max_features=10000, also its ngram_range, searching between (1,1) and (1,2) and <em>the LinearSVC()’s C parameter</em>.</p>

<p>I used a few other algorithms, but I found the best performance with LinearSVC, though I did not try all the other possibilities <em>(I could create many pipelines to test many algorithms, but my computer would be unusable during many hours, probably, which I couldn’t afford)</em>.</p>

<p>Why Linear SVC?</p>

<p>The Linear SVC is very similar to the SVC object but using the <em>kernel = “linear”</em>. LinearSVC is a type of <em>Support Vector Machines</em>, it’s known that SVMs are effective in <strong>high dimensional spaces</strong>, which is our case here. Also, it can give weights to classes, helping to go through unbalanced datasets
<img src="http://scikit-learn.org/stable/_images/plot_separating_hyperplane_unbalanced_0011.png" alt="" /></p>

<p>And, basically, what the SVMs does is create a hyper-plane (or a set of it) in a high or even infinite dimensional space. And it solves the primal problem:</p>

<div class="imgcenter">
<img src="http://scikit-learn.org/stable/_images/math/396704acdf11cc18d2d02b32275c0ee42d76b95e.png" />
</div>
<p> </p>

<p>Where Xi are the training vectors and Y is a vector in {1,-1}^n.</p>

<p>So, here’s the code where I built the pipeline, the gridsearch, use a subset of the dataset to develop the grid search and another subset to validate using the <em>cross_val_scores</em>, then, I call <em>predict</em> to get the vector of predictions to build a report <em>(or, with this, I can start to build a ROC curve, confusion matrix and other statistics debug tools)</em></p>

<pre><code>from sklearn.datasets import load_files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.grid_search import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC
from sklearn.cross_validation import train_test_split
from sklearn.cross_validation import cross_val_score
from sklearn.metrics import classification_report

data = load_files('../txt_sentoken/')

X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.5, random_state=0)

pipeline = make_pipeline(TfidfVectorizer(sublinear_tf=True, max_features=10000), LinearSVC())

parameters = {
        'tfidfvectorizer__ngram_range': [(1,1), (1,2)],
        'linearsvc__C':(.01,.1,1),
        }

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1) 

grid_search.fit(X_train, y_train)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters iset:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print("\t%s: %r" % (param_name, best_parameters[param_name]))

print "The model was trained on the full development set"
print "the scores are going to be computed with the evaluation set"
scores = cross_val_score(grid_search, X_test, y_test, cv=5)

print scores.mean(), scores.std()

y_pred = grid_search.predict(X_test)

print classification_report(y_test, y_pred)
</code></pre>

<p>and the output:</p>

<pre><code>Best score: 0.849

Best parameters iset:
	linearsvc__C: 1
	tfidfvectorizer__ngram_range: (1, 2)

The model was trained on the full development set
the scores are going to be computed with the evaluation set
0.865933623341 0.0281523948704

Classification report:
             precision    recall  f1-score   support

          0       0.88      0.86      0.87       496
          1       0.87      0.88      0.87       504

avg / total       0.87      0.87      0.87      1000
</code></pre>

<p>So, around 86%~87% of precision/f1-score, not <em>that</em> bad.</p>

<h3 id="further-improvements">Further Improvements</h3>

<p>There are many things I could to do elevate this performance, but due the lack of time to play with this dataset, those improvements will be in a next post.</p>

<p>Possible improvements could be: find and extracting new features, to do this, we gotta understand better this data, extract more information about it. Trying new algorithms and techniques such as Ensemble, Classifiers Combination or Fusion. Stemming and Stop Words could improve it too. If you solved this problem and got a better result, share it with me, I’m very curious about how to improve this!</p>

    
  </div>

</article>
<hr/>


<article class="home">

  <h2>
    <a href="/general/setup/demo/2015/06/30/A-Generic-Architecture-for-Text-Classification.html">A Generic Architecture for Text Classification with Machine Learning</a>
  </h2>

  <div class="post-date" style="margin-bottom:15px; margin-top:-15px;">
    
    June
    30th,
    
    2015
  </div>


  <div>
    
    <p>One of the most commons tasks in Machine Learning is text classification, which is simply teaching your machine how to read and interpret a text and predict what kind of text it is. </p>

<p>The purpose of this essay is to talk about a simple and generic enough Architecture to a supervised learning text classification. The interesting point of this Architecture is that you can use it as a basic/initial model for many classifications tasks. </p>

<h2 id="supervised-learning">Supervised Learning</h2>

<p>If you’re already familiar with this concept, just jump this step, but I feel it’s important to beginners to know. </p>

<p>Supervised Learning is when you have to first train your model with already existing labeled dataset, just like teaching a kid how to differentiate between a car and a motorcycle, you have to expose its differences, similarities and such. Whereas unsupervised learning is about learning and predicting without a pre-labeled dataset.</p>

<h2 id="starting-to-sketch-the-architecture">Starting to sketch the Architecture</h2>

<p>With the dataset in hands, we start to think about how is going to be our architecture to achieve the given goal, we can resume the steps in:</p>

<ol>
  <li>Cleaning the dataset</li>
  <li>Partitioning the dataset</li>
  <li>Feature Engineering</li>
  <li>Chosing the right Algorithms, Mathematical Models and Methods</li>
  <li>Wrapping everything up</li>
</ol>

<h2 id="cleaning-the-dataset">Cleaning the dataset</h2>

<p>Cleaning the dataset is a crucial initial step in Machine Learning, many Toy Datasets don’t need to be cleaned, because it’s already clean, peer-reviewed and published in a way you can use it exactly to work on the learning algorithms.</p>

<p>The problem is: </p>

<h4 id="the-real-world-is-full-of-painful-and-noisy-datasets">The real world is full of painful and noisy datasets</h4>

<p>If there’s one thing I learned while working with Machine Learning is, there’s no such thing as shiny and perfect dataset in the real world, so we have to deal with this beforehand. Situations where there are many empty fields, wrong and non-homogeneous formats, broken characters, is very common. I won’t talk about such techniques now, but I will write something about it in another post.</p>

<h2 id="partitioning-the-dataset">Partitioning the Dataset</h2>

<p>We always need to partition the dataset in, at least, 2 partitions: the training dataset and the test/validation dataset. Why?</p>

<p>Suppose we fed the learning algorithm with a training data X and it already known the output Y (because it’s a training data pair (X,Y)), which is, for given text X, Y is its classification, the algorithm will learn it. </p>

<p>Great, the algorithm learned this. But now, we’re going to validate the learning, so we use the same data X, I mean, we pass X to the model and ask what’s its classification… </p>

<p>Do you see the problem here?</p>

<p>Of course the algorithm will output Y, the same Y we passed to its training. So, if we pass the complete dataset D in the training phase, then we validate the model using the SAME D dataset, we will be steping onto this very same situation. It’s like cheating, it’s like we point to a car and say “this is a car”, then, at the same time and with the same car, we ask to the kid “is this a car?”, it will be highly probable that the kid will answer correctly, though it may not learned correctly. What we must do is, point to a car and say “this is a car”, and then, point to a different car and ask “is this a car?”.</p>

<p>Stepping out of the metaphor, we must check if the machine learned correctly by using a diferent portion of the dataset.</p>

<p>So, at the end of this step, we’ll have the training dataset and the test dataset, both are subset of the same initial dataset. </p>

<p>With Python’s Scikit Learn you can do this easily using <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html" target="_blank">Train Test Split</a> <em>(read the docs, it’s very simple to use it.)</em>.</p>

<p> </p>

<div class="imgcenter">
<img src="/content/images/images/ml1.png" />
</div>
<p> </p>

<h2 id="feature-engineering">Feature Engineering</h2>

<p>This is one of the most important steps when doing Machine Learning. Briefly, Features are the data that the learning algorithm will use as the “X”, which will be used to compute and understand the patterns. A simpler example would be a non-text classification or a regression, e.g: house pricing predictions, where our Features could be: number of rooms, size in squared meters, location and more. As you can see, these feature will describe the patterns of each data point and will affect how the house will be priced. </p>

<p>The crucial point is, features can vary, you can identify new features that can level up your model’s predictions in huge proportions, a simple example would be, in a text classification you count the frequency of each word, this is one feature… you feed your learning algorithm with it… and the result isn’t enough: 50% of precision. But then you see that, in this case, the length of the whole data point (the text) plays a huge role determining a pattern for each class. Now that you <strong>identified</strong> this feature, you do something to <strong>extract</strong> that feature and then add this new feature and feed your learning algorithm with it, then, the result: 95% of precision. </p>

<p>Ok, so you know the importance of the feature engineering phase, now it’s time to understand the most common technique to extract feature in texts: <strong>TF-IDF</strong></p>

<p> </p>

<h4 id="tf-idf">TF-IDF</h4>

<p>Though the most intuitive way to look for patterns in texts is to count each word in the text (and use it as a Feature), it may not be the best way to do it. A few reasons for it is, larger texts will have higher averages than the shorter texts, these discrepancies can hurt the learning algorithm, and this Feature <strong>doesn’t say much about the importance of the words</strong>, which is very important to find patterns.</p>

<p>So, instead of computing the occurrence, it’s better to compute the importance of the words. to accomplish this we can use 2 statistic’s techniques:</p>

<p><strong>TF</strong> - which is basically the raw frequency of the word given the document, raw frequency of t by f(t,d), then the simple tf scheme is tf(t,d) = f(t,d).</p>

<p><strong>IDF</strong> - Inverse Document Frequency, which is a technique to give emphasis to words that <em>don’t appear with high frequency</em>, because these are the words that can differentiate the texts, which means, in this case, these are the most important words, so we inverse its frequencies. So, an example, if the word <em>“the”</em> happens to appear very often in a text, it will weight <strong>less</strong>, because it’s a common word, thus, don’t cause very impact when fiding patterns to differentiate the texts.</p>

<p>So the whole TF-IDF can be computed by</p>

<p><img src="https://upload.wikimedia.org/math/e/8/1/e81492e44713270fd230d821ccebd100.png" alt="" /></p>

<p>Scikit Learn gives us a great API to use the TF-IDF method, it’s really simple. </p>

<pre><code>    from sklearn.feature_extraction.text import TfidfVectorizer

    vectorizer = TfidfVectorizer()
    vectorized_x = vectorizer.fit_transform(X)
</code></pre>

<p>Which will do the TF-IDF on the X data, and then vectorize this data, in other words, transform the whole thing into an array of inverse frequencies.
So, extracting those features, <strong>this</strong> will be our training and test data, that will feed the algorithm (along with the Y, which is the output, the class of each training data).</p>

<p>Re-thinking our Architecture, now we have:</p>

<p> </p>

<div class="imgcenter">
<img src="/content/images/images/ml2.svg" />
</div>
<p> </p>

<p>Remembering that X is the set of features (e.g: vector with the TF-IDF of each data point) and Y is the output, which is, the labels/class (e.g: Spam or not-spam).</p>

<h2 id="chosing-the-right-algorithms-mathematical-models-and-methods">Chosing the right Algorithms, Mathematical Models and Methods</h2>

<p>With the data prepared, features selected and extracted, it’s time to feed the algorithm with this data, this topic <em>per se</em> could go pages and pages, as learning algorithms is such a huge fields, with many publications and ideas to solve every kind of problem.</p>

<p>To not extend it very much, and as the purpose of this essay is to discuss the architecture, I’ll use a few commons algorithms, such as Logistic Regression, Decision Trees, SVM and Neural Networks.</p>

<p>So, at this point, we’ll treat the learning algorithm as a black-box algorithm, where it:</p>

<ol>
  <li>receive a training data X which is a vector of features, and training data Y, which is the label/class/output (we can binarize it, such as spam=1, ham=0)</li>
  <li>return a model, where given an text X, can output its predicted class Y.</li>
</ol>

<p>After generating this model, we will test it with our test dataset and check its performance, if it can predict correctly <em>(the test dataset has output values (Y) so we can check them)</em>, it is ready to predict new data <em>(data completely outside our initial dataset)</em>, so we say that the machine learned the task. </p>

<p>Our architecure now:</p>

<p> </p>

<div class="imgcenter">
<img src="/content/images/images/ml3.svg" height="800" />
</div>
<p> </p>

<h2 id="wrapping-everything-up">Wrapping everything up</h2>

<p>With this architecture, we should be able to do most of the simple text classification tasks, as the main flow is: get data, clean data, identify and extract features, train your algorithm/mathematical model of choice, validate it and then, use the generated model to do the estimations.</p>

<p>Of course there are many improvements that can be made on this architecture and many, many, many lower level details, but you can see this architecture as a <em>“boilerplate code”</em> to get you started with the machine learning engineering task.</p>

<p>A few tips:</p>

<p>Learn the underlying mathematical models of the most commons learning algorithms, this will teach you the trade offs of each one, so you can apply the correct algorithms to the given dataset and scenario. For example, SVM can be good for unbalanced dataset, but why? You gotta know this. Neural Networks can be slow to be trained, but, if training time is not critical, it’s okay to use Neural Networks.</p>

<p>Master the skills to clean data, if using Python, learn to use Pandas. This will be an extremely important skill.</p>

<p>Master the skills to understand data, this will be crucial to make you see what algorithm to use, you can’t make something learn if you don’t know about what you are teaching.</p>


    
  </div>

</article>
<hr/>


<article class="home">

  <h2>
    <a href="/general/setup/demo/2015/06/19/Case-Study:-Python-Performance-on-rotating-one-dimensional-vector.html">Case Study: Python Performance on rotating one-dimensional vectors</a>
  </h2>

  <div class="post-date" style="margin-bottom:15px; margin-top:-15px;">
    
    June
    19th,
    
    2015
  </div>


  <div>
    
    <p>I’m a big fan of a nice challenge, therefore, I like books like Programming Pearls, I like to dive into many kinds of solutions to the same problem and try to differentiate them by novelty, performance, elegance, etc… </p>

<p>This time I was playing with a fun problem, from the column 2:</p>

<blockquote>
  <p>“rotate a one dimensional array of <em>N</em> elements left by <em>I</em> positions”. </p>
</blockquote>

<p>The author says it should consume little space and time, so, there are many solutions, obviously. The fun thing is that I was doing it in Python, so you can solve it in many ways, but you can decrease the performance a lot if you choose poorly. In C we could just swap pointers in a doubly linked list, which is what happens in the real implementation of CPython.</p>

<p>The idea that the Author exposed is pretty clever, which is based in reversing the vector only 3 times:</p>

<ol>
  <li>Reverse the vector, from the first position to the <em>I</em>-th position, where <em>I</em> is the number of position that is wanted to move to the left</li>
  <li>Reverse the vector, from <em>I</em>-th + 1 position to <em>N</em>, where <em>N</em> is the size of this vector</li>
  <li>Reverse the whole vector, again. </li>
</ol>

<p>In other words, we have the vector <em>AB</em> (Where <em>A</em> is the first part, <em>B</em> is the second), we reverse <em>A</em> so we have <em>Ar</em>, reverse <em>B</em> so we have <em>Br</em>, now we have <em>ArBr</em>, then we reverse the whole thing <em>(ArBr)r</em>, after that, we have the rotated vector. It’s quite beautiful. A picture can help the visualization:</p>

<div class="imgcenter">
<img src="/content/images/images/img1.jpeg" height="450" />
</div>

<p>here’s an example with a simple vector:</p>

<div class="imgcenter">
<img src="/content/images/images/img2.jpeg" height="350" />
</div>

<p>Now, searching for other approaches, I looked inside the code from CPython, and there’s a nice comment: </p>

<pre><code> "Conceptually, a rotate by one is equivalent to a pop on one side and an append on the other"
</code></pre>

<p>Quite elegant solution as well. So, an example:</p>

<div class="imgcenter">
<img src="/content/images/images/img4.jpeg" height="350" />
</div>

<p>And the last example I found while searching through Pythonic ways to solve this problem (Even though may not be very fast)</p>

<pre>
<code class="python hljs">
def rotate_pythonic_way(arr, i):
         return arr[i:] + arr[:i] 
</code>
</pre>
<p>Which is super simple and elegant, what it’s doing is the following:</p>

<div class="imgcenter">
<img src="/content/images/images/img3.jpeg" height="350" />
</div>

<p>At first I thought this solution would be the slowest, but we’ll get to that.</p>

<p>One thing that took my attention was the internal method to reverse a list that Python offers, there are two of them, one returns a reversed iterator and you can cast it into a list and the other reverse the exactly same list that is passed as parameter, it shouldn’t take you long to realize which is faster. So I wrote a code with the many solutions to it and I profiled the functions, which gave us a interesting result when ran over a vector with 10 millions integers.</p>

<p>First, the original idea (<em>reverse the vector 3 times</em>), but using the reversed(array) method:</p>

<pre>
<code class="python hljs">
def rotate_original_solution_with_reversed(arr, i):
    n = len(arr)
    array_A_reversed = list(reversed(arr[0:i]))
    array_B_reversed = list(reversed(arr[i:n]))
    ArBr = array_A_reversed + array_B_reversed
    rotated_array = list(reversed(ArBr))
    return rotated_array
</code>
</pre>

<p>and the result:</p>

<pre><code>    Original Idea using reversed(arr): 

    4 function calls in 0.611 seconds

    Ordered by: standard name

    ncalls tottime percall cumtime percall filename:lineno(function)
    1 0.440 0.440 0.440 0.440 2_1b.py:21(rotate_original_solution_with_reversed)
    1 0.171 0.171 0.611 0.611 string:1 module
    1 0.000 0.000 0.000 0.000 {len}
</code></pre>

<h4 id="damn-06s-thats-too-slow">Damn, 0.6s, that’s too slow!</h4>

<p>After this one, I changed the way I was reversing the vector, using the other internal method from Python:</p>

<pre>
<code class="python hljs">
def rotate_original_solution_with_reverse(arr, i):
    n = len(arr)
    
    array_A = arr[0:i]
    array_B = arr[i:n]
    
    array_A.reverse()
    array_B.reverse()

    ArBr = array_A + array_B
    ArBr.reverse()
    return ArBr
</code>
</pre>

<p>And the result:</p>

<pre><code>    Original Idea using arr.reversed(): 
    7 function calls in 0.322 seconds

    Ordered by: standard name

    ncalls tottime percall cumtime percall filename:lineno(function)
    1 0.187 0.187 0.210 0.210 2_1b.py:30(rotate_original_solution_with_reverse)
    1 0.113 0.113 0.322 0.322 string:1 module
    1 0.000 0.000 0.000 0.000 {len}
    1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}
    3 0.023 0.008 0.023 0.008 {method 'reverse' of 'list' objects}
</code></pre>

<h4 id="from-06s-to-03s-thats-a-great-improvement-lesson-choose-your-built-insdata-structuresalgorithms-carefully">From 0.6s to 0.3s, that’s a great improvement. lesson: choose your built-ins/Data Structures/Algorithms carefully.</h4>

<p>And now, using the Pythonic Way, which I was thinking that would be the slowest:</p>

<p>And the result:</p>

<pre>
<code class="bash hljs">
Pythonic Way: 

3 function calls in 0.295 seconds

Ordered by: standard name

ncalls tottime percall cumtime percall filename:lineno(function)
1 0.238 0.238 0.238 0.238 2_1b.py:45(rotate_pythonic_way)
1 0.056 0.056 0.295 0.295 string:1(module)
1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}

</code>
</pre>

<p>It was faster than the other methods! Well played, Python. </p>

<p>And the last one, using the pop/append technique, repeated i times:</p>

<p>And the result:</p>
<pre>
<code class="bash hljs">

Pop and append way: 

5 function calls in 0.013 seconds

Ordered by: standard name

ncalls tottime percall cumtime percall filename:lineno(function)
1 0.000 0.000 0.013 0.013 2_1b.py:49(rotate_pop_append)
1 0.000 0.000 0.013 0.013 string:1(module)
1 0.000 0.000 0.000 0.000 {method 'append' of 'list' objects}
1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}
1 0.013 0.013 0.013 0.013 {method 'pop' of 'list' objects}
</code>
</pre>

<p>Ok. That was weird. 0.013 seconds?  13 milliseconds? Crazy, right? I thought the fact of repeating it many times would make it go way slower, but I guess I was wrong!</p>

<p>So, as you can see, an interesting problem can have many solutions, one more elegant, others faster. In the end you may pick the simplest and most intuitive solution, this may be the best for the situation. Some times you gotta go with the strangest and most non-intuitive solution (which reminds me the first time I saw the Quick Sort and all its non-intuitive way to think)</p>

<p>So, if you’re willing to reverse a list with Python, go with List.reverse() method (unless you want the iterators to do something), and if you want to rotate a vector, go with pop/append, it looks faster.</p>

    
  </div>

</article>
<hr/>


<article class="home">

  <h2>
    <a href="/general/setup/demo/2015/05/25/Web-Scaling:-Using-Redis-as-Cache.html">Web Scaling - using Redis as Cache</a>
  </h2>

  <div class="post-date" style="margin-bottom:15px; margin-top:-15px;">
    
    May
    25th,
    
    2015
  </div>


  <div>
    
    <p><strong>Redis is such a great technology.</strong> Unfortunately, there’s still people who don’t know Redis or don’t know that Redis can be used as a Cache System to improve the speed of responses.</p>

<h2 id="why-redis">Why Redis</h2>

<p>Well, let’s start this discussion remembering how a common Relational Database basically works: Suppose we’re using a MySQL, every time your app sends a request to the MySQL client, the MySQL client gotta make a trip to the hard drive to get the data asked in the request, this can become a problem if the data asked in request is big… and if there are many requests at the same time, this can generate a huge latency, annoying users or worse.</p>

<p>This is where Redis comes into play, Redis is a key-value database that will be running and storing data inside your memory, if you remember the basic of computers architecture:</p>

<p><img src="/content/images/2015/06/memchart.jpg" alt="" /></p>

<p>It’s way faster to access data in memory (Physical RAM, main memory) than to access data in the Hard Drive, so it’s easy to notice that if the data that the application wants to access is inside the main memory, it’s way easier to reach to that data than if it was stored in the Hard Drive.</p>

<p>So, like I said, Redis will be storing its data inside the memory, but you may ask yourself: “but what if I turn off the machine?? isn’t the ram memory volatile? “ Yes, that’s why Redis will be flushing the data to the hard drive from time to time, it’s up to you to choose this time between flushes, it’s all about Performance vs. Security.</p>

<p>So, What we’ll be doing is just:</p>

<p><img src="/content/images/2015/06/atv1.png" width="350" height="400" /></p>

<p><em>note that this print is taken from a talk I gave in my country, so it’s in portuguese. aplicação = application, Não acha key = key don’t found, retorna dados = return data</em></p>

<p><strong>As you can see: So much win. We avoided redundant trips to the disk.</strong></p>

<p>Now that we understand the concept of what we’ll be doing, the code becomes very easy to implement, here’s a simple idea <em>(thougt it can be improved and extended in many ways, but it can demonstrate the idea we’re working here)</em></p>

<pre>
<code class="python hljs">
    from flask import Flask
    from flask.ext.sqlalchemy import SQLAlchemy
    from sqlalchemy import create_engine
    import redis

    app = Flask(__name__)
    url = 'mysql+pymysql://username:password@ip/dbname'
    app.config['SQLALCHEMY_DATABASE_URI'] = url
    db = SQLAlchemy(app)
    cache = redis.StrictRedis(host='localhost', port=6379, db=0)

    #improvement: model could stay in a different file
    class User(db.Model):
        id = db.Column(db.Integer, primary_key=True)
        username = db.Column(db.String(80))
        email = db.Column(db.String(120))

        def __init__(self, username, email):
            self.username = username
            self.email = email

        def __repr__(self):
            return '' % self.username

    def createUsers():
            for x in xrange(0,100000):
                user = User('test', 'test')
                db.session.add(user)
            db.session.commit()

    def getUsers():
            users = cache.get('users')
            if not users:
                users = User.query.all()
                cache.set('users', users)

    # improvement: views/routing should stay in a different file
    @app.route('/')
    def hello_world():
        getUsers()
        return 'Hello Worldd!'

    if __name__ == '__main__':
        app.debug=True
        app.run(host='0.0.0.0')
</code>
</pre>

<p>So, after you create the correct database, populate the database (there’s a function in the code for that) and change username/password/dbname in the code, we’ll run the app.py and go to localhost:port-you-exposed.</p>

<h2 id="what-will-happen"><em>What will happen?</em></h2>

<p>1. The first time you access it, it will take a few seconds to get the data from the MySQL</p>

<p>2. The second time you access it, it will take just a few milliseconds to get the same data from redis</p>

<p>In my computer the result was:</p>

<p>first access: 45861 milliseconds</p>

<p><strong>second access: 5ms</strong></p>

<p><strong>I know, right? That’s just blazing fast!</strong></p>

<p>And it can save a lot of computational resources and human time. Now, with this logic applied to one method (the method to get users), we can apply it to whichever method we want, or we can even create a generic decorator and annotate the methods that we want do the caching!!</p>

    
  </div>

</article>
<hr/>


<article class="home">

  <h2>
    <a href="/general/setup/demo/2015/04/22/Build-your-first-Rest-API-with-Python.html">Building your first REST API with Python</a>
  </h2>

  <div class="post-date" style="margin-bottom:15px; margin-top:-15px;">
    
    April
    22nd,
    
    2015
  </div>


  <div>
    
    <p>Are you total lost in this world full of jargons like: API, Rest API, microservices and stuffs? Come here, sit, grab a cup of coffee, and let’s talk briefly about it.</p>

<p>Today a lot is said about APIs. Everything has an API, every programmer (newbie to expert) uses tons of API. Also, today we can see a lot of people talking about microservices and the idea of total separation of backend, frontend, web services or <em>whatever</em>. So we can <em>(and we do)</em> hear a lot about REST/RESTful APIs. We have 3 current problems with it:</p>

<p>1. Many new programmers don’t have a single clue of what a Rest API is.</p>

<p>2. It’s probable that they’re using Rest APIs and don’t know about it.</p>

<p>3. When they feel that they should learn more about it, there’s hardly any good and accessible material to learn it.</p>

<h3 id="so-heres-a-extremely-simplistic-approach-to-try-to-explain-what-is-a-rest-api">So here’s a extremely simplistic approach to try to explain what is a Rest API</h3>

<p>Starting with the concept of API, which stands for <strong>A</strong>pplication <strong>P</strong>rogramming <strong>I</strong>nterface, it’s just an interface which you, <em>dear programmer</em>, will be dealing with to extract whatever you <em>(or your program)</em> want.</p>

<p>Suppose your program needs to create a connection with a given database, normally you do:</p>

<p>1. You import the <em>library</em> that will abstract the connection with the database</p>

<p>2. You create an object to represent a Connection with the database</p>

<p>3. You call functions that this object provide to you, so you can do whatever you want (and whatever they provide)</p>

<p>Here’s a silly example:</p>

<pre><code>from database_library import Connection

connection = Connection()
connection.OpenConnection()
… do whatever you want
connection.CloseConnection()
</code></pre>

<p>When you called those function, you were dealing with the interface that the object provided to you, sure the object may be doing thousand of things at the moment you call its functions, but, it doesn’t matter to you, does it? You just want the <em>damn</em> connection open and then closed.</p>

<p>So the library that you imported is giving you something you want, offering a service or a resource. That’s the sole purpose of an API. It’s a layer that you can use to get things from other(s) component(s).</p>

<p>So, it’s easy to deduct that, the better the engineer planned the API, the easier it will be to deal with and extract what you want, and the contrary is true.</p>

<p>But still, you’re processing the core of this API in your own machine, which isn’t that great, here’s when the API evolves to whole web hosted services, so you can request this API something, and this API can give you something, through the WEB, via HTTP request. And that’s amazing.</p>

<p>And the community, recently, decided that the request and response that happen between applications and APIs, should be done with JSON, so this communication can become uniform and APIs can talk to other APIs effortlessly.</p>

<p>So, basically, what’s going on is:</p>

<p><img src="/content/images/2015/06/json-rest3.png" alt="" /></p>

<p>So… yes, you make your function calls, now, with just a simple URL + HTTP methods, wanna see a real example? We can just send a request to the Facebook’s API by accessing this URL: <a href="http://graph.facebook.com/contatodigo">http://graph.facebook.com/contatodigo</a></p>

<p>Which will request my profile’s data, and the Facebook’s API will return:</p>

<pre><code>{
   "id": "100001638888259",
   "first_name": "Rodrigo",
   "gender": "male",
   "last_name": "Ara\u00fajo",
   "link": "https://www.facebook.com/contatodigo",
   "locale": "pt_BR",
   "name": "Rodrigo Ara\u00fajo",
   "username": "contatodigo"
}
</code></pre>

<p>As simple as that.</p>

<p>Now with this basic idea explained, we can do a simple Rest API using Python and Flask. (I’m assuming you’re already familiar with both technologies).</p>

<p>All we’re gonna do is, using the flask routing, create routes to the users so they can interact with the resources of our API. Let’s suppose our API will serve and receive only Books. So this is the resource we’re dealing with, users may use our API to get Books and insert new Books, so our only URIs will be:</p>

<p>1. <em>/bookapi/v1.0/books</em> with a GET method, which will just return the list of books</p>

<p>2. <em>/bookapi/v1.0/books</em> with a POST method, which will insert a book</p>

<p>So after the API is built, you or you program can get books or insert books by sending HTTP request to the API’s URIs, simple as that. I’ll be very straight forward, here’s the code:</p>

<pre><code>#!flask/bin/python
from flask import Flask, jsonify, request

app = Flask(__name__)

books = [
    {
        'id': 1,
        'title': u'Game of Thrones',
        'description': u'Cool dragons', 
        'finished': False
    },
    {
        'id': 2,
        'title': u'50 shadows of grey',
        'description': u'It start as bullshit, end as a huge bullshit', 
        'finished': True
    }
]

@app.route('/bookapi/v1.0/books/', methods=['GET'])
def get_books():
    return jsonify({'books': books})

@app.route('/bookapi/v1.0/books', methods=['POST'])
def create_book():
    if not request.json or not 'title' in request.json:
        abort(404)

    book = {
            'id': books[-1]['id'] + 1,
            'title': request.json['title'],
            'description': request.json.get('description', ""),
            'finished': False
            }

    books.append(book)
    return jsonify({'book': book}), 201 

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>

<p>Note that we’re creating a simple in-memory <em>database</em>, which is a simple python’s dict. This could be a database. But for the sake of simplicity, I’m using just a dict.</p>

<p>When the requests come, it verifies the route that the user is asking for, which is: what resources is he/she wanting? and then, the code do whatever it must do (<em>remember the API idea of hiding the complexity, the user requesting just want the result</em>), and then it put everything in a JSON and returns it. That simple!</p>

<p>Of course, many improvements and extensions (<em>there are infinity possibilities</em>) can be made to this code, but, here’s a skeleton of the idea of an API, it’s just a start. One good practice it’s to <em>not</em> return the ID of the resource, but return just its URI, which is surely a great idea. Another good practice is to ask authentication in every HTTP request, so your API won’t be exposed to everybody. There are, indeed, endless improvements, but in this code, you, that are completely beginner to the API’s concept, can now understand what’s going on underneath and start planning and building your own API using Flask.</p>

    
  </div>

</article>
<hr/>

<hr/>

<ul class="pager"> 

  
  <li class="previous disabled">
    <a>&larr; Newer</a>
  </li>
  
  
  <li>
    <span class="page_number">Page: 1 of 3</span>
  </li>

  
  <li class="next">
    <a href="/page2">Older &rarr;</a>
  </li>
  

</ul>




		<footer>
			<hr/>
			<p>
				&copy; 2015 Rodrigo Araújo with Jekyll.</a>
			</p>
		</footer>
	</div>

	<script type="text/javascript" src="/assets/resources/jquery/jquery.min.js"></script>
	<script type="text/javascript" src="/assets/resources/bootstrap/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>

	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">  
MathJax.Hub.Config({  
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});


</script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-38327934-3', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>

