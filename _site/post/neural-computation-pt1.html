<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Neural Computation: Toward an Intuitive Understanding of the Perceptron</title>
  <meta name="description" content="Artificial Neurons is one of the most beautiful ways to simulate a biological behavior through computation, despite the fact that it’s not very close to the ...">

  <!-- CSS files -->
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/main.css">

  <link rel="canonical" href="/post/neural-computation-pt1">
  <link rel="alternate" type="application/rss+xml" title="Rodrigo Araujo" href="/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="/favicon.png">
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="sidebar cover-card table-cell table-middle">
  <a href="/" class="author_name">Rodrigo Araujo</a>
  <span class="author_job">Computer scientist and SW Engineer</span>
  <span class="author_bio mbm">Thoughts and stories on computer science, software engineering and mathematics</span>
  <nav class="nav">
    <ul class="nav-list">
       
      <li class="nav-item">
        <a href="/about/">About</a>
        
          <span>/</span>
        
      </li>
        
      <li class="nav-item">
        <a href="/archive/">Archive</a>
        
          <span>/</span>
        
      </li>
          
      <li class="nav-item">
        <a href="/categories/">Categories</a>
        
          <span>/</span>
        
      </li>
            
      <li class="nav-item">
        <a href="/projects/">Projects</a>
        
          <span>/</span>
        
      </li>
        
      <li class="nav-item">
        <a href="/publications/">Publications</a>
        
          <span>/</span>
        
      </li>
               
    </ul>
  </nav>
  <script type="text/javascript">
  // based on http://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('rod.dearaujo@gmail.com', 'Hello from website');</script>
      </li>
    
    <li><a href="http://twitter.com/digorithm" class="social-link-item" target="_blank"><i class="fa fa-fw fa-twitter"></i></a></li>
    
    
    
    
    
    
    
    <li><a href="http://github.com/digorithm" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <div class="post-content">
        <h1 style="max-width: 45%;">Neural Computation: Toward an Intuitive Understanding of the Perceptron</h1>
        <p>Artificial Neurons is one of the most beautiful ways to simulate a biological behavior through computation, despite the fact that it’s not very close to the level of details of a real neuron. But it captured the core of what a neuron is doing.</p>

<p>And I find that trying to understand this mathematical method by first understanding the concept through a metaphor <em>(which, in this case, I think that the metaphor is our mathematical side, and not the biological one, the biological is just, you know, the real thing)</em> is really valuable to gain an intuition on this topic.</p>

<!--more-->

<p>If you were about to make a decision, given many variables, how would you make this decision? It may not look obvious, due the fact that, on normal occasions, we’re not thinking about our thinking process, but the truth is, we are collecting those variables that affect the future decision, and we’re giving weights to them. Weights? how? Simple, some variables are more crucial than other while making decision, isn’t it? the word <em>‘crucial’</em>, in this case, means a huge weight on this variable, and it will strongly affect the final decision.</p>

<p>Suppose a scenario: we’re deciding whether we should go to beach or not. Let’s put a few binary variables on table:</p>

<ol>
  <li>Are we in the mood to go to the beach?</li>
  <li>Is it raining?</li>
  <li>Our other friends are going?</li>
  <li>Do we have money?</li>
</ol>

<p>So, depending on those variables, it will be more likely that we will go to the beach… or not. Let’s think about the variable <em>‘is it raining?’</em>, if it’s raining, we can say that it’s a deal breaker, so we can conclude that this variable has more weights than the other. The fact is that we already trained those weights inside our brain, long time ago.</p>

<p>That’s what an Artificial Neuron try to do, the AN try to learn the weights of things so it can make decisions. So, drawing back to the mathematical and computational aspect of it, the perceptron is a single unit that will receive external inputs<em>(variables)</em>, it will have a weight for each input, it will do some computation, send this result to a decision function, and, finally, output the final decision.</p>

<p>Those inputs are the variables we’re talking before, and like our process to make a decision, the Perceptron will weight each input to make a decision.</p>

<p>Now, how the Perceptron knows how the weights should be for each input? Well… it doesn’t. At least, not from the beginning of the learning process, just like you and me, when trying to learn something new.</p>

<p>That’s why this case is a case of supervised learning, what the Perceptron will do is: start with a random small weights, take one previously trained example <em>(e.g: (1) yes to mood to beach, (2) not raining, (3) yes to friends going to beach and (4) yes to money, the output: 1 - yes, we shall go to the beach)</em> do some computation taking in consideration the random weights we defined before, send it to a decision function, output something <em>(1 - yes, 0 - no)</em> and check if this output is equal to the expected <em>(we’re using a trained example, remember?)</em>, of course that at the first try, it won’t be equal. So the Perceptron calculate the error rate and update its weights… after this, guess what? it repeat the process of trying to predict, but now, with the updated weights, after a few tries, the error rate tends to reduce and it starts predicting correctly. So it learned to predict a behavior, by training with previously trained examples.</p>

<p>So, a nice learning behavior would be something like this:</p>

<div class="imgcenter">
	<img src="/content/images/images/perceptron_0.1.gif" />
</div>

<p>Which means, at every iteration <em>(we call it epochs)</em>, the error rate tend to decrease and, as consequence, the Perceptron starts to predict correctly!</p>

<p>As you may have noted, I omitted a few details of the process so you could see the whole picture of the process: we take an example, we practice on it, we see what we missed, we try again, we learn. That’s the core process.</p>

<p>Now, to the details:</p>

<h3 id="the-computation">The computation</h3>

<p>The first step that the Perceptron does it’s a computation that I referred as <em>“some computation”</em>, this is a simple computation, it’s just a simple Linear Combination of the feature vector <em>(the variables)</em> and the weight vector, which is just the sum of the products between feature/input and its respective weight:</p>

<script type="math/tex; mode=display">x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}+ \cdots +x_{n}w_{n}</script>

<p> </p>

<h3 id="the-decision-function">The Decision Function</h3>
<p>So, after the Linear Combination, we send the result of it to a decision function, which will, somehow, based on some threshold <em>(or without a threshold as we’re going to see)</em>, give us the output, that will be checked with the correct output, in case if it’s correct, fine, keep the weights like this, otherwise, it will calculate the error rate, adjust the weights and restart the process.</p>

<p> </p>

<h4 id="binary-output-with-threshold">Binary output with threshold</h4>

<p>This technique is very simple, after the linear combination, if the output is bigger than some threshold, it outputs 1 and we say that the neuron was activated, otherwise, output 0.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
    X=
    \begin{cases}
      1, & \text{if}\ Linear\,Combination>0 \\
      0, & \text{otherwise}
    \end{cases}
  \end{equation} %]]></script>

<p> </p>

<h4 id="using-sigmoid-function">Using sigmoid function</h4>

<p>Now, that a interesting one, it won’t use a threshold anymore, we’ll send the output of the Linear Combination to a sigmoid function</p>

<script type="math/tex; mode=display">S(\vec{w}\vec{x}) = \dfrac{1}{1+e^{-\vec{w}\vec{x}}}</script>

<p>which will, then, smooth the output, making it in the range of 0 and 1. Which is the most used in many Machine Learning Algorithms.</p>

<div class="imgcenter">
	<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png" />
</div>

<h3 id="adjusting-the-weights-and-learning">Adjusting the weights and learning</h3>

<p>After the perceptron fails to predict correctly, it’s time to adjust the weight, using some rule, the classic perceptron has 2 rules:</p>

<p> </p>

<h4 id="perceptron-learning-rule">Perceptron Learning Rule</h4>
<p>This is very simple, when the perceptron has the incorrect output, it update its weights following this:</p>

<script type="math/tex; mode=display">w_{i} = w_{i} + \eta (t_{i} - o_{i})x_{i}</script>

<p>Which is simply multiplying a learning rate <em>(usually 0.001)</em> by the difference between the correct output and the wrong output and then multiplying it by its original input, after this, we add this value to the previous weight and then we have the value of the new weight. 
It’s fair simple, isn’t it? The problem arises when the data isn’t linearly separable, like this:</p>

<div class="imgcenter">
	<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/DBSCAN-density-data.svg/2000px-DBSCAN-density-data.svg.png" width="400" />
</div>

<p> </p>

<p>With this scenario, the result just won’t converge. So we adopt another rule!</p>

<p> </p>

<h4 id="delta-rule">Delta Rule</h4>

<p>Now we must find a way to have a non-linear output, and what’s the best for this if not the classic Gradient Descent algorithm? GD will simply search through hypothesis spaces and try to minimize the cost function, I wrote about this <a href="http://rodrigoaraujo.me/general/setup/demo/2015/01/09/Making-Your-Machine-Think-Learn-And-Predict--Gradient-Descent-Algorithm-in-Java.html">here</a>.</p>

<div class="imgcenter">
	<img src="http://www.yaldex.com/game-development/FILES/17fig06.gif" />
</div>

<p>So, it’s just a technique to solve our previous problem, but still, the weights will be updated <em>(now, using the GD)</em> and then the process start over!</p>

<p> </p>

<h3 id="code-of-a-perceptron">Code of a Perceptron</h3>

<p>Now, here’s a code of a Perceptron that will behave like a simple Boolean function. I didn’t use the Delta Dule (Gradient Descent) to optimize the weights, as this problem (binary Boolean functions) is linearly separable.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code>  <span class="kn">import</span> <span class="nn">math</span>
  <span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="n">array</span>
  <span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">choice</span>
  <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

  <span class="k">class</span> <span class="nc">Perceptron</span><span class="p">():</span>
      
      <span class="s">"""
      this is the thereshold to activate the unit 
      """</span>
      <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

      <span class="k">def</span> <span class="nf">linear_combination</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
          <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)):</span>
              <span class="n">total</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
          <span class="k">return</span> <span class="n">total</span>
      
      <span class="k">def</span> <span class="nf">unit_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
          <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_combination</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

      <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
          
          <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.5</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>

          <span class="c"># initializing weights</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>
          
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
              <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">choice</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span>
              <span class="c"># &gt; calculate output with current weight</span>
              <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
              <span class="c"># &gt; calculate error rate (t - o)</span>
              <span class="n">error_rate</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">output</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error_rate</span><span class="p">)</span>
              <span class="k">print</span> <span class="s">'the X: '</span><span class="p">,</span> <span class="n">X</span>
              <span class="k">print</span> <span class="s">'output: '</span><span class="p">,</span><span class="n">output</span>
              <span class="k">print</span> <span class="s">'correct target: '</span><span class="p">,</span> <span class="n">y</span>
              <span class="c"># &gt; apply learning rule which will update weights</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">error_rate</span> <span class="o">*</span> <span class="n">X</span>

      <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
          <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">y</span>
                  

          <span class="n">training_data</span> <span class="o">=</span> <span class="p">[</span>
                  <span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">1</span><span class="p">),</span>
                  
          <span class="p">]</span>

  <span class="n">p</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">()</span>

  <span class="n">p</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
  <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">errors</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'errors'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epochs'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre>
</div>

<h3 id="conclusion-and-next-steps">Conclusion and next steps</h3>

<p>With this we conclude what a simple single Perceptron is doing! Of course there are many improvements on it and we can connect many of these Artificial Neurons in many layers… that’s what is called Artificial Neural Network, I’ll be writing about it soon!</p>


        <footer>
  &copy; 2017 Rodrigo Araujo 
</footer>

        </div>
      </div>
    </div>
  </div>
  <script type="text/javascript" src="/js/jquery-2.1.4.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-38327934-3', 'auto');
  ga('send', 'pageview');
</script>


</body>

</html>
