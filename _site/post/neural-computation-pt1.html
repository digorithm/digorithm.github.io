<!DOCTYPE html>
<html lang="en">
  
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Neural Computation: Toward an Intuitive Understanding of the Perceptron</title>

  <meta name="author" content="Rodrigo Araújo" />
  
  

  <link rel="alternate" type="application/rss+xml" title="Rodrigo Araújo - Stories and thoughts on Computer Science" href="/feed.xml" />
  
 <!-- everything has to be repeated twice because on 2016-02-01 GitHub pages migrated to jekyll 3; see bug https://github.com/jekyll/jekyll/issues/4439 -->

    
  
    
      <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" />
    
      
  
  
  
    
      <link rel="stylesheet" href="/css/bootstrap.min.css" />
    
      <link rel="stylesheet" href="/css/main.css" />
    
    
  
  
  
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
    
  

    
    
  
  
  
  
  
     
  
  <!-- Facebook OpenGraph tags -->
  <meta property="og:title" content="Neural Computation: Toward an Intuitive Understanding of the Perceptron" />
  <meta property="og:type" content="website" />
  
  
  <meta property="og:url" content="http://digorithm.github.io/post/neural-computation-pt1/" />
  
  
  
  <meta property="og:image" content="http://digorithm.github.io/img/pic.jpg" />
  
  
</head>


  <body>
  
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://digorithm.github.io">Rodrigo Araújo</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
          <li>
            
            





<a href="/aboutme">About Me</a>

          </li>
        
        
        
          <li>
            
            





<a href="/publications">Publications</a>

          </li>
        
        
        
          <li>
            
            





<a href="/archive">Archive</a>

          </li>
        
        
        
          <li>
            
            





<a href="/readings">Recommended Readings</a>

          </li>
        
        
      </ul>
    </div>

	
	<div class="avatar-container">
	  <div class="avatar-img-border">
	    <a href="http://digorithm.github.io ">
	      <img class="avatar-img" src="/img/pic.jpg" />
		</a>
	  </div>
	</div>
	

  </div>
</nav>


    <!-- TODO this file has become a mess, refactor it -->





<header class="header-section ">

<div class="intro-header no-img">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-heading">
          <h1>Neural Computation: Toward an Intuitive Understanding of the Perceptron</h1>
		  
		  
		  
		  <span class="post-meta">Posted on July 16, 2015</span>
		  
        </div>
      </div>
    </div>
  </div>
</div>
</header>




<div class="container">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
	      <p>Artificial Neurons is one of the most beautiful ways to simulate a biological behavior through computation, despite the fact that it’s not very close to the level of details of a real neuron. But it captured the core of what a neuron is doing.</p>

<p>And I find that trying to understand this mathematical method by first understanding the concept through a metaphor <em>(which, in this case, I think that the metaphor is our mathematical side, and not the biological one, the biological is just, you know, the real thing)</em> is really valuable to gain an intuition on this topic.</p>

<p>If you were about to make a decision, given many variables, how would you make this decision? It may not look obvious, due the fact that, on normal occasions, we’re not thinking about our thinking process, but the truth is, we are collecting those variables that affect the future decision, and we’re giving weights to them. Weights? how? Simple, some variables are more crucial than other while making decision, isn’t it? the word <em>‘crucial’</em>, in this case, means a huge weight on this variable, and it will strongly affect the final decision.</p>

<p>Suppose a scenario: we’re deciding whether we should go to beach or not. Let’s put a few binary variables on table:</p>

<ol>
  <li>Are we in the mood to go to the beach?</li>
  <li>Is it raining?</li>
  <li>Our other friends are going?</li>
  <li>Do we have money?</li>
</ol>

<p>So, depending on those variables, it will be more likely that we will go to the beach… or not. Let’s think about the variable <em>‘is it raining?’</em>, if it’s raining, we can say that it’s a deal breaker, so we can conclude that this variable has more weights than the other. The fact is that we already trained those weights inside our brain, long time ago.</p>

<p>That’s what an Artificial Neuron try to do, the AN try to learn the weights of things so it can make decisions. So, drawing back to the mathematical and computational aspect of it, the perceptron is a single unit that will receive external inputs<em>(variables)</em>, it will have a weight for each input, it will do some computation, send this result to a decision function, and, finally, output the final decision.</p>

<p>Those inputs are the variables we’re talking before, and like our process to make a decision, the Perceptron will weight each input to make a decision.</p>

<p>Now, how the Perceptron knows how the weights should be for each input? Well… it doesn’t. At least, not from the beginning of the learning process, just like you and me, when trying to learn something new.</p>

<p>That’s why this case is a case of supervised learning, what the Perceptron will do is: start with a random small weights, take one previously trained example <em>(e.g: (1) yes to mood to beach, (2) not raining, (3) yes to friends going to beach and (4) yes to money, the output: 1 - yes, we shall go to the beach)</em> do some computation taking in consideration the random weights we defined before, send it to a decision function, output something <em>(1 - yes, 0 - no)</em> and check if this output is equal to the expected <em>(we’re using a trained example, remember?)</em>, of course that at the first try, it won’t be equal. So the Perceptron calculate the error rate and update its weights… after this, guess what? it repeat the process of trying to predict, but now, with the updated weights, after a few tries, the error rate tends to reduce and it starts predicting correctly. So it learned to predict a behavior, by training with previously trained examples.</p>

<p>So, a nice learning behavior would be something like this:</p>

<div class="imgcenter">
	<img src="/content/images/images/perceptron_0.1.gif" />
</div>

<p>Which means, at every iteration <em>(we call it epochs)</em>, the error rate tend to decrease and, as consequence, the Perceptron starts to predict correctly!</p>

<p>As you may have noted, I omitted a few details of the process so you could see the whole picture of the process: we take an example, we practice on it, we see what we missed, we try again, we learn. That’s the core process.</p>

<p>Now, to the details:</p>

<h3 id="the-computation">The computation</h3>

<p>The first step that the Perceptron does it’s a computation that I referred as <em>“some computation”</em>, this is a simple computation, it’s just a simple Linear Combination of the feature vector <em>(the variables)</em> and the weight vector, which is just the sum of the products between feature/input and its respective weight:</p>

<script type="math/tex; mode=display">x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}+ \cdots +x_{n}w_{n}</script>

<p> </p>

<h3 id="the-decision-function">The Decision Function</h3>
<p>So, after the Linear Combination, we send the result of it to a decision function, which will, somehow, based on some threshold <em>(or without a threshold as we’re going to see)</em>, give us the output, that will be checked with the correct output, in case if it’s correct, fine, keep the weights like this, otherwise, it will calculate the error rate, adjust the weights and restart the process.</p>

<p> </p>

<h4 id="binary-output-with-threshold">Binary output with threshold</h4>

<p>This technique is very simple, after the linear combination, if the output is bigger than some threshold, it outputs 1 and we say that the neuron was activated, otherwise, output 0.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
    X=
    \begin{cases}
      1, & \text{if}\ Linear\,Combination>0 \\
      0, & \text{otherwise}
    \end{cases}
  \end{equation} %]]></script>

<p> </p>

<h4 id="using-sigmoid-function">Using sigmoid function</h4>

<p>Now, that a interesting one, it won’t use a threshold anymore, we’ll send the output of the Linear Combination to a sigmoid function</p>

<script type="math/tex; mode=display">S(\vec{w}\vec{x}) = \dfrac{1}{1+e^{-\vec{w}\vec{x}}}</script>

<p>which will, then, smooth the output, making it in the range of 0 and 1. Which is the most used in many Machine Learning Algorithms.</p>

<div class="imgcenter">
	<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png" />
</div>

<h3 id="adjusting-the-weights-and-learning">Adjusting the weights and learning</h3>

<p>After the perceptron fails to predict correctly, it’s time to adjust the weight, using some rule, the classic perceptron has 2 rules:</p>

<p> </p>

<h4 id="perceptron-learning-rule">Perceptron Learning Rule</h4>
<p>This is very simple, when the perceptron has the incorrect output, it update its weights following this:</p>

<script type="math/tex; mode=display">w_{i} = w_{i} + \eta (t_{i} - o_{i})x_{i}</script>

<p>Which is simply multiplying a learning rate <em>(usually 0.001)</em> by the difference between the correct output and the wrong output and then multiplying it by its original input, after this, we add this value to the previous weight and then we have the value of the new weight. 
It’s fair simple, isn’t it? The problem arises when the data isn’t linearly separable, like this:</p>

<div class="imgcenter">
	<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/DBSCAN-density-data.svg/2000px-DBSCAN-density-data.svg.png" width="400" />
</div>

<p> </p>

<p>With this scenario, the result just won’t converge. So we adopt another rule!</p>

<p> </p>

<h4 id="delta-rule">Delta Rule</h4>

<p>Now we must find a way to have a non-linear output, and what’s the best for this if not the classic Gradient Descent algorithm? GD will simply search through hypothesis spaces and try to minimize the cost function, I wrote about this <a href="http://rodrigoaraujo.me/general/setup/demo/2015/01/09/Making-Your-Machine-Think-Learn-And-Predict--Gradient-Descent-Algorithm-in-Java.html">here</a>.</p>

<div class="imgcenter">
	<img src="http://www.yaldex.com/game-development/FILES/17fig06.gif" />
</div>

<p>So, it’s just a technique to solve our previous problem, but still, the weights will be updated <em>(now, using the GD)</em> and then the process start over!</p>

<p> </p>

<h3 id="code-of-a-perceptron">Code of a Perceptron</h3>

<p>Now, here’s a code of a Perceptron that will behave like a simple Boolean function. I didn’t use the Delta Dule (Gradient Descent) to optimize the weights, as this problem (binary Boolean functions) is linearly separable.</p>

<pre>
<code class="python hljs">

import math
from numpy import random, array
from random import choice
import matplotlib.pyplot as plt

class Perceptron():
    
    """
    this is the thereshold to activate the unit 
    """
    def activation(self,x):
        return 1/(1 + math.exp(-x))

    def linear_combination(self, X):
        total = 0
        for i in xrange(len(self.weights)):
            total += self.weights[i] * X[i]
        return total
    
    def unit_output(self, X):
        return self.activation(self.linear_combination(X))

    def train(self, training_data, epochs):
        
        self.learning_rate = 0.5
        self.errors = []

        # initializing weights
        self.weights = random.rand(len(training_data[1][0]))
        
        for i in xrange(epochs):
            X, y = choice(training_data)
            # &gt; calculate output with current weight
            output = self.unit_output(X)
            # &gt; calculate error rate (t - o)
            error_rate = y - output
            self.errors.append(error_rate)
            print 'the X: ', X
            print 'output: ',output
            print 'correct target: ', y
            # &gt; apply learning rule which will update weights
            self.weights += self.learning_rate * error_rate * X

    def predict(self,X):
        y = self.unit_output(X)
        return y
                

        training_data = [
                (array([0,0,1]), 0),
                (array([0,1,1]), 0),
                (array([1,0,1]), 0),
                (array([1,1,1]), 1),
                
        ]

p = Perceptron()

p.train(training_data, 2000)
print p.predict([1,0,1])
plt.plot(p.errors)
plt.ylabel('errors')
plt.xlabel('epochs')
plt.show()
</code>
</pre>

<h3 id="conclusion-and-next-steps">Conclusion and next steps</h3>

<p>With this we conclude what a simple single Perceptron is doing! Of course there are many improvements on it and we can connect many of these Artificial Neurons in many layers… that’s what is called Artificial Neural Network, I’ll be writing about it soon!</p>


	    </article>
	  	  
      <ul class="pager blog-pager">
        
        <li class="previous">
          <a href="/post/case-study-sentiment-analysis-movie-reviews" data-toggle="tooltip" data-placement="top" title="Case Study: Sentiment Analysis On Movie Reviews">&larr; Previous Post</a>
        </li>
        
        
        <li class="next">
          <a href="/post/thoughts-visualizing-software-arch" data-toggle="tooltip" data-placement="top" title="Thoughts on Visualizing Software Architecture">Next Post &rarr;</a>
        </li>
        
      </ul>
      
	    
        <div class="disqus-comments">
	        

        </div>
	    
    </div>
  </div>
</div>


    <footer>
  <div class="container beautiful-jekyll-footer">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          <li>
            <a href="https://www.facebook.com/contatodigo" title="Facebook">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li>
            <a href="https://github.com/digorithm" title="GitHub">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
          <li>
            <a href="https://twitter.com/digorithm" title="Twitter">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
          <li>
            <a href="mailto:rod.dearaujo@gmail.com" title="Email me">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
		  
		  		  
        </ul>
        <p class="copyright text-muted">
		  Rodrigo Araújo
		  &nbsp;&bull;&nbsp;
		  2016
		  
		  
		  &nbsp;&bull;&nbsp;
		  <a href="http://digorithm.github.io">rodrigoaraujo.me</a>
		  
	    </p>
      </div>
    </div>
  </div>
</footer>

  
    <!-- everything has to be repeated twice because on 2016-02-01 GitHub pages migrated to jekyll 3; see bug https://github.com/jekyll/jekyll/issues/4439 -->




<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<link rel="stylesheet" href="/assets/css/styles/hybrid.css">
	<script src="/assets/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>








  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script>
      	if (typeof jQuery == 'undefined') {
      	  document.write('<script src="/js/jquery-1.11.2.min.js"></scr' + 'ipt>');
      	}
      </script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/bootstrap.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="/js/main.js"></script>
    
  






  
  </body>
</html>
