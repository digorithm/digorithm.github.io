<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Making your machine think, learn and predict - Gradient Descent Algorithm in Java</title>
	
	<meta name="author" content="Rodrigo Araújo">

	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

	
	<meta name="description" content="I'm a Computer Scientist and Software Engineer, I enjoy Artificial Intelligence, Programming, Software Engineering."/>
	
	
	<meta name="keywords" content="machine learning, artificial intelligence, java, programming, engineering, tutorial"/>
	

	<!-- Le styles -->
	<link href="/assets/resources/bootstrap/css/bootstrap.min.css" rel="stylesheet">
	<link href="/assets/resources/font-awesome/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/resources/syntax/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">
        <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

	<!-- Le fav and touch icons -->
	<!-- Update these with your own images
	<link rel="shortcut icon" href="images/favicon.ico">
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
	-->

	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
	<link rel="stylesheet" href="/assets/css/styles/hybrid.css">
	<script src="/assets/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/digorithm">
				<i class="fa fa-github"></i>
			</a>
			
			
			<a type="button" class="navbar-toggle nav-link" href="http://twitter.com/digorithm">
				<i class="fa fa-twitter"></i>
			</a>
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:rod.dearaujo@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
			<a class="navbar-brand" href="/">
				<img src="http://www.gravatar.com/avatar/?s=35" class="img-circle" />
				
			</a>
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="col-sm-3 sidebar hidden-xs">
		<!-- sidebar.html -->
<style>
#focusbutton {
        position: absolute;
        top: 0;
        right:0;
        cursor: pointer;
        padding-right: 5px;
        padding-top:5px;
}

.fa-2x {
        color:#00ccd6;
}

#focusoff {
        display:none;
}
</style>

<header class="sidebar-header" role="banner">
	<a href="">
		<img src="/content/images/images/pic5.jpg" width="180" height="180" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/">Rodrigo Araújo</a>
    </h3>
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
</header>
        <div id='focusbutton'>
        <i id="focus" class="fa fa-minus-circle fa-2x"></i>
        <i id="focusoff" class="fa fa-plus-circle fa-2x"></i>
        </div>

<div id="bio" class="text-center">
	Stories, Thoughts and Ideas on Computer Science, Mathematics and Technology
</div>

<div class="pages">
	<ul>
		<li><a href="/about">Who I am</a></li>
		<li>
			<div id="workexpand"><a href="/work">Publications</a>
			<div id="subitem">
			<ul>
				<li>
					<a href="/work/#pub">Papers</a>
				</li>
				<li>
					<a href="/work/#talk">Talks</a>
				</li>
				<li>
					<a href="/work/#releases">Releases</a>
				</li>
			</ul>
			</div>
			</div>
			
			
		</li>
		<li><a href="/archives">Archives</a></li>
		<li><a href="/readings">Recommended Readings</a></li>
	</ul>
</div>
<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/digorithm">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="https://twitter.com/digorithm">
				<i class="fa fa-twitter fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:rod.dearaujo@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
	</ul>
	<ul id="contact-list-secondary" class="list-unstyled list-inline">
		
		
		<li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->
<script>

$('#focus').on("click", function(){
    $('.sidebar-header').css('display', 'none');
    $('#bio').css('display', 'none');
    $('.pages').css('display', 'none');
    $('#contact-list').css('display', 'none');
    $('.col-sm-3').css('width', '3%');
    $('#focusoff').css('display', 'block');
    $('#focus').css('display', 'none');

});

$('#focusoff').on("click", function(){
    $('.sidebar-header').css('display', 'block');
    $('#bio').css('display', 'block');
    $('.pages').css('display', 'block');
    $('#contact-list').css('display', 'block');
    $('.col-sm-3').css('width', '25%');
    $('#focusbutton').css('display', 'block');
    $('#focusoff').css('display', 'none');
    $('#focus').css('display', 'block');

});


	$(function () {

	var pathname = window.location.pathname;
	console.log(pathname);
	if (pathname == "/work/"){
		$("#subitem").fadeIn();
	} else {
		$("#subitem").fadeOut();
	}
});

$(function() {
  $('a[href*=#]:not([href=#])').click(function() {
    if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'') && location.hostname == this.hostname) {
      var target = $(this.hash);
      target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
      if (target.length) {
        $('html,body').animate({
          scrollTop: target.offset().top
        }, 1000);
        return false;
      }
    }
  });
});
</script>

	</div>

	<div class="col-sm-9 col-sm-offset-3">
		<div class="page-header">
  <h1>Making your machine think, learn and predict - Gradient Descent Algorithm in Java </h1>
</div>
	
<article>

	<div class="col-sm-10">
	 <span class="post-date" style="margin-left:-15px;">
	   
	   January 
	   9th,
	   
	   2015
	 </span>
	  <div class="article_body">
	  <h2 id="how-can-we-make-a-machine-learn-from-data">How can we make a machine learn from data?</h2>

<p>Then, how can we make the machine predicts things based on that learned data? Those are the question answered by one of the most classic Machine Learning Algorithms, the <strong>Gradient Descent Algorithm</strong>, from a Mathematical-Statistical side it’s called <strong>Univariate Linear Regression</strong>.</p>

<p>This is one of the tools of the Machine Learning toolbox, and what it tries to do is to model a relationship between a scalar dependent variable Y and a explanatory variable X.</p>

<h2 id="in-laymans-term">In Layman’s term…</h2>

<p>Let’s suppose you have a few points distributed in a Graph, so you already know that in a point A you have a well defined X and Y, which means, if you input X, your output will be Y, and in a point B you have a well defined X’ and Y’ as well. But, thing is, if a point emerge between A and B, and you only have the X… what will be the Y <em>(the output)</em>?</p>

<p>What this algorithm does is: <strong>It tries to predict this Y value, based on the previous data</strong>! Amazing, right?</p>

<p>At the end of the execution, you’ll have a full trend line that you can use to predict values! just like the image below.</p>

<p><img src="/content/images/2015/06/linearRegression.png" alt="" /></p>

<h2 id="the-theory-behind-it">The Theory Behind It</h2>

<p>I’ll cover a few theories about this algorithm here, but, it won’t be complete, as this demand a great coverage of mathematical material that if I would write it all here, It would be a <em>long, long</em>, <strong>very</strong> <em>long</em> post. So I’m assuming that you’re already familiar with Calculus <em>(Sums, Partial Derivatives)</em>, Statistics and Discrete Mathematics.</p>

<p>So, our goal here is to <strong>fit the best straight line in our initial data</strong>, right?</p>

<p>Thus, we need something to represent this straight line, which will be our hypothesis function:</p>

<div id="math"> 
$ h_{\Theta}(x) = \Theta_{0} + \Theta_{1}x $ 
</div>

<p>Where this \( \Theta_{0} \) and \( \Theta_{1} \) are the parameters of the function, and <strong>finding the best parameters for this function is what is going to give us the correct straight line to plot on our data</strong>.</p>

<p>Here’s an example of what we’re trying to do, which is, fit the best straight line in the data:</p>

<p><img src="/content/images/2015/06/Linear-regression.svg" alt="" /></p>

<p>So what we want to do is to find a \( \Theta_{0} \) and \( \Theta_{1} \) so our Hypothesis outputs can be very close to the real Y output.</p>

<p>Formally we want:</p>

<div id="math">$ \underset{\Theta_{0}\Theta_{1}}{min}(h_{\Theta}(x) - y)^2 $</div>

<p>So we want <strong>minimize</strong> \( \Theta_{0} \) and \( \Theta_{1} \) so the difference between the <strong>Hypothesis</strong> and the <strong>real output</strong> is minimal.</p>

<p><strong>But we want it for every point in our data</strong>, which is \(x(i)\) <em>(which is the i-th x of our data)</em>, so we want the <strong>sum of this average</strong>, which is, formally:</p>

<div id="math"> 
$ \underset{\Theta_{0}\Theta_{1}}{min}\,\frac{1}{2m} \sum_{i=1}^{m} (h_{0}(x^{(i)}) - y^{(i)})^2 $
</div>
<p><br /></p>

<p>And we’re going to call this function <strong>Cost Function</strong>, with the following notation:</p>

<div id="math"> 
$ J(\Theta_{0}, \Theta_{1}) = \,\frac{1}{2m} \sum_{i=1}^{m} (h_{0}(x^{(i)}) - y^{(i)})^2 $
</div>
<p><br /></p>

<p>So, our goals is to <strong>minimize this cost function</strong>:</p>

<div id="math"> 
$ \underset{\Theta_{0}\Theta_{1}}{min}\,\, J(\Theta_{0}, \Theta_{1}) $
</div>
<p><br /></p>

<p>This cost function is also called <a href="http://en.wikipedia.org/wiki/Mean_squared_error">Square Error Function</a>.</p>

<h3 id="now-the-gradient-descent-algorithm">Now, the gradient descent algorithm</h3>

<p>With our cost function built, we need to “keep” finding values for \( \Theta_{0} \) and \( \Theta_{1} \) so we can reach our ideal trend line. So, basically:</p>

<p>1. We start with some \( \Theta_{0} \) and \( \Theta_{1} \)</p>

<p>2. keep changing \( \Theta_{0} \) and \( \Theta_{1} \) to reduce our cost function \( J(\Theta_{0}, \Theta_{1}) \), until we find the minimum.</p>

<p>That’s quite simple and intuitive, right? There’s a lot of intuitive explanation and more visual examples of what this algorithm is doing in the <a href="https://class.coursera.org/ml-007/">machine learning course taught by Andrews Ng (From Stanford)</a>.</p>

<p>What this algorithm will be doing is: partially derive our cost function for \( \Theta_{0} \) and \( \Theta_{1} \) simultaneously, so we can find the minimum value for them, with every iteration updating our \( \Theta_{0} \) and \( \Theta_{1} \) with their new value!</p>

<blockquote>
  <p><em>- Meh, talk is cheap show me the math!</em></p>
</blockquote>

<p>Formally, the algorithm is:</p>

<div id="math">
$ repeat \, until \, convergence\,\{ \\ \,\,\,\,\,\,\,\,\,\,\,\, \Theta_{j} := \Theta_{j} - \alpha \frac{\partial }{\partial \Theta_{j}} \,\, J(\Theta_{0}, \Theta_{1}) \\ \} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, $
</div>
<p><br /></p>

<p><em><strong>Make sure that this will run for j = 0 and j = 1.</strong></em></p>

<p>But, pay attention, this is the generic version, the \( \Theta_{j} \) represent both \( \Theta_{0} \) and \( \Theta_{1} \).</p>

<p>What the algorithm is saying is that we’ll be doing this procedure to \( \Theta_{0} \) and \( \Theta_{1} \) at the same time! So, we can put it in this way:</p>

<div id="math">
$ repeat \, until \, convergence\,\{ \\ \Theta_{0} := \Theta_{0} - \alpha \frac{\partial }{\partial \Theta_{0}} \,\, J(\Theta_{0}, \Theta_{1}) \\ \Theta_{1} := \Theta_{1} - \alpha \frac{\partial }{\partial \Theta_{1}} \,\, J(\Theta_{0}, \Theta_{1}) \\ \} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\ $
</div>
<p><br /></p>

<p>Also, we can expand the <strong>Cost Function</strong> that is being derived, doing this, it will be exactly what I’ll be putting into code soon, so, our <strong>final algorithm</strong> is:</p>

<div id="math">
$ repeat \, until \, convergence\,\{ \\ \Theta_{0} := \Theta_{0} - \alpha \frac{\partial }{\partial \Theta_{0}} \,\, \Big (\frac{1}{2m} \sum_{i=1}^{m} (h_{\Theta}(x^{(i)}) - y^{(i)})^2 \Big) \\ \Theta_{1} := \Theta_{1} - \alpha \frac{\partial }{\partial \Theta_{1}} \,\, \Big (\frac{1}{2m} \sum_{i=1}^{m} (h_{\Theta}(x^{(i)}) - y^{(i)})^2 \Big) \\ \}
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\ $
</div>
<p><br /></p>

<p><strong>Now it’s time to code all of it!</strong></p>

<p>First things first, we’ll be using <a href="http://code.google.com/p/jmathplot/">Google’s JMathPlot</a> to plot graphs using Java and Swing to use its JFrame, we shall start with our class to represent our <strong>Initial Data</strong>, which will be the <strong>Training Set</strong>.</p>
<pre>
<code class="java hljs">
    import javax.swing.JFrame;
    import org.math.plot.*;

    public class InitialData{
            public static double[] x = {2, 4, 6, 8};
            public static double[] y = {2, 5, 5, 8};

            public void plotData(){
                    Plot2DPanel plot = new Plot2DPanel();
                    plot.addScatterPlot("X-Y", this.x, this.y);
                    JFrame frame = new JFrame("Original X-Y Data");
                    frame.setContentPane(plot);
                    frame.setSize(600, 600);
                    frame.setVisible(true);
            }
    }
</code>
</pre>

<p><br />
To try out the data plotting, create a main class and call <em>plotData()</em> from <strong>InitialData.java</strong>, so we can have this:</p>

<p><img src="/content/images/2015/06/example-plot.png" alt="" /></p>

<p>Now we’re going to take our first steps on writing the <strong>GradientDescent.java</strong>, we must be very careful here. Let’s start with the main settings and parameters of it:</p>

<pre>
<code class="java hljs">
    import javax.swing.JFrame;
    import org.math.plot.*;

    public class GradientDescent{
            private double theta0;
            private double theta1;

            private int trendline;

            // Algorithm settings
        double alpha = 0.01;  // learning rate
        double tol = 1e-11;   // tolerance to determine convergence
        int maxiter = 9000;   // maximum number of iterations in case convergence is not reached
        int dispiter = 100;   // interval for displaying results during iterations
        int iters = 0;

        //track of results
        double[] theta0plot = new double[maxiter+1];
        double[] theta1plot = new double[maxiter+1];
        double[] tplot = new double[maxiter+1];

        InitialData initial_data;

        Plot2DPanel plot;

        public GradientDescent(InitialData id){
                    //initial guesses
            this.theta0 = 0;
            this.theta1 = 0;
            this.initial_data = id;

            plot = new Plot2DPanel();
            plot.addScatterPlot("X-Y", initial_data.x, initial_data.y);
            JFrame frame = new JFrame("Final X-Y Data");
            frame.setContentPane(plot);
            frame.setSize(600, 600);
            frame.setVisible(true);
        }
    [...]
</code>
</pre>

<p>Now let me explain a few details of this part:</p>

<p><strong>Alpha</strong> is the <strong>Learning Rate</strong>, it’s a <em>dangerous variable</em>, it’s used to set the size of the step that the algorithm will take while trying to find the \( \Theta_{0} \) and \( \Theta_{1} \), that’s the learning rate of the algorithm. If Alpha is <strong>too low</strong>, the algorithm can be very slow, although very precise, if Alpha is <strong>higher</strong>, it will be taking <strong>larger steps</strong>, which can be <strong>faster</strong>, or <strong>dangerous</strong>, causing the algorithm to <strong>DIVERGE</strong>, which, <em>trust me</em>, you don’t want this! <em>(Andrew Ng explain this part very well in its course)</em></p>

<p>The variable <strong>TrendLine</strong> is what we’ll use to plot the straight line which is our main goal.</p>

<p>The <strong>tol</strong> variable is our safe move in case of a dangerous convergence, which means, in case of convergence, it will stop the execution.</p>

<p>The other variables and objects in this part are very intuitive to understand, it’s auto explainable! <em>(forgive if i’m wrong, just say something and I’ll put more detail on that)</em>.</p>

<p>About the Constructor, we’re saying that our initial guesses for \( \Theta_{0} \) and \( \Theta_{1} \) is 0. The rest is just data plotting.</p>

<p><strong>next: our Hypothesis Function</strong> <em>(that will be doing exactly as the model that I did show above)</em></p>
<pre>
<code class="java hljs">
    public double hypothesisFunction(double x){
            return this.theta1*x + theta0;
        }
</code>
</pre>

<p>Now, our two function to derive our \( \Theta \), again, it will be doing exactly the same as the mathematical model, there’s no magic!</p>

<pre>
<code class="java hljs">
    public double deriveTheta1(){
            double sum = 0;

            for (int j=0; j&lt;initial_data.x.length; j++){
                    sum += (initial_data.y[j] - hypothesisFunction(initial_data.x[j])) * initial_data.x[j];
            }
            return -2 * sum / initial_data.x.length;
        }

        public double deriveTheta0(){
            double sum = 0;

            for (int j=0; j&lt;initial_data.x.length; j++) {
                    sum += initial_data.y[j] - hypothesisFunction(initial_data.x[j]);
            }
            return -2 * sum / initial_data.x.length;

        }
</code>
</pre>

<p><br />
Now, the <strong>Gradient Descent Algorithm</strong> <em>per se</em>, making use of the functions above:</p>

<pre>
<code class="java hljs">
    public void execute(){
            do {

                    this.theta1 -= alpha * deriveTheta1();
                    this.theta0 -= alpha * deriveTheta0();

                    //used for plotting
                    tplot[iters] = iters;
                    theta0plot[iters] = theta0;
                    theta1plot[iters] = theta1;
                    iters++;

                    if (iters % dispiter == 0){
                            addTrendLine(plot, true);

                    }

                    if (iters &gt; maxiter) break;
            } while (Math.abs(theta1) &gt; tol || Math.abs(theta0) &gt; tol);
            plot.addScatterPlot("X-Y", initial_data.x, initial_data.y);
            System.out.println("theta0 = " + this.theta0 + " and theta1 = " + this.theta1);
        }
</code>
</pre>

<p>Note that it does <em>almost</em> exactly the same as the mathematical model of the Gradient Descent demonstrated above, the difference is only a few details, such as the <em>if</em> and <em>while</em> to verify convergence or divergence <em>(which is, if it reached the iteration’s limit)</em></p>

<p>Next we have the <strong>addTrendLine</strong> function, used to keep plotting our straight line as it will become more updated.</p>

<pre>
<code class="java hljs">
    public void addTrendLine(Plot2DPanel plot, boolean removePrev){
            if (removePrev){
                    plot.removePlot(trendline);
            }
            double[] yEnd = new double[initial_data.x.length];
            for (int i=0; i&lt;initial_data.x.length; i++)
                    yEnd[i] = hypothesisFunction(initial_data.x[i]);
            trendline = plot.addLinePlot("final", initial_data.x, yEnd);
        }
</code>
</pre>

<p>And now we have an extra, it’s a function to store and plot the convergence history of both \( \Theta_{0} \) and \( \Theta_{1} \), so we can see how it happened.</p>

<pre>
<code class="java hljs">
    public void printConvergence(){

          double[] theta0plot2 = new double[iters];
          double[] theta1plot2 = new double[iters];
          double[] tplot2 = new double[iters];
          System.arraycopy(theta0plot, 0, theta0plot2, 0, iters);
          System.arraycopy(theta1plot, 0, theta1plot2, 0, iters);
          System.arraycopy(tplot, 0, tplot2, 0, iters);

          // Plot the convergence of data
          Plot2DPanel convPlot = new Plot2DPanel();

          // add a line plot to the PlotPanel
          convPlot.addLinePlot("theta0", tplot2, theta0plot2);
          convPlot.addLinePlot("theta1", tplot2, theta1plot2);

          // put the PlotPanel in a JFrame, as a JPanel
          JFrame frame2 = new JFrame("Convergence of parameters over time");
          frame2.setContentPane(convPlot);
          frame2.setSize(600, 600);
          frame2.setVisible(true);
        }
</code>
</pre>

<p>Finally, we have our <strong>Test Class</strong>, that will execute everything:</p>

<pre>
<code class="java hljs">
    import javax.swing.JFrame;

    import org.math.plot.*;

    public class TestGradDescent {
            public static void main(String[] args ){
                    InitialData id = new InitialData();
                    id.plotData();
                    GradientDescent gd = new GradientDescent(id);
                    gd.execute();
                    gd.printConvergence();

            }
    }
</code>
</pre>

<p>Now, executing the code, the output will be, at first, the initial data:</p>

<p><img src="/content/images/2015/06/example-plot-2.png" alt="" /></p>

<p><strong>Executing the algorithm, it learns and generate its prediction based on its initial data:</strong></p>

<p><img src="/content/images/2015/06/example-plot-line-e1420763780646.png" alt="" /></p>

<h2 id="magical-right">Magical, right?</h2>

<p>And then, we can see the <strong>convergence</strong>:</p>

<p><img src="/content/images/2015/06/example-convergence.png" alt="" /></p>

<p>And you can see in the terminal the final values of \( \Theta_{0} \) and \( \Theta_{1} \) that minimized the Cost Function.</p>

<p><em>If it wasn’t Science, probably would be black magic. heh.</em></p>

<h2 id="a-few-considerations">A few considerations</h2>

<p>You can download the complete code <a href="https://github.com/digorithm/ArtificialIntelligenceAlgorithms">here</a>.</p>

<p>If you have any questions/suggestion, drop me an email so we can talk!</p>

<p>If you need further details of the mathematical model, I ultra advice to watch Andrew’s Ng Videos at Stanford@Coursera. <strong>His skills to teach it is something unbelievable awesome</strong>.</p>

<p><em>Thanks for reading!</em></p>

	  </div>

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#machine learning-ref">
					machine learning <span>(7)</span>
					,
					</a>
				</li>
			 
				<li>
					<a href="/tags.html#artificial intelligence-ref">
					artificial intelligence <span>(7)</span>
					,
					</a>
				</li>
			 
				<li>
					<a href="/tags.html#java-ref">
					java <span>(4)</span>
					,
					</a>
				</li>
			 
				<li>
					<a href="/tags.html#programming-ref">
					programming <span>(10)</span>
					,
					</a>
				</li>
			 
				<li>
					<a href="/tags.html#engineering-ref">
					engineering <span>(13)</span>
					,
					</a>
				</li>
			 
				<li>
					<a href="/tags.html#tutorial-ref">
					tutorial <span>(8)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-6">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=Making your machine think, learn and predict - Gradient Descent Algorithm in Java&via=digorithm"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/post/learn-design-patterns" title="Why, how and where to Learn Design Patterns">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="/post/using-python-and-ai-to-predict-wine" title="Using Python and AI to predict types of wine">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>

<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    // Required: on line below, replace text in quotes with your forum shortname
    var disqus_shortname = 'rodrigoaraujo';
        
        /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                                        
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>



		<footer>
			<hr/>
			<p>
				&copy; 2015 Rodrigo Araújo with Jekyll.</a>
			</p>
		</footer>
	</div>

	<script type="text/javascript" src="/assets/resources/jquery/jquery.min.js"></script>
	<script type="text/javascript" src="/assets/resources/bootstrap/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>

	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">  
MathJax.Hub.Config({  
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});


</script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-38327934-3', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>

