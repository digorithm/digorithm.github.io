<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rodrigo Araujo</title>
    <description>Thoughts on computer science, software engineering, distributed systems, and machine learning</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 10 Aug 2019 00:04:38 +0000</pubDate>
    <lastBuildDate>Sat, 10 Aug 2019 00:04:38 +0000</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>From synchronous service calls to message-passing dataflow systems</title>
        <description>&lt;p&gt;In this past year I have been working on a &lt;em&gt;big&lt;/em&gt; system that is fundamentally an ETL to process real estate data across the US and Canada. I can’t talk much about the business details, but let’s say that if you’re browsing for real estate properties, it’s very likely you’re using this system. You’re welcome. Or I’m sorry, who knows?&lt;/p&gt;

&lt;p&gt;I joined this project from the very beginning. When I joined, a couple of technical decisions had been made. Initially, before I’d arrived on this project, the microservices would talk to each other directly, REST/gRPC style. Nothing new here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/2019-07-11_19-154e43ee-0c5f-4866-9efc-7af7ac54e7f4.22.05.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Simple architecture where the services talk to each other directly&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The great majority of these services communicated with each other through synchronous calls. In most scenarios, this would be 100% fine. In an ETL constantly streaming copious amounts of data that need fast processing though? Not so much.&lt;/p&gt;

&lt;p&gt;One big problem with this architecture is that this application isn’t an “user-triggered” application and the events happens in a streaming fashion, i.e this is a streaming system, synchronous calls then quickly become a bottleneck. For instance: to process property X we’d need to wait for all other services to do their jobs before moving onto the property X + 1.&lt;/p&gt;

&lt;p&gt;And because of that property (streaming system), it means we can go full asynchronous here; we want to stream data in a constant flow and each service will do its job independently.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/2019-07-12_15-15a5f630-c173-4a14-ab93-ee1bf2ca5092.07.25.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Direct communication through an API gateway vs. streaming system using message-passing (fully async)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, an unfortunate bad technical decision had been made before I arrived on this project: yes, they would go from sync requests to async requests, but they would do it through a service developed internally called &lt;code class=&quot;highlighter-rouge&quot;&gt;Queuer&lt;/code&gt;, which, in hindsight, was nothing like a queue. Queuer would just take the request coming from one service, buffer it, and route it to another service (again, not a queue), which sounded more like what you’d get from a service mesh, but without all the reliability that you’d get from battle-tested robust tools. We had something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/2019-07-11_19-9c20194e-2150-4c4a-b12b-89bd5224e801.28.54.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;A queueing system that acts exactly like an API gateway&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We had many problems with this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Maintaining a “queueing” system built in-house was terrible.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Queuer&lt;/code&gt; wasn’t service-agnostic; we had to encode domain knowledge about other services inside &lt;code class=&quot;highlighter-rouge&quot;&gt;Queuer&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;My favourite: Queuer had a buffer, which was simply a in-memory variable. It would buffer thousands of jobs there. If queuer died, well, the jobs were lost. Oh and forget about deliver-once guarantees and/or other goodies you can get by using a battle-tested queueing system such as RabbitMQ or Kafka.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;About 3 months into the project I convinced people to move away from Queuer, it had become too much of a hassle to maintain and evolve it. Godspeed &lt;code class=&quot;highlighter-rouge&quot;&gt;Queuer&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Also, around that time I was reading &lt;code class=&quot;highlighter-rouge&quot;&gt;designing high intensive data applications&lt;/code&gt;. It advocates – with very good arguments – for what’s called Message-Passing Dataflow. I’d seen this somewhere else with the name event-centric architecture or something like that; but I believe the philosophies between them are very similar. I like the simple way the author explains it, so here it is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Composing stream operators into dataflow systems has a lot of similar characteristics
to the microservices approach. However, the underlying communication mechanism is very different: one-directional, asynchronous message streams rather than
synchronous request/response interactions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I then decided to experiment with Message-Passing Dataflow; after all, we’re talking about a never-ending stream processing system. Therefore, I didn’t want much synchronous communication between the services and wanted something that’s closer to an actual streaming system.&lt;/p&gt;

&lt;p&gt;To achieve this, we need a central component that would act as a communication bus, streaming messages between the services. And the services, instead of being just HTTP rest APIs, would be active consumers, consuming messages from this bus.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/2019-07-12_15-542f22e7-5cae-4f21-aabe-d232f4f56360.27.22.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Services consume messages being passed to the channels they’re subscribed to&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Not surprisingly, I decided to use Kafka as our communication bus. The services around Kafka are Kafka consumers and producers. All communication between the services happens through &lt;em&gt;message passing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Here are some observations after adopting this paradigm:&lt;/p&gt;

&lt;h4 id=&quot;async-usually-means-that-it-will-be-faster&quot;&gt;Async usually means that it will be faster&lt;/h4&gt;
&lt;p&gt;I noticed that this approach is faster that what we had before. A service does one thing and does it as fast as it can and don’t have to wait for the next steps. Then, it might produce a message to Kafka, which will be a job being performed by other services, and that’s it.&lt;/p&gt;

&lt;h4 id=&quot;more-robustness&quot;&gt;More robustness&lt;/h4&gt;
&lt;p&gt;This approach is much, much more robust. All services are completely stateless and they don’t hold jobs in memory. The service died? All good, the job can be easily recovered from Kafka, Kafka only moves its offsets when the offsets are commited, i.e when the service actually finishes the job.&lt;/p&gt;

&lt;h4 id=&quot;you-must-think-about-how-you-handle-the-offsets&quot;&gt;You must think about how you handle the offsets&lt;/h4&gt;
&lt;p&gt;Even though it’s robust, consumers must be smart about it. Consuming offsets from topics (think of it as jobs in a queue) and failing to commit the new offset properly can be catastrophic. For instance, if we have a Kafka producer’s &lt;code class=&quot;highlighter-rouge&quot;&gt;auto.commit&lt;/code&gt; enabled, the service reads a message from a topic, the auto commit mechanism commits after a few milliseconds, but then the process fail. That means we marked that offset as consumed, but we didn’t finished the job, so it’s… well, lost. Manually committing the offsets is much more reliable in high risk cases.&lt;/p&gt;

&lt;h4 id=&quot;a-different-and-refreshing-computational-model&quot;&gt;A different and refreshing computational model&lt;/h4&gt;
&lt;p&gt;It’s a very different computational model. Instead of proactively querying things from other services, we &lt;em&gt;subscribe&lt;/em&gt; to channels of messages (called topics) and work our way through the messages in there.
It’s closer to Alan Kay’s view of object-oriented programming (&lt;a href=&quot;https://ovid.github.io/articles/alan-kay-and-oo-programming.html&quot;&gt;https://ovid.github.io/articles/alan-kay-and-oo-programming.html&lt;/a&gt;). And I’m not talking about &lt;em&gt;that&lt;/em&gt; OOP created by the C++/Java community, but the core ideas of OOP thought by Kay and colleagues back in the early days of computing – which curiously was grounded in Biology. The idea of passing messages to objects in order to better scale systems (like biological cells do!). Kafka is nothing but a (very reliable) communication bus, and the objects (services, microservices, whatever) attach themselves to it and listen to messages, responding accordingly and sending other kinds of messages back to kafka which will be consumed by other services. Once you have well-defined messages and protocols, things flow beautifully.&lt;/p&gt;

&lt;h4 id=&quot;watch-out-for-waste&quot;&gt;Watch out for waste&lt;/h4&gt;
&lt;p&gt;Be careful with idle consumers, that means you gotta think about your partitions carefully. What’s worked for us is to set 1:1 partition-to-consumer ratio. For instance, we have an image processing service that fetches jobs from kafka. If we have more instances of that services than partitions, that means some of those instances will be idle, preventing us from achieving a higher throughput than we could.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Closing thoughts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I am, now, a big fan of this architectural style I can totally recommend &lt;em&gt;if it fits your problems&lt;/em&gt;. Which in most cases it does. I believe we’ve been going with the flow that was set in the early 00s when it comes to architecting systems; we’ve been using the request-response paradigm without really questioning its efficacy.&lt;/p&gt;

&lt;p&gt;To close, the author of &lt;code class=&quot;highlighter-rouge&quot;&gt;designing data-intensive applications&lt;/code&gt; has some really cool insights about this paradigm:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It would be very natural to extend this programming model to also allow a server to
push state-change events into this client-side event pipeline. Thus, state changes
could flow through an end-to-end write path: from the interaction on one device that
triggers a state change, via event logs and through several derived data systems and
stream processors, all the way to the user interface of a person observing the state on
another device. These state changes could be propagated with fairly low delay—say,
under one second end to end.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Some applications, such as instant messaging and online games, already have such a
“real-time” architecture (in the sense of interactions with low delay, not in the sense
of “Response time guarantees”). &lt;strong&gt;But why don’t we build all applications
this way?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The challenge is that the assumption of stateless clients and request/response interac‐
tions is very deeply ingrained in our databases, libraries, frameworks, and protocols.
Many datastores support read and write operations where a request returns one
response, but much fewer provide an ability to subscribe to changes—i.e., a request
that returns a stream of responses over time.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;In order to extend the write path all the way to the end user, we would need to funda‐
mentally rethink the way we build many of these systems: moving away from request/
response interaction and toward publish/subscribe dataflow. I think that the
advantages of more responsive user interfaces and better offline support would make
it worth the effort. If you are designing data systems, I hope that you will keep in
mind the option of subscribing to changes, not just querying the current state.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My next steps? Experimenting with this architecture for applications where the events &lt;em&gt;are&lt;/em&gt; user-triggered and the results are near real-time and user-facing.&lt;/p&gt;
</description>
        <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/message-passing-dataflow</link>
        <guid isPermaLink="true">http://localhost:4000/post/message-passing-dataflow</guid>
        
        <category>architecture</category>
        
        <category>software</category>
        
        <category>thoughts</category>
        
        <category>systems</category>
        
        
        <category>system-design</category>
        
      </item>
    
      <item>
        <title>On machine learning enhanced software systems</title>
        <description>&lt;p&gt;I have been brewing the idea of using machine learning to improve software systems since 2016. It was pretty vague and broad, without an actionable plan. I just had the intuition — the software configuration and tuning, especially after the adoption of microservices, was getting too complex.&lt;/p&gt;

&lt;h3 id=&quot;the-increasing-complexity-of-configuring-and-tuning-systems&quot;&gt;The increasing complexity of configuring and tuning systems&lt;/h3&gt;
&lt;p&gt;If you have enough experience in the software industry, then it’s very likely that you’ve struggled with either a configuration problem or a tuning problem.&lt;/p&gt;

&lt;p&gt;Configuration and tuning problems are pretty common and can lead to really bad outages. They often occur when:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Some parts of the system are poorly or wrongly configured, or&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A configuration that worked before now doesn’t work because the context of the system has changed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Think of a number of database replicas and their writing schemes. Or in Postgresql, think of the number of shared buffers, effective cache size, and the min and max wal size.&lt;/p&gt;

&lt;p&gt;If wrongly configured from the start, it won’t work in the given context, plain and simple. What’s more interesting, though, is if it’s &lt;em&gt;correctly&lt;/em&gt; configured, it might work at a given time. But as the context changes — system workload, system resources usage, overall system architecture — the system will behave poorly. Or, even worse, an outage might happen.&lt;/p&gt;

&lt;p&gt;This will, inevitably, lead to manually-performed operations and the creation of heuristics. In other words, it will lead to:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Oh, we should set X to A, when workload is T, but it should be A+10 when workload is T+100 and we have system resources usage above 80%… I guess. Or maybe let’s just up a queue in front of this component, queues solve everything, right?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now multiply this scenario by tens or hundreds of services. Think for a second about the cognitive burden resulting from these configurations.&lt;/p&gt;

&lt;p&gt;This is not a new concern. In 2003, Ganek and Corbi &lt;a href=&quot;http://ieeexplore.ieee.org/document/5386835/?reload=true&quot;&gt;discussed&lt;/a&gt; the need for autonomic computing to handle the complexity of managing software systems. They noted that managing complex systems became too costly, labor-intensive, and prone to error due to the pressure engineers felt while maintaining them. This increased the potential of system outages with a concurrent impact on business.&lt;/p&gt;

&lt;p&gt;Even nowadays, most of the configurations and tuning of the systems are performed manually, often in run-time, which is known to be a very time-consuming and risky practice. Check out these two links (&lt;a href=&quot;https://link.springer.com/book/10.1007/978-3-642-35813-5&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.8651&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;here&lt;/a&gt;) to read more about it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2000/1*6Kh0EXVHQ9zmau8kLs4pzQ.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-need-for-autonomic-computing&quot;&gt;The need for autonomic computing&lt;/h3&gt;

&lt;p&gt;Most decisions to configure and tune the system are made based on the context — there are many different variables such as workload, number of instances of some services, resources usage, and more. So why not delegate these tasks to something that excels at exactly that? &lt;em&gt;Machine learning sounds like a feasible tool for the job.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;After starting my Masters at the University of British Columbia, I kept working on this idea. It seemed interesting although quite weird, and, sometimes, unpractical and impossible to implement.&lt;/p&gt;

&lt;p&gt;To my surprise, I realized I wasn’t alone. Some very interesting people were working on these ideas — so it might not be that weird, unpractical, and impossible.&lt;/p&gt;

&lt;p&gt;Recently, Jeff Dean — a man that I admire a lot — &lt;a href=&quot;https://news.ycombinator.com/item?id=15892956&quot;&gt;gave a talk at NIPS 2017 talking about machine learning for systems&lt;/a&gt;, where he stated:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Learning should be used throughout our computing systems. Traditional low-level systems code (operating systems, compilers, storage systems) does not make extensive use of machine learning today. This should change!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Computer Systems are filled with heuristics: compilers, networking code, operating systems. Heuristics have to work well “in general case”. [They] generally don’t adapt to actual pattern of usage and don’t take into account available context&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Learning in the core of all of our computer systems will make them better/more adaptive.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I was in complete awe when I read this. One of the engineers I admire the most was talking about the very same ideas I’ve been thinking about and working on.&lt;/p&gt;

&lt;p&gt;This led me to think that it’s not only interesting but &lt;strong&gt;natural to think about enhancing software systems with machine learning.&lt;/strong&gt; Throughout the whole software stack, we have many heuristics that, although they work well, could be improved by machine learning.&lt;/p&gt;

&lt;p&gt;Is it challenging and potentially risky? Yes, most definitely. Especially given that interpretability, apparently, has become a secondary goal in the machine learning community. How can we interpret and explain the decisions made by neural nets?&lt;/p&gt;

&lt;p&gt;However, with that said, these obstacles shouldn’t hinder scientific and technological progress. &lt;a href=&quot;https://arxiv.org/pdf/1712.01208.pdf&quot;&gt;Yes, we should question old paradigms &lt;/a&gt;and try to improve things.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2000/1*puiL2EVDE6Ztlocw3JD1uQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;towards-machine-learning-enhanced-software-systems&quot;&gt;Towards machine learning-enhanced software systems&lt;/h3&gt;

&lt;p&gt;As Jeff Dean pointed out: we need to find &lt;strong&gt;practical&lt;/strong&gt; ways to make systems data-aware. We need systems that collect metrics and metadata about themselves. To achieve this, we could learn a thing a two from the ideas in systems observability and instrumentation. We have been instrumenting systems for decades, and the data is already there.&lt;/p&gt;

&lt;p&gt;We also need to find &lt;strong&gt;practical&lt;/strong&gt; and &lt;strong&gt;clean&lt;/strong&gt; ways to &lt;strong&gt;integrate&lt;/strong&gt; machine learning components into software systems, making learning a first-class citizen in the system. This will lead to &lt;strong&gt;systems that learn how to improve themselves,&lt;/strong&gt; beating heuristics and manually-performed operations. Think about this for a second. It does sound cool &lt;em&gt;and&lt;/em&gt; feasible.&lt;/p&gt;

&lt;p&gt;I would also add that we need &lt;strong&gt;practical&lt;/strong&gt; and &lt;strong&gt;clean&lt;/strong&gt; ways to propagate the decisions made by the learned models to the rest of the system. This would allow the system to have self-adaptive capabilities. Here, we could learn something from the control theory community.&lt;/p&gt;

&lt;p&gt;The general idea is fairly simple: make a system learn about its behavior by training a model on its context. Then allow it to change its structures and configurations in order to optimize for a certain scenario. Now implement this idea in such a way that it could be possible to integrate it into many kinds of systems.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;The most interesting questions I have in mind are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Can self-adaptation by learned models lead to more stable, faster, safer software systems? Can it reduce the need for manually configuring and tuning systems, allowing engineers to focus on more important tasks?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Can this be easily integrated into software systems, requiring only small changes to the codebase?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Can this work with low overhead?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is worth noting that this &lt;strong&gt;would not&lt;/strong&gt; replace good engineers, but would rather free the engineers’ cognitive abilities to focus on what matters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.sysml.cc/&quot;&gt;I genuinely believe that this will become a trend in the next few years&lt;/a&gt;. I myself am working on these ideas as part of my graduate studies, and I will be posting the results of my research, so &lt;a href=&quot;https://twitter.com/digorithm&quot;&gt;stay tuned&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/machine-learning-enhanced-software</link>
        <guid isPermaLink="true">http://localhost:4000/post/machine-learning-enhanced-software</guid>
        
        <category>architecture</category>
        
        <category>software</category>
        
        <category>machine-learning</category>
        
        <category>thoughts</category>
        
        <category>systems</category>
        
        
        <category>software-architecture</category>
        
        <category>systems</category>
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>Thoughts on visualizing software architecture</title>
        <description>&lt;p&gt;The more I think about the advantages and disadvantages of upfront design, the more I find it complex, I mean, the whole software architecture diagramming in general. I’ve been following the development of the &lt;a href=&quot;http://www.codingthearchitecture.com/2014/08/24/c4_model_poster.html&quot;&gt;C4 model&lt;/a&gt; for a considerable time, I find it interesting, but it still brings me the old struggles about how much detail we should put into the diagrams and whether we should do it while planning the software, while developing it, or after.&lt;/p&gt;

&lt;p&gt;I believe that when drawing architecture diagrams we should focus on getting the right purpose and the right level of abstraction.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;I do think that the diagram should reflect the code in certain level of abstraction and that this could be really useful for a new engineer in a large project. Tackling a huge codebase is not an easy task, and the diagrams should act as a map of the code, saying how things communicate with each other.&lt;/p&gt;

&lt;p&gt;Perhaps what we need is different types of architecture diagrams, each one for a specific development phase. What comes to my mind is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Project planning phase&lt;/strong&gt;: we can’t code without any upfront planning, at this level I feel necessary to draw the most &lt;em&gt;visible&lt;/em&gt; containers and components, this way we can plan what to do next, how to split up teams to each part of the system. It’s when a deployment architecture can be written as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Project evolution and maintenance&lt;/strong&gt;: as we reach this phase of the project, it’s interesting to see the Big Picture, the Not-So-Big Picture and the Small Picture. I can see many cases where this could be helpful, for instance: new engineers coming to the project, debugging something that crosses many components and containers, and I’m sure there are many more.&lt;/p&gt;

&lt;p&gt;Given this task, I’d prefer taking a top-down approach, going from Big Picture architecture to a more detailed one. As an example, I’m going to use a side project I was working on, which is a simple webapp to search for simple food recipes.&lt;/p&gt;

&lt;p&gt;I followed a backend + client side app approach: in the backend is where the main logic happens, the core business classes and everything, in addition to exposing a simple REST API. Client side just consumes the backend data through its API.&lt;/p&gt;

&lt;p&gt;Each of these 2 parts will be a different application, which can be dockerized and run in different plataforms – two separate amazon EC2, for example. So if we’re planning it, we could start by saying:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/gdd20overview.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The client will talk to the backend’s api over HTTP, the backend will communicate to the client and to a database instance. Ok, that’s a start. We can easily separate a team for the backend and its api and another team for the web client.&lt;/p&gt;

&lt;p&gt;Still, it doesn’t say much about the inner details of each container, and now we need something more detailed. So we break it down a bit more:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/gdd20arch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, it’s the same structure, but with way more details, which makes it clearer to everybody how we can tackle the problem with actual coding. An extra level of detail would be going down one more level of abstraction and describe the classes through a class diagram, explaining how the core backend would be implemented, for example.&lt;/p&gt;

&lt;p&gt;Now, this diagram, plus all requirements gathered can be a nice start for the implementation phase. We can add now how we would deploy it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/gdd20deploy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I believe those 3 diagrams plus the codebase could be helpful for efficient communication.&lt;/p&gt;

&lt;p&gt;Though I still have many open questions about this, e.g: the cost of this upfront design is a concern,so I’m open to discussions.&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/thoughts-visualizing-software-arch</link>
        <guid isPermaLink="true">http://localhost:4000/post/thoughts-visualizing-software-arch</guid>
        
        <category>architecture</category>
        
        <category>software</category>
        
        <category>visualization</category>
        
        <category>thoughts</category>
        
        
        <category>software-architecture</category>
        
      </item>
    
      <item>
        <title>Neural Computation: An Intuitive Understanding of the Perceptron</title>
        <description>&lt;p&gt;Artificial Neurons is one of the most beautiful ways to simulate a biological behavior through computation, despite the fact that it’s not very close to the level of details of a real neuron. But it captured the core of what a neuron is doing.&lt;/p&gt;

&lt;p&gt;And I find that trying to understand this mathematical method by first understanding the concept through a metaphor &lt;em&gt;(which, in this case, I think that the metaphor is our mathematical side, and not the biological one, the biological is just, you know, the real thing)&lt;/em&gt; is really valuable to gain an intuition on this topic.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;If you were about to make a decision, given many variables, how would you make this decision? It may not look obvious, due the fact that, on normal occasions, we’re not thinking about our thinking process, but the truth is, we are collecting those variables that affect the future decision, and we’re giving weights to them. Weights? how? Simple, some variables are more crucial than other while making decision, isn’t it? the word &lt;em&gt;‘crucial’&lt;/em&gt;, in this case, means a huge weight on this variable, and it will strongly affect the final decision.&lt;/p&gt;

&lt;p&gt;Suppose a scenario: we’re deciding whether we should go to beach or not. Let’s put a few binary variables on table:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Are we in the mood to go to the beach?&lt;/li&gt;
  &lt;li&gt;Is it raining?&lt;/li&gt;
  &lt;li&gt;Our other friends are going?&lt;/li&gt;
  &lt;li&gt;Do we have money?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, depending on those variables, it will be more likely that we will go to the beach… or not. Let’s think about the variable &lt;em&gt;‘is it raining?’&lt;/em&gt;, if it’s raining, we can say that it’s a deal breaker, so we can conclude that this variable has more weights than the other. The fact is that we already trained those weights inside our brain, long time ago.&lt;/p&gt;

&lt;p&gt;That’s what an Artificial Neuron try to do, the AN try to learn the weights of things so it can make decisions. So, drawing back to the mathematical and computational aspect of it, the perceptron is a single unit that will receive external inputs&lt;em&gt;(variables)&lt;/em&gt;, it will have a weight for each input, it will do some computation, send this result to a decision function, and, finally, output the final decision.&lt;/p&gt;

&lt;p&gt;Those inputs are the variables we’re talking before, and like our process to make a decision, the Perceptron will weight each input to make a decision.&lt;/p&gt;

&lt;p&gt;Now, how the Perceptron knows how the weights should be for each input? Well… it doesn’t. At least, not from the beginning of the learning process, just like you and me, when trying to learn something new.&lt;/p&gt;

&lt;p&gt;That’s why this case is a case of supervised learning, what the Perceptron will do is: start with a random small weights, take one previously trained example &lt;em&gt;(e.g: (1) yes to mood to beach, (2) not raining, (3) yes to friends going to beach and (4) yes to money, the output: 1 - yes, we shall go to the beach)&lt;/em&gt; do some computation taking in consideration the random weights we defined before, send it to a decision function, output something &lt;em&gt;(1 - yes, 0 - no)&lt;/em&gt; and check if this output is equal to the expected &lt;em&gt;(we’re using a trained example, remember?)&lt;/em&gt;, of course that at the first try, it won’t be equal. So the Perceptron calculate the error rate and update its weights… after this, guess what? it repeat the process of trying to predict, but now, with the updated weights, after a few tries, the error rate tends to reduce and it starts predicting correctly. So it learned to predict a behavior, by training with previously trained examples.&lt;/p&gt;

&lt;p&gt;So, a nice learning behavior would be something like this:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;/content/images/images/perceptron_0.1.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Which means, at every iteration &lt;em&gt;(we call it epochs)&lt;/em&gt;, the error rate tend to decrease and, as consequence, the Perceptron starts to predict correctly!&lt;/p&gt;

&lt;p&gt;As you may have noted, I omitted a few details of the process so you could see the whole picture of the process: we take an example, we practice on it, we see what we missed, we try again, we learn. That’s the core process.&lt;/p&gt;

&lt;p&gt;Now, to the details:&lt;/p&gt;

&lt;h3 id=&quot;the-computation&quot;&gt;The computation&lt;/h3&gt;

&lt;p&gt;The first step that the Perceptron does it’s a computation that I referred as &lt;em&gt;“some computation”&lt;/em&gt;, this is a simple computation, it’s just a simple Linear Combination of the feature vector &lt;em&gt;(the variables)&lt;/em&gt; and the weight vector, which is just the sum of the products between feature/input and its respective weight:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}+ \cdots +x_{n}w_{n}&lt;/script&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;the-decision-function&quot;&gt;The Decision Function&lt;/h3&gt;
&lt;p&gt;So, after the Linear Combination, we send the result of it to a decision function, which will, somehow, based on some threshold &lt;em&gt;(or without a threshold as we’re going to see)&lt;/em&gt;, give us the output, that will be checked with the correct output, in case if it’s correct, fine, keep the weights like this, otherwise, it will calculate the error rate, adjust the weights and restart the process.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;binary-output-with-threshold&quot;&gt;Binary output with threshold&lt;/h4&gt;

&lt;p&gt;This technique is very simple, after the linear combination, if the output is bigger than some threshold, it outputs 1 and we say that the neuron was activated, otherwise, output 0.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
    X=
    \begin{cases}
      1, &amp; \text{if}\ Linear\,Combination&gt;0 \\
      0, &amp; \text{otherwise}
    \end{cases}
  \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;using-sigmoid-function&quot;&gt;Using sigmoid function&lt;/h4&gt;

&lt;p&gt;Now, that a interesting one, it won’t use a threshold anymore, we’ll send the output of the Linear Combination to a sigmoid function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(\vec{w}\vec{x}) = \dfrac{1}{1+e^{-\vec{w}\vec{x}}}&lt;/script&gt;

&lt;p&gt;which will, then, smooth the output, making it in the range of 0 and 1. Which is the most used in many Machine Learning Algorithms.&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;adjusting-the-weights-and-learning&quot;&gt;Adjusting the weights and learning&lt;/h3&gt;

&lt;p&gt;After the perceptron fails to predict correctly, it’s time to adjust the weight, using some rule, the classic perceptron has 2 rules:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;perceptron-learning-rule&quot;&gt;Perceptron Learning Rule&lt;/h4&gt;
&lt;p&gt;This is very simple, when the perceptron has the incorrect output, it update its weights following this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{i} = w_{i} + \eta (t_{i} - o_{i})x_{i}&lt;/script&gt;

&lt;p&gt;Which is simply multiplying a learning rate &lt;em&gt;(usually 0.001)&lt;/em&gt; by the difference between the correct output and the wrong output and then multiplying it by its original input, after this, we add this value to the previous weight and then we have the value of the new weight. 
It’s fair simple, isn’t it? The problem arises when the data isn’t linearly separable, like this:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/DBSCAN-density-data.svg/2000px-DBSCAN-density-data.svg.png&quot; width=&quot;400&quot; /&gt;
&lt;/div&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;With this scenario, the result just won’t converge. So we adopt another rule!&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;delta-rule&quot;&gt;Delta Rule&lt;/h4&gt;

&lt;p&gt;Now we must find a way to have a non-linear output, and what’s the best for this if not the classic Gradient Descent algorithm? GD will simply search through hypothesis spaces and try to minimize the cost function, I wrote about this &lt;a href=&quot;http://rodrigoaraujo.me/general/setup/demo/2015/01/09/Making-Your-Machine-Think-Learn-And-Predict--Gradient-Descent-Algorithm-in-Java.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;http://www.yaldex.com/game-development/FILES/17fig06.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;So, it’s just a technique to solve our previous problem, but still, the weights will be updated &lt;em&gt;(now, using the GD)&lt;/em&gt; and then the process start over!&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;code-of-a-perceptron&quot;&gt;Code of a Perceptron&lt;/h3&gt;

&lt;p&gt;Now, here’s a code of a Perceptron that will behave like a simple Boolean function. I didn’t use the Delta Dule (Gradient Descent) to optimize the weights, as this problem (binary Boolean functions) is linearly separable.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;
  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;
  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
      
      &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
      this is the thereshold to activate the unit 
      &quot;&quot;&quot;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear_combination&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;
      
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;unit_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_combination&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;# initializing weights
&lt;/span&gt;          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
          
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# &amp;gt; calculate output with current weight
&lt;/span&gt;              &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unit_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# &amp;gt; calculate error rate (t - o)
&lt;/span&gt;              &lt;span class=&quot;n&quot;&gt;error_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
              &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'the X: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'output: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'correct target: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# &amp;gt; apply learning rule which will update weights
&lt;/span&gt;              &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unit_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
                  

          &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  
          &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'errors'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'epochs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;conclusion-and-next-steps&quot;&gt;Conclusion and next steps&lt;/h3&gt;

&lt;p&gt;With this we conclude what a simple single Perceptron is doing! Of course there are many improvements on it and we can connect many of these Artificial Neurons in many layers… that’s what is called Artificial Neural Network, I’ll be writing about it soon!&lt;/p&gt;

</description>
        <pubDate>Thu, 16 Jul 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/neural-computation-pt1</link>
        <guid isPermaLink="true">http://localhost:4000/post/neural-computation-pt1</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>engineering</category>
        
        <category>tutorial</category>
        
        <category>text classification</category>
        
        <category>NLP</category>
        
        <category>Sentiment Analysis</category>
        
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>Case Study: Sentiment Analysis On Movie Reviews</title>
        <description>&lt;p&gt;Well, this case was a fun one, sentiment analysis is a sub type of NLP &lt;em&gt;(natural language processing)&lt;/em&gt; in which you have to train a model to analyze a text, and classify it was a negative sentiment or a positive sentiment. In this case, I wanted to apply it on movies reviews, to see if a review is a negative one or a positive one. So, basically, teaching this kind of sentiment to the machine.&lt;/p&gt;

&lt;p&gt;There’s a lot of challenges in doing this kind of NLP, experience has told me that when the text is big, it is usually hard to predict things about it… and most of the times, movie reviews are big chunk of text.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;That’s the case where I needed to learn and use a few &lt;em&gt;extremely&lt;/em&gt; useful things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pipeline&lt;/li&gt;
  &lt;li&gt;Grid Search&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These two techniques made total difference in my results. Let’s go through the concept of these two:&lt;/p&gt;

&lt;h3 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h3&gt;

&lt;p&gt;In a &lt;a href=&quot;http://rodrigoaraujo.me/general/setup/demo/2015/06/30/A-Generic-Architecture-for-Text-Classification.html&quot; target=&quot;_blank&quot;&gt;previous post&lt;/a&gt; I talked about a generic Architecture or Flow to achieve basic text classification tasks, so, Pipeline is a way to automate all those tasks, so, instead of explicitly and separately select features, extract features, train the learning algorithms, you can do all of these things inside the pipeline, and, the amazing scikit learn API gives you an awesome Pipeline interface, pretty similar to a basic estimator’s interface, so you can call methods such as &lt;em&gt;fit, fit_transform, predict&lt;/em&gt;, etc…&lt;/p&gt;

&lt;p&gt;So you can do even more inside a pipeline, such as new methods for feature extraction/selection, FeatureUnion, use other estimators to select better features for your estimator &lt;em&gt;(do you even neural netception? LOL).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here’s an example of a simple pipeline that does PCA to reduce the dimension of the features and creates a SVC:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    from sklearn.pipeline import Pipeline
    from sklearn.svm import SVC
    from sklearn.decomposition import PCA
    estimators = [('reduce_dim', PCA()), ('svm', SVC())]
    clf = Pipeline(estimators)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;grid-search&quot;&gt;Grid Search&lt;/h3&gt;

&lt;p&gt;This was the answer for many hours of pure pain tweaking gazillions of parameters to improve the algorithm’s performance. Grid Search is a way to automate parameters changes, so it can run many times with many different parameters… and choose the best for you, magical, right?&lt;/p&gt;

&lt;p&gt;And, once again, the Grid Search object has an interface similar to the basic estimator’s interface, so you can call all the same methods. But, when you create the object grid search, you gotta pass a estimator as parameter… and here’s when things get interesting:&lt;/p&gt;

&lt;p&gt;You can pass a pipeline to the grid search. &lt;strong&gt;so much win!&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;http://cdn.meme.am/instances2/500x/581044.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;But, the truth is: this operation is &lt;em&gt;extremely&lt;/em&gt; expensive, and it’s recommended to use it only on the few first tries, where you don’t know exactly the best parameters values for the task.&lt;/p&gt;

&lt;p&gt;Another interesting thing is, after the model inside the grid search is trained, methods like &lt;em&gt;predict&lt;/em&gt; are computed using the best features. If we look inside the SKlearn’s grid search’s code, we can see how easily and elegant it’s done:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_start&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_fits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_folds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_test_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;all_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_n_test_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_folds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;all_scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;this_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_n_test_samples&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;n_test_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_n_test_samples&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_score&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_test_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_folds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# TODO: shall we also store the test_fold_sizes?
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;grid_scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_CVScoreTuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Store the computed scores
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_scores_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_scores&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Find the best parameters by comparing on the mean validation score:
# note that `sorted` is deterministic in the way it breaks ties
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_validation_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_score_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_validation_score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, in the end it keeps the best parameters and scores, so it can be used when it needs to predict or output its &lt;em&gt;predict_proba&lt;/em&gt; or &lt;em&gt;decision_function&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;correctly-mixing-pipeline-grid-search-and-cross-validation-correctly-the-hidden-wizardry&quot;&gt;Correctly mixing Pipeline, Grid Search and Cross Validation correctly: the hidden wizardry&lt;/h3&gt;

&lt;p&gt;It’s tricky to mix all these things up, the most dangerous mistake that everybody does sometime is: &lt;strong&gt;using the same dataset to the grid search AND the cross validation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So, here’s a nice technique to avoid this: Split the initial dataset into Development Dataset, which will be used to train the algorithm and to execute the grid search, and the Validation Dataset, which will be passed to the cross_val_score and internally splitted again to avoid bias (CV parameter can configure this).&lt;/p&gt;

&lt;p&gt;So, in the end, you train the grid search with a subset of the dataset, and validate it with another subset, avoiding many kind of undesired effects.&lt;/p&gt;

&lt;h3 id=&quot;the-movie-review-problem&quot;&gt;The Movie Review Problem&lt;/h3&gt;

&lt;p&gt;Ok, so now let’s head to the real problem. The dataset can be downloaded with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz
tar xzf review_polarity.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What I did to solve it was: I used grid search to search a few parameters, such as &lt;em&gt;TfidfVectorizer’s max_features&lt;/em&gt;, that in the end I kept with max_features=10000, also its ngram_range, searching between (1,1) and (1,2) and &lt;em&gt;the LinearSVC()’s C parameter&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I used a few other algorithms, but I found the best performance with LinearSVC, though I did not try all the other possibilities &lt;em&gt;(I could create many pipelines to test many algorithms, but my computer would be unusable during many hours, probably, which I couldn’t afford)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Why Linear SVC?&lt;/p&gt;

&lt;p&gt;The Linear SVC is very similar to the SVC object but using the &lt;em&gt;kernel = “linear”&lt;/em&gt;. LinearSVC is a type of &lt;em&gt;Support Vector Machines&lt;/em&gt;, it’s known that SVMs are effective in &lt;strong&gt;high dimensional spaces&lt;/strong&gt;, which is our case here. Also, it can give weights to classes, helping to go through unbalanced datasets
&lt;img src=&quot;http://scikit-learn.org/stable/_images/plot_separating_hyperplane_unbalanced_0011.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And, basically, what the SVMs does is create a hyper-plane (or a set of it) in a high or even infinite dimensional space. And it solves the primal problem:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;http://scikit-learn.org/stable/_images/math/396704acdf11cc18d2d02b32275c0ee42d76b95e.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Where Xi are the training vectors and Y is a vector in {1,-1}^n.&lt;/p&gt;

&lt;p&gt;So, here’s the code where I built the pipeline, the gridsearch, use a subset of the dataset to develop the grid search and another subset to validate using the &lt;em&gt;cross_val_scores&lt;/em&gt;, then, I call &lt;em&gt;predict&lt;/em&gt; to get the vector of predictions to build a report &lt;em&gt;(or, with this, I can start to build a ROC curve, confusion matrix and other statistics debug tools)&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from sklearn.datasets import load_files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.grid_search import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC
from sklearn.cross_validation import train_test_split
from sklearn.cross_validation import cross_val_score
from sklearn.metrics import classification_report

data = load_files('../txt_sentoken/')

X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.5, random_state=0)

pipeline = make_pipeline(TfidfVectorizer(sublinear_tf=True, max_features=10000), LinearSVC())

parameters = {
        'tfidfvectorizer__ngram_range': [(1,1), (1,2)],
        'linearsvc__C':(.01,.1,1),
        }

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1) 

grid_search.fit(X_train, y_train)

print(&quot;Best score: %0.3f&quot; % grid_search.best_score_)
print(&quot;Best parameters iset:&quot;)
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print(&quot;\t%s: %r&quot; % (param_name, best_parameters[param_name]))

print &quot;The model was trained on the full development set&quot;
print &quot;the scores are going to be computed with the evaluation set&quot;
scores = cross_val_score(grid_search, X_test, y_test, cv=5)

print scores.mean(), scores.std()

y_pred = grid_search.predict(X_test)

print classification_report(y_test, y_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and the output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Best score: 0.849

Best parameters iset:
	linearsvc__C: 1
	tfidfvectorizer__ngram_range: (1, 2)

The model was trained on the full development set
the scores are going to be computed with the evaluation set
0.865933623341 0.0281523948704

Classification report:
             precision    recall  f1-score   support

          0       0.88      0.86      0.87       496
          1       0.87      0.88      0.87       504

avg / total       0.87      0.87      0.87      1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, around 86%~87% of precision/f1-score, not &lt;em&gt;that&lt;/em&gt; bad.&lt;/p&gt;

&lt;h3 id=&quot;further-improvements&quot;&gt;Further Improvements&lt;/h3&gt;

&lt;p&gt;There are many things I could to do elevate this performance, but due the lack of time to play with this dataset, those improvements will be in a next post.&lt;/p&gt;

&lt;p&gt;Possible improvements could be: find and extracting new features, to do this, we gotta understand better this data, extract more information about it. Trying new algorithms and techniques such as Ensemble, Classifiers Combination or Fusion. Stemming and Stop Words could improve it too. If you solved this problem and got a better result, share it with me, I’m very curious about how to improve this!&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jul 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/case-study-sentiment-analysis-movie-reviews</link>
        <guid isPermaLink="true">http://localhost:4000/post/case-study-sentiment-analysis-movie-reviews</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>engineering</category>
        
        <category>tutorial</category>
        
        <category>text classification</category>
        
        <category>NLP</category>
        
        <category>Sentiment Analysis</category>
        
        
        <category>machine-learning</category>
        
        <category>case-study</category>
        
      </item>
    
      <item>
        <title>A Generic Architecture for Text Classification with Machine Learning</title>
        <description>&lt;p&gt;One of the most commons tasks in Machine Learning is text classification, which is simply teaching your machine how to read and interpret a text and predict what kind of text it is.&lt;/p&gt;

&lt;p&gt;The purpose of this essay is to talk about a simple and generic enough Architecture to a supervised learning text classification. The interesting point of this Architecture is that you can use it as a basic/initial model for many classifications tasks.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;supervised-learning&quot;&gt;Supervised Learning&lt;/h2&gt;

&lt;p&gt;If you’re already familiar with this concept, just jump this step, but I feel it’s important to beginners to know.&lt;/p&gt;

&lt;p&gt;Supervised Learning is when you have to first train your model with already existing labeled dataset, just like teaching a kid how to differentiate between a car and a motorcycle, you have to expose its differences, similarities and such. Whereas unsupervised learning is about learning and predicting without a pre-labeled dataset.&lt;/p&gt;

&lt;h2 id=&quot;starting-to-sketch-the-architecture&quot;&gt;Starting to sketch the Architecture&lt;/h2&gt;

&lt;p&gt;With the dataset in hands, we start to think about how is going to be our architecture to achieve the given goal, we can resume the steps in:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Cleaning the dataset&lt;/li&gt;
  &lt;li&gt;Partitioning the dataset&lt;/li&gt;
  &lt;li&gt;Feature Engineering&lt;/li&gt;
  &lt;li&gt;Chosing the right Algorithms, Mathematical Models and Methods&lt;/li&gt;
  &lt;li&gt;Wrapping everything up&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;cleaning-the-dataset&quot;&gt;Cleaning the dataset&lt;/h2&gt;

&lt;p&gt;Cleaning the dataset is a crucial initial step in Machine Learning, many Toy Datasets don’t need to be cleaned, because it’s already clean, peer-reviewed and published in a way you can use it exactly to work on the learning algorithms.&lt;/p&gt;

&lt;p&gt;The problem is:&lt;/p&gt;

&lt;h4 id=&quot;the-real-world-is-full-of-painful-and-noisy-datasets&quot;&gt;The real world is full of painful and noisy datasets&lt;/h4&gt;

&lt;p&gt;If there’s one thing I learned while working with Machine Learning is, there’s no such thing as shiny and perfect dataset in the real world, so we have to deal with this beforehand. Situations where there are many empty fields, wrong and non-homogeneous formats, broken characters, is very common. I won’t talk about such techniques now, but I will write something about it in another post.&lt;/p&gt;

&lt;h2 id=&quot;partitioning-the-dataset&quot;&gt;Partitioning the Dataset&lt;/h2&gt;

&lt;p&gt;We always need to partition the dataset in, at least, 2 partitions: the training dataset and the test/validation dataset. Why?&lt;/p&gt;

&lt;p&gt;Suppose we fed the learning algorithm with a training data X and it already known the output Y (because it’s a training data pair (X,Y)), which is, for given text X, Y is its classification, the algorithm will learn it.&lt;/p&gt;

&lt;p&gt;Great, the algorithm learned this. But now, we’re going to validate the learning, so we use the same data X, I mean, we pass X to the model and ask what’s its classification…&lt;/p&gt;

&lt;p&gt;Do you see the problem here?&lt;/p&gt;

&lt;p&gt;Of course the algorithm will output Y, the same Y we passed to its training. So, if we pass the complete dataset D in the training phase, then we validate the model using the SAME D dataset, we will be steping onto this very same situation. It’s like cheating, it’s like we point to a car and say “this is a car”, then, at the same time and with the same car, we ask to the kid “is this a car?”, it will be highly probable that the kid will answer correctly, though it may not learned correctly. What we must do is, point to a car and say “this is a car”, and then, point to a different car and ask “is this a car?”.&lt;/p&gt;

&lt;p&gt;Stepping out of the metaphor, we must check if the machine learned correctly by using a diferent portion of the dataset.&lt;/p&gt;

&lt;p&gt;So, at the end of this step, we’ll have the training dataset and the test dataset, both are subset of the same initial dataset.&lt;/p&gt;

&lt;p&gt;With Python’s Scikit Learn you can do this easily using &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html&quot; target=&quot;_blank&quot;&gt;Train Test Split&lt;/a&gt; &lt;em&gt;(read the docs, it’s very simple to use it.)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/ml1.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;##Feature Engineering&lt;/p&gt;

&lt;p&gt;This is one of the most important steps when doing Machine Learning. Briefly, Features are the data that the learning algorithm will use as the “X”, which will be used to compute and understand the patterns. A simpler example would be a non-text classification or a regression, e.g: house pricing predictions, where our Features could be: number of rooms, size in squared meters, location and more. As you can see, these feature will describe the patterns of each data point and will affect how the house will be priced.&lt;/p&gt;

&lt;p&gt;The crucial point is, features can vary, you can identify new features that can level up your model’s predictions in huge proportions, a simple example would be, in a text classification you count the frequency of each word, this is one feature… you feed your learning algorithm with it… and the result isn’t enough: 50% of precision. But then you see that, in this case, the length of the whole data point (the text) plays a huge role determining a pattern for each class. Now that you &lt;strong&gt;identified&lt;/strong&gt; this feature, you do something to &lt;strong&gt;extract&lt;/strong&gt; that feature and then add this new feature and feed your learning algorithm with it, then, the result: 95% of precision.&lt;/p&gt;

&lt;p&gt;Ok, so you know the importance of the feature engineering phase, now it’s time to understand the most common technique to extract feature in texts: &lt;strong&gt;TF-IDF&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;tf-idf&quot;&gt;TF-IDF&lt;/h4&gt;

&lt;p&gt;Though the most intuitive way to look for patterns in texts is to count each word in the text (and use it as a Feature), it may not be the best way to do it. A few reasons for it is, larger texts will have higher averages than the shorter texts, these discrepancies can hurt the learning algorithm, and this Feature &lt;strong&gt;doesn’t say much about the importance of the words&lt;/strong&gt;, which is very important to find patterns.&lt;/p&gt;

&lt;p&gt;So, instead of computing the occurrence, it’s better to compute the importance of the words. to accomplish this we can use 2 statistic’s techniques:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TF&lt;/strong&gt; - which is basically the raw frequency of the word given the document, raw frequency of t by f(t,d), then the simple tf scheme is tf(t,d) = f(t,d).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;IDF&lt;/strong&gt; - Inverse Document Frequency, which is a technique to give emphasis to words that &lt;em&gt;don’t appear with high frequency&lt;/em&gt;, because these are the words that can differentiate the texts, which means, in this case, these are the most important words, so we inverse its frequencies. So, an example, if the word &lt;em&gt;“the”&lt;/em&gt; happens to appear very often in a text, it will weight &lt;strong&gt;less&lt;/strong&gt;, because it’s a common word, thus, don’t cause very impact when fiding patterns to differentiate the texts.&lt;/p&gt;

&lt;p&gt;So the whole TF-IDF can be computed by&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/e/8/1/e81492e44713270fd230d821ccebd100.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scikit Learn gives us a great API to use the TF-IDF method, it’s really simple.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    from sklearn.feature_extraction.text import TfidfVectorizer

    vectorizer = TfidfVectorizer()
    vectorized_x = vectorizer.fit_transform(X)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which will do the TF-IDF on the X data, and then vectorize this data, in other words, transform the whole thing into an array of inverse frequencies.
So, extracting those features, &lt;strong&gt;this&lt;/strong&gt; will be our training and test data, that will feed the algorithm (along with the Y, which is the output, the class of each training data).&lt;/p&gt;

&lt;p&gt;Re-thinking our Architecture, now we have:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/ml2.svg&quot; /&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Remembering that X is the set of features (e.g: vector with the TF-IDF of each data point) and Y is the output, which is, the labels/class (e.g: Spam or not-spam).&lt;/p&gt;

&lt;h2 id=&quot;chosing-the-right-algorithms-mathematical-models-and-methods&quot;&gt;Chosing the right Algorithms, Mathematical Models and Methods&lt;/h2&gt;

&lt;p&gt;With the data prepared, features selected and extracted, it’s time to feed the algorithm with this data, this topic &lt;em&gt;per se&lt;/em&gt; could go pages and pages, as learning algorithms is such a huge fields, with many publications and ideas to solve every kind of problem.&lt;/p&gt;

&lt;p&gt;To not extend it very much, and as the purpose of this essay is to discuss the architecture, I’ll use a few commons algorithms, such as Logistic Regression, Decision Trees, SVM and Neural Networks.&lt;/p&gt;

&lt;p&gt;So, at this point, we’ll treat the learning algorithm as a black-box algorithm, where it:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;receive a training data X which is a vector of features, and training data Y, which is the label/class/output (we can binarize it, such as spam=1, ham=0)&lt;/li&gt;
  &lt;li&gt;return a model, where given an text X, can output its predicted class Y.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After generating this model, we will test it with our test dataset and check its performance, if it can predict correctly &lt;em&gt;(the test dataset has output values (Y) so we can check them)&lt;/em&gt;, it is ready to predict new data &lt;em&gt;(data completely outside our initial dataset)&lt;/em&gt;, so we say that the machine learned the task.&lt;/p&gt;

&lt;p&gt;Our architecure now:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/ml3.svg&quot; height=&quot;800&quot; /&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;wrapping-everything-up&quot;&gt;Wrapping everything up&lt;/h2&gt;

&lt;p&gt;With this architecture, we should be able to do most of the simple text classification tasks, as the main flow is: get data, clean data, identify and extract features, train your algorithm/mathematical model of choice, validate it and then, use the generated model to do the estimations.&lt;/p&gt;

&lt;p&gt;Of course there are many improvements that can be made on this architecture and many, many, many lower level details, but you can see this architecture as a &lt;em&gt;“boilerplate code”&lt;/em&gt; to get you started with the machine learning engineering task.&lt;/p&gt;

&lt;p&gt;A few tips:&lt;/p&gt;

&lt;p&gt;Learn the underlying mathematical models of the most commons learning algorithms, this will teach you the trade offs of each one, so you can apply the correct algorithms to the given dataset and scenario. For example, SVM can be good for unbalanced dataset, but why? You gotta know this. Neural Networks can be slow to be trained, but, if training time is not critical, it’s okay to use Neural Networks.&lt;/p&gt;

&lt;p&gt;Master the skills to clean data, if using Python, learn to use Pandas. This will be an extremely important skill.&lt;/p&gt;

&lt;p&gt;Master the skills to understand data, this will be crucial to make you see what algorithm to use, you can’t make something learn if you don’t know about what you are teaching.&lt;/p&gt;

</description>
        <pubDate>Tue, 30 Jun 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/generic-architecture-text-classification</link>
        <guid isPermaLink="true">http://localhost:4000/post/generic-architecture-text-classification</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>engineering</category>
        
        <category>tips</category>
        
        <category>architecture</category>
        
        <category>text classification</category>
        
        
        <category>machine-learning</category>
        
        <category>software-architecture</category>
        
      </item>
    
      <item>
        <title>Case Study: Python Performance on rotating one-dimensional vectors</title>
        <description>&lt;p&gt;I’m a big fan of a nice challenge, therefore, I like books like Programming Pearls, I like to dive into many kinds of solutions to the same problem and try to differentiate them by novelty, performance, elegance, etc…&lt;/p&gt;

&lt;p&gt;This time I was playing with a fun problem, from the column 2:&lt;/p&gt;

&lt;!--more--&gt;

&lt;blockquote&gt;
  &lt;p&gt;“rotate a one dimensional array of &lt;em&gt;N&lt;/em&gt; elements left by &lt;em&gt;I&lt;/em&gt; positions”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The author says it should consume little space and time, so, there are many solutions, obviously. The fun thing is that I was doing it in Python, so you can solve it in many ways, but you can decrease the performance a lot if you choose poorly. In C we could just swap pointers in a doubly linked list, which is what happens in the real implementation of CPython.&lt;/p&gt;

&lt;p&gt;The idea that the Author exposed is pretty clever, which is based in reversing the vector only 3 times:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reverse the vector, from the first position to the &lt;em&gt;I&lt;/em&gt;-th position, where &lt;em&gt;I&lt;/em&gt; is the number of position that is wanted to move to the left&lt;/li&gt;
  &lt;li&gt;Reverse the vector, from &lt;em&gt;I&lt;/em&gt;-th + 1 position to &lt;em&gt;N&lt;/em&gt;, where &lt;em&gt;N&lt;/em&gt; is the size of this vector&lt;/li&gt;
  &lt;li&gt;Reverse the whole vector, again.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In other words, we have the vector &lt;em&gt;AB&lt;/em&gt; (Where &lt;em&gt;A&lt;/em&gt; is the first part, &lt;em&gt;B&lt;/em&gt; is the second), we reverse &lt;em&gt;A&lt;/em&gt; so we have &lt;em&gt;Ar&lt;/em&gt;, reverse &lt;em&gt;B&lt;/em&gt; so we have &lt;em&gt;Br&lt;/em&gt;, now we have &lt;em&gt;ArBr&lt;/em&gt;, then we reverse the whole thing &lt;em&gt;(ArBr)r&lt;/em&gt;, after that, we have the rotated vector. It’s quite beautiful. A picture can help the visualization:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/img1.jpeg&quot; height=&quot;450&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;here’s an example with a simple vector:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/img2.jpeg&quot; height=&quot;350&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Now, searching for other approaches, I looked inside the code from CPython, and there’s a nice comment:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &quot;Conceptually, a rotate by one is equivalent to a pop on one side and an append on the other&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Quite elegant solution as well. So, an example:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/img4.jpeg&quot; height=&quot;350&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And the last example I found while searching through Pythonic ways to solve this problem (Even though may not be very fast)&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
def rotate_pythonic_way(arr, i):
         return arr[i:] + arr[:i] 
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Which is super simple and elegant, what it’s doing is the following:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/img3.jpeg&quot; height=&quot;350&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;At first I thought this solution would be the slowest, but we’ll get to that.&lt;/p&gt;

&lt;p&gt;One thing that took my attention was the internal method to reverse a list that Python offers, there are two of them, one returns a reversed iterator and you can cast it into a list and the other reverse the exactly same list that is passed as parameter, it shouldn’t take you long to realize which is faster. So I wrote a code with the many solutions to it and I profiled the functions, which gave us a interesting result when ran over a vector with 10 millions integers.&lt;/p&gt;

&lt;p&gt;First, the original idea (&lt;em&gt;reverse the vector 3 times&lt;/em&gt;), but using the reversed(array) method:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
def rotate_original_solution_with_reversed(arr, i):
    n = len(arr)
    array_A_reversed = list(reversed(arr[0:i]))
    array_B_reversed = list(reversed(arr[i:n]))
    ArBr = array_A_reversed + array_B_reversed
    rotated_array = list(reversed(ArBr))
    return rotated_array
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;and the result:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    Original Idea using reversed(arr): 

    4 function calls in 0.611 seconds

    Ordered by: standard name

    ncalls tottime percall cumtime percall filename:lineno(function)
    1 0.440 0.440 0.440 0.440 2_1b.py:21(rotate_original_solution_with_reversed)
    1 0.171 0.171 0.611 0.611 string:1 module
    1 0.000 0.000 0.000 0.000 {len}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;####Damn, 0.6s, that’s too slow!&lt;/p&gt;

&lt;p&gt;After this one, I changed the way I was reversing the vector, using the other internal method from Python:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
def rotate_original_solution_with_reverse(arr, i):
    n = len(arr)
    
    array_A = arr[0:i]
    array_B = arr[i:n]
    
    array_A.reverse()
    array_B.reverse()

    ArBr = array_A + array_B
    ArBr.reverse()
    return ArBr
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;And the result:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    Original Idea using arr.reversed(): 
    7 function calls in 0.322 seconds

    Ordered by: standard name

    ncalls tottime percall cumtime percall filename:lineno(function)
    1 0.187 0.187 0.210 0.210 2_1b.py:30(rotate_original_solution_with_reverse)
    1 0.113 0.113 0.322 0.322 string:1 module
    1 0.000 0.000 0.000 0.000 {len}
    1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}
    3 0.023 0.008 0.023 0.008 {method 'reverse' of 'list' objects}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;####From 0.6s to 0.3s, that’s a great improvement. lesson: choose your built-ins/Data Structures/Algorithms carefully.&lt;/p&gt;

&lt;p&gt;And now, using the Pythonic Way, which I was thinking that would be the slowest:&lt;/p&gt;

&lt;p&gt;And the result:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;bash hljs&quot;&gt;
Pythonic Way: 

3 function calls in 0.295 seconds

Ordered by: standard name

ncalls tottime percall cumtime percall filename:lineno(function)
1 0.238 0.238 0.238 0.238 2_1b.py:45(rotate_pythonic_way)
1 0.056 0.056 0.295 0.295 string:1(module)
1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}

&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;It was faster than the other methods! Well played, Python.&lt;/p&gt;

&lt;p&gt;And the last one, using the pop/append technique, repeated i times:&lt;/p&gt;

&lt;p&gt;And the result:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;bash hljs&quot;&gt;

Pop and append way: 

5 function calls in 0.013 seconds

Ordered by: standard name

ncalls tottime percall cumtime percall filename:lineno(function)
1 0.000 0.000 0.013 0.013 2_1b.py:49(rotate_pop_append)
1 0.000 0.000 0.013 0.013 string:1(module)
1 0.000 0.000 0.000 0.000 {method 'append' of 'list' objects}
1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}
1 0.013 0.013 0.013 0.013 {method 'pop' of 'list' objects}
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Ok. That was weird. 0.013 seconds?  13 milliseconds? Crazy, right? I thought the fact of repeating it many times would make it go way slower, but I guess I was wrong!&lt;/p&gt;

&lt;p&gt;So, as you can see, an interesting problem can have many solutions, one more elegant, others faster. In the end you may pick the simplest and most intuitive solution, this may be the best for the situation. Some times you gotta go with the strangest and most non-intuitive solution (which reminds me the first time I saw the Quick Sort and all its non-intuitive way to think)&lt;/p&gt;

&lt;p&gt;So, if you’re willing to reverse a list with Python, go with List.reverse() method (unless you want the iterators to do something), and if you want to rotate a vector, go with pop/append, it looks faster.&lt;/p&gt;
</description>
        <pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/case-study-python-performance-rotating-vector</link>
        <guid isPermaLink="true">http://localhost:4000/post/case-study-python-performance-rotating-vector</guid>
        
        
        <category>algorithms</category>
        
        <category>case-study</category>
        
        <category>python</category>
        
        <category>profiling</category>
        
      </item>
    
      <item>
        <title>Web Scaling - using Redis as Cache</title>
        <description>&lt;p&gt;&lt;strong&gt;Redis is such a great technology.&lt;/strong&gt; Unfortunately, there’s still people who don’t know Redis or don’t know that Redis can be used as a Cache System to improve the speed of responses.&lt;/p&gt;

&lt;h3 id=&quot;why-redis&quot;&gt;Why Redis&lt;/h3&gt;

&lt;p&gt;Well, let’s start this discussion remembering how a common Relational Database basically works: Suppose we’re using a MySQL, every time your app sends a request to the MySQL client, the MySQL client gotta make a trip to the hard drive to get the data asked in the request, this can become a problem if the data asked in request is big… and if there are many requests at the same time, this can generate a huge latency, annoying users or worse.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;This is where Redis comes into play, Redis is a key-value database that will be running and storing data inside your memory, if you remember the basic of computers architecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/memchart.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s way faster to access data in memory (Physical RAM, main memory) than to access data in the Hard Drive, so it’s easy to notice that if the data that the application wants to access is inside the main memory, it’s way easier to reach to that data than if it was stored in the Hard Drive.&lt;/p&gt;

&lt;p&gt;So, like I said, Redis will be storing its data inside the memory, but you may ask yourself: “but what if I turn off the machine?? isn’t the ram memory volatile? “ Yes, that’s why Redis will be flushing the data to the hard drive from time to time, it’s up to you to choose this time between flushes, it’s all about Performance vs. Security.&lt;/p&gt;

&lt;p&gt;So, What we’ll be doing is just:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/atv1.png&quot; width=&quot;350&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;note that this print is taken from a talk I gave in my country, so it’s in portuguese. aplicação = application, Não acha key = key don’t found, retorna dados = return data&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As you can see: So much win. We avoided redundant trips to the disk.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now that we understand the concept of what we’ll be doing, the code becomes very easy to implement, here’s a simple idea &lt;em&gt;(thougt it can be improved and extended in many ways, but it can demonstrate the idea we’re working here)&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
    from flask import Flask
    from flask.ext.sqlalchemy import SQLAlchemy
    from sqlalchemy import create_engine
    import redis

    app = Flask(__name__)
    url = 'mysql+pymysql://username:password@ip/dbname'
    app.config['SQLALCHEMY_DATABASE_URI'] = url
    db = SQLAlchemy(app)
    cache = redis.StrictRedis(host='localhost', port=6379, db=0)

    #improvement: model could stay in a different file
    class User(db.Model):
        id = db.Column(db.Integer, primary_key=True)
        username = db.Column(db.String(80))
        email = db.Column(db.String(120))

        def __init__(self, username, email):
            self.username = username
            self.email = email

        def __repr__(self):
            return '' % self.username

    def createUsers():
            for x in xrange(0,100000):
                user = User('test', 'test')
                db.session.add(user)
            db.session.commit()

    def getUsers():
            users = cache.get('users')
            if not users:
                users = User.query.all()
                cache.set('users', users)

    # improvement: views/routing should stay in a different file
    @app.route('/')
    def hello_world():
        getUsers()
        return 'Hello Worldd!'

    if __name__ == '__main__':
        app.debug=True
        app.run(host='0.0.0.0')
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;So, after you create the correct database, populate the database (there’s a function in the code for that) and change username/password/dbname in the code, we’ll run the app.py and go to localhost:port-you-exposed.&lt;/p&gt;

&lt;h3 id=&quot;what-will-happen&quot;&gt;&lt;em&gt;What will happen?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;1. The first time you access it, it will take a few seconds to get the data from the MySQL&lt;/p&gt;

&lt;p&gt;2. The second time you access it, it will take just a few milliseconds to get the same data from redis&lt;/p&gt;

&lt;p&gt;In my computer the result was:&lt;/p&gt;

&lt;p&gt;first access: 45861 milliseconds&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;second access: 5ms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I know, right? That’s just blazing fast!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;And it can save a lot of computational resources and human time. Now, with this logic applied to one method (the method to get users), we can apply it to whichever method we want, or we can even create a generic decorator and annotate the methods that we want do the caching!!&lt;/p&gt;
</description>
        <pubDate>Mon, 25 May 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/web-scaling-using-redis-as-cache</link>
        <guid isPermaLink="true">http://localhost:4000/post/web-scaling-using-redis-as-cache</guid>
        
        <category>redis</category>
        
        <category>web</category>
        
        <category>scalability</category>
        
        <category>programming</category>
        
        <category>python</category>
        
        <category>programming</category>
        
        <category>engineering</category>
        
        
        <category>python</category>
        
        <category>software-architecture</category>
        
        <category>software-scalability</category>
        
        <category>caching</category>
        
      </item>
    
      <item>
        <title>Building your first REST API with Python</title>
        <description>&lt;p&gt;Are you total lost in this world full of jargons like: API, Rest API, microservices and stuffs? Come here, sit, grab a cup of coffee, and let’s talk briefly about it.&lt;/p&gt;

&lt;p&gt;Today a lot is said about APIs. Everything has an API, every programmer (newbie to expert) uses tons of API. Also, today we can see a lot of people talking about microservices and the idea of total separation of backend, frontend, web services or &lt;em&gt;whatever&lt;/em&gt;. So we can &lt;em&gt;(and we do)&lt;/em&gt; hear a lot about REST/RESTful APIs. We have 3 current problems with it:&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;1. Many new programmers don’t have a single clue of what a Rest API is.&lt;/p&gt;

&lt;p&gt;2. It’s probable that they’re using Rest APIs and don’t know about it.&lt;/p&gt;

&lt;p&gt;3. When they feel that they should learn more about it, there’s hardly any good and accessible material to learn it.&lt;/p&gt;

&lt;h3 id=&quot;so-heres-a-extremely-simplistic-approach-to-try-to-explain-what-is-a-rest-api&quot;&gt;So here’s a extremely simplistic approach to try to explain what is a Rest API&lt;/h3&gt;

&lt;p&gt;Starting with the concept of API, which stands for &lt;strong&gt;A&lt;/strong&gt;pplication &lt;strong&gt;P&lt;/strong&gt;rogramming &lt;strong&gt;I&lt;/strong&gt;nterface, it’s just an interface which you, &lt;em&gt;dear programmer&lt;/em&gt;, will be dealing with to extract whatever you &lt;em&gt;(or your program)&lt;/em&gt; want.&lt;/p&gt;

&lt;p&gt;Suppose your program needs to create a connection with a given database, normally you do:&lt;/p&gt;

&lt;p&gt;1. You import the &lt;em&gt;library&lt;/em&gt; that will abstract the connection with the database&lt;/p&gt;

&lt;p&gt;2. You create an object to represent a Connection with the database&lt;/p&gt;

&lt;p&gt;3. You call functions that this object provide to you, so you can do whatever you want (and whatever they provide)&lt;/p&gt;

&lt;p&gt;Here’s a silly example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from database_library import Connection

connection = Connection()
connection.OpenConnection()
… do whatever you want
connection.CloseConnection()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When you called those function, you were dealing with the interface that the object provided to you, sure the object may be doing thousand of things at the moment you call its functions, but, it doesn’t matter to you, does it? You just want the &lt;em&gt;damn&lt;/em&gt; connection open and then closed.&lt;/p&gt;

&lt;p&gt;So the library that you imported is giving you something you want, offering a service or a resource. That’s the sole purpose of an API. It’s a layer that you can use to get things from other(s) component(s).&lt;/p&gt;

&lt;p&gt;So, it’s easy to deduct that, the better the engineer planned the API, the easier it will be to deal with and extract what you want, and the contrary is true.&lt;/p&gt;

&lt;p&gt;But still, you’re processing the core of this API in your own machine, which isn’t that great, here’s when the API evolves to whole web hosted services, so you can request this API something, and this API can give you something, through the WEB, via HTTP request. And that’s amazing.&lt;/p&gt;

&lt;p&gt;And the community, recently, decided that the request and response that happen between applications and APIs, should be done with JSON, so this communication can become uniform and APIs can talk to other APIs effortlessly.&lt;/p&gt;

&lt;p&gt;So, basically, what’s going on is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/json-rest3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So… yes, you make your function calls, now, with just a simple URL + HTTP methods, wanna see a real example? We can just send a request to the Facebook’s API by accessing this URL: &lt;a href=&quot;http://graph.facebook.com/contatodigo&quot;&gt;http://graph.facebook.com/contatodigo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Which will request my profile’s data, and the Facebook’s API will return:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
   &quot;id&quot;: &quot;100001638888259&quot;,
   &quot;first_name&quot;: &quot;Rodrigo&quot;,
   &quot;gender&quot;: &quot;male&quot;,
   &quot;last_name&quot;: &quot;Ara\u00fajo&quot;,
   &quot;link&quot;: &quot;https://www.facebook.com/contatodigo&quot;,
   &quot;locale&quot;: &quot;pt_BR&quot;,
   &quot;name&quot;: &quot;Rodrigo Ara\u00fajo&quot;,
   &quot;username&quot;: &quot;contatodigo&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As simple as that.&lt;/p&gt;

&lt;p&gt;Now with this basic idea explained, we can do a simple Rest API using Python and Flask. (I’m assuming you’re already familiar with both technologies).&lt;/p&gt;

&lt;p&gt;All we’re gonna do is, using the flask routing, create routes to the users so they can interact with the resources of our API. Let’s suppose our API will serve and receive only Books. So this is the resource we’re dealing with, users may use our API to get Books and insert new Books, so our only URIs will be:&lt;/p&gt;

&lt;p&gt;1. &lt;em&gt;/bookapi/v1.0/books&lt;/em&gt; with a GET method, which will just return the list of books&lt;/p&gt;

&lt;p&gt;2. &lt;em&gt;/bookapi/v1.0/books&lt;/em&gt; with a POST method, which will insert a book&lt;/p&gt;

&lt;p&gt;So after the API is built, you or you program can get books or insert books by sending HTTP request to the API’s URIs, simple as that. I’ll be very straight forward, here’s the code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#!flask/bin/python
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;books&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'Game of Thrones'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'description'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'Cool dragons'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;s&quot;&gt;'finished'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'50 shadows of grey'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'description'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'It start as bullshit, end as a huge bullshit'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;s&quot;&gt;'finished'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/bookapi/v1.0/books/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'GET'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_books&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'books'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;books&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/bookapi/v1.0/books'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'POST'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;abort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;404&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;book&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;books&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'description'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'description'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'finished'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;books&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'book'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;201&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that we’re creating a simple in-memory &lt;em&gt;database&lt;/em&gt;, which is a simple python’s dict. This could be a database. But for the sake of simplicity, I’m using just a dict.&lt;/p&gt;

&lt;p&gt;When the requests come, it verifies the route that the user is asking for, which is: what resources is he/she wanting? and then, the code do whatever it must do (&lt;em&gt;remember the API idea of hiding the complexity, the user requesting just want the result&lt;/em&gt;), and then it put everything in a JSON and returns it. That simple!&lt;/p&gt;

&lt;p&gt;Of course, many improvements and extensions (&lt;em&gt;there are infinity possibilities&lt;/em&gt;) can be made to this code, but, here’s a skeleton of the idea of an API, it’s just a start. One good practice it’s to &lt;em&gt;not&lt;/em&gt; return the ID of the resource, but return just its URI, which is surely a great idea. Another good practice is to ask authentication in every HTTP request, so your API won’t be exposed to everybody. There are, indeed, endless improvements, but in this code, you, that are completely beginner to the API’s concept, can now understand what’s going on underneath and start planning and building your own API using Flask.&lt;/p&gt;
</description>
        <pubDate>Wed, 22 Apr 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/build-your-first-rest-api-python</link>
        <guid isPermaLink="true">http://localhost:4000/post/build-your-first-rest-api-python</guid>
        
        <category>api</category>
        
        <category>python</category>
        
        <category>programming</category>
        
        <category>engineering</category>
        
        <category>rest</category>
        
        <category>flask</category>
        
        
        <category>python</category>
        
        <category>software-architecture</category>
        
        <category>api</category>
        
      </item>
    
      <item>
        <title>Using Python and AI to predict types of wine</title>
        <description>&lt;p&gt;I’ve been working with AI/Machine Learning at &lt;a href=&quot;http://www.jusbrasil.com.br/&quot;&gt;Jusbrasil&lt;/a&gt; recently, and it’s being pretty challenging due to the &lt;em&gt;huge&lt;/em&gt; amount of data that we have to deal with, so cleaning this data and making predictions and classifications in an acceptable time demands a nice AI architecture.&lt;/p&gt;

&lt;p&gt;That said I can say that I’m extremely thankful for a few technologies that are helping me go through this challenge &lt;em&gt;(and the pain of cleaning this amount of data)&lt;/em&gt;: Python, Scikit Learn, Pandas, and the whole stack that the Scikit Learn use, such as NumPy, SciPy, matplotlib and few others.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;So, this inspired me to &lt;em&gt;spread the word&lt;/em&gt;, so I’ll be showing here a simple example of Machine Learning using Python, Pandas and Scikit Learn to predict, given a great amount of data/features about wines, if a wine is white or red.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;disclaimer:&lt;/strong&gt; &lt;em&gt;I’m assuming that you already have a small knowledge on the ideas of the machine learning and its mathematical aspects (although not necessary to implement the code that I’ll show here), this is just a simple introduction to scikit learn and its power, so the example is pretty simple and straight forward, if you just want the code, here it is: &lt;a href=&quot;https://gist.github.com/digorithm/ad742f9314f76e732888&quot;&gt;github link&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-pandas&quot;&gt;What’s Pandas?&lt;/h2&gt;

&lt;p&gt;Pandas is an amazing library for data manipulation, it makes the process of dealing with data very easy and straight forward, we can work with CSV, JSON and plenty other formats without struggling to manipulate the data, &lt;a href=&quot;https://signalvnoise.com/posts/3124-give-it-five-minutes&quot;&gt;give it five minutes&lt;/a&gt; and skim their &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/dev/&quot;&gt;docs&lt;/a&gt;, it will definitely worth it!&lt;/p&gt;

&lt;h2 id=&quot;fetching-the-data&quot;&gt;Fetching the data&lt;/h2&gt;

&lt;p&gt;Let’s start fetching the data with Pandas, &lt;em&gt;(you can download the data &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/&quot;&gt;here&lt;/a&gt;)&lt;/em&gt; to do so, just import Pandas and read the CSV file, just like that:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
import pandas as pd
reds = pd.read_csv('winequality-red.csv', sep=';')
whites = pd.read_csv('winequality-white.csv', sep=';')
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;We can see how the data is structured by doing a few commands:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
reds.values[0:6]
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;As output, we have the first 5 rows of the red wine’s data&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
array([[7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4, 5],
       [7.8, 0.88, 0.0, 2.6, 0.098, 25.0, 67.0, 0.9968, 3.2, 0.68, 9.8, 5],
       [7.8, 0.76, 0.04, 2.3, 0.092, 15.0, 54.0, 0.997, 3.26, 0.65, 9.8, 5],
       [11.2, 0.28, 0.56, 1.9, 0.075, 17.0, 60.0, 0.998, 3.16, 0.58, 9.8,6],
       [7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4, 5],
       [7.4, 0.66, 0.0, 1.8, 0.075, 13.0, 40.0, 0.9978, 3.51, 0.56, 9.4, 5]], 
dtype=object)
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Or we just use a function from Pandas that describe very well our data&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
reds.head()
&lt;/code&gt;
&lt;/pre&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;

&amp;lt;class 'pandas.core.frame.dataframe'=&quot;&quot;&amp;gt;Int64Index: 5 entries, 0 to 4
Data columns:
fixed acidity           5  non-null values
volatile acidity        5  non-null values
citric acid             5  non-null values
residual sugar          5  non-null values
chlorides               5  non-null values
free sulfur dioxide     5  non-null values
total sulfur dioxide    5  non-null values
density                 5  non-null values
pH                      5  non-null values
sulphates               5  non-null values
alcohol                 5  non-null values
quality                 5  non-null values
dtypes: float64(11), int64(1), object(1)&amp;lt;/class&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;

&lt;h2 id=&quot;understanding-our-data&quot;&gt;Understanding our data&lt;/h2&gt;

&lt;p&gt;Matplotlib gives you many ways to plot our data into graphs so we can understand what is going on with the data so we can choose the best model/algorithm for the given scenario,&lt;/p&gt;

&lt;p&gt;For instance, let’s take a look at the relation between the red wines and its fixed acidity&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
x = plt.subplots(figsize=(10, 5))
plt.plot(reds.index, reds.get(&quot;fixed acidity&quot;), 'ro')
ax.set_title('Wines vs fixed acidity')
ax.set_xlabel('red wine index')
ax.set_ylabel('Fixed Acidity')
plt.show()
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/mlproblem.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;preparing-the-data-for-classification&quot;&gt;Preparing the Data for classification&lt;/h2&gt;

&lt;p&gt;Now we’re going to add a new feature/variable to our data, which is our target variable, the &lt;em&gt;Y&lt;/em&gt;, that will be telling if the wine from our dataset is white or red&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
reds['kind'] = 'red'
whites['kind'] = 'white'
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;We need to get all of our feature into a vector called X, that will be set into our algorithm, right? And, we need to get all our targets (white or red) and set into a Y variable&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
wines = reds.append(whites, ignore_index=True)
X = wines.ix[:, 0:-1]
y = wines.kind
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Notice that we’re merging both datasets together, the one with the red wines and the one with the white wines, so we can send them together to the algorithm. Now we’re going to binarize the labels ‘white’ and ‘red’, so the mathematical model can use it. It’s pretty simple&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
y = y.apply(lambda val: 0 if val == 'white' else 1)
&lt;/code&gt;
&lt;/pre&gt;

&lt;h2 id=&quot;the-algorithm&quot;&gt;The algorithm&lt;/h2&gt;

&lt;p&gt;Now that we have our data well structured and we do understand it, we can start looking for an algorithm to use. A good algorithm for our scenario is a simple &lt;strong&gt;Logistic Regression&lt;/strong&gt;, that will return a model that we’ll use to make our predictions/classification. The mathematical linear model that will use is the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(w) := \lambda\, R(w) + \frac1n \sum_{i=1}^n L(w;x_i,y_i) \label{eq:regPrimal}\&lt;/script&gt;

&lt;p&gt;In addition with the loss function defined by the logistic loss&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w;x,y) := \log(1+\exp( -y w^T x ))&lt;/script&gt;

&lt;p&gt;The Scikit learn provides an awesome library with an amazingly ease of use, so that we don’t have to implement the whole model from scratch. All we have to do is create a object from the model we want to use, understand how it works &lt;em&gt;(at least understand how to use its interface to do what we want)&lt;/em&gt;. In this case, we will be using the &lt;em&gt;cross validation&lt;/em&gt; object, which is another discussion for another time, but in a few words, it will divide our dataset and test it against all parts of the divided dataset, this way we make sure that we’re validating the quality of the result.&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
clf = LogisticRegression()
scores = cross_val_score(clf, X, y, cv=5)
print scores.mean(), scores.std()
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;And as the result we get:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
0.981376321334 0.00638795038332
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;And that’s the exactly the precision of the algorithm over the given dataset, &lt;strong&gt;98% of precision&lt;/strong&gt;, which is quite good! Now, for example, we can save this trained classifier and use it for future classifications of incoming data about wines that need to be classified as white or red, to do so, we just call the method &lt;em&gt;clf.predict(X)&lt;/em&gt; where this X will be the new wine’s data. simplicity at its best!&lt;/p&gt;
</description>
        <pubDate>Tue, 31 Mar 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/using-python-and-ai-to-predict-wine</link>
        <guid isPermaLink="true">http://localhost:4000/post/using-python-and-ai-to-predict-wine</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>python</category>
        
        <category>sklearn</category>
        
        <category>programming</category>
        
        <category>engineering</category>
        
        <category>tutorial</category>
        
        
        <category>python</category>
        
        <category>case-study</category>
        
        <category>machine-learning</category>
        
      </item>
    
  </channel>
</rss>
