<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rodrigo Araujo</title>
    <description>Thoughts on computer science, software engineering, distributed systems, and machine learning</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 16 Oct 2018 19:43:07 +0000</pubDate>
    <lastBuildDate>Tue, 16 Oct 2018 19:43:07 +0000</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
      <item>
        <title>On machine learning enhanced software systems</title>
        <description>&lt;p&gt;I have been brewing the idea of using machine learning to improve software systems since 2016. It was pretty vague and broad, without an actionable plan. I just had the intuition — the software configuration and tuning, especially after the adoption of microservices, was getting too complex.&lt;/p&gt;

&lt;h3 id=&quot;the-increasing-complexity-of-configuring-and-tuning-systems&quot;&gt;The increasing complexity of configuring and tuning systems&lt;/h3&gt;
&lt;p&gt;If you have enough experience in the software industry, then it’s very likely that you’ve struggled with either a configuration problem or a tuning problem.&lt;/p&gt;

&lt;p&gt;Configuration and tuning problems are pretty common and can lead to really bad outages. They often occur when:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Some parts of the system are poorly or wrongly configured, or&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A configuration that worked before now doesn’t work because the context of the system has changed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Think of a number of database replicas and their writing schemes. Or in Postgresql, think of the number of shared buffers, effective cache size, and the min and max wal size.&lt;/p&gt;

&lt;p&gt;If wrongly configured from the start, it won’t work in the given context, plain and simple. What’s more interesting, though, is if it’s &lt;em&gt;correctly&lt;/em&gt; configured, it might work at a given time. But as the context changes — system workload, system resources usage, overall system architecture — the system will behave poorly. Or, even worse, an outage might happen.&lt;/p&gt;

&lt;p&gt;This will, inevitably, lead to manually-performed operations and the creation of heuristics. In other words, it will lead to:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Oh, we should set X to A, when workload is T, but it should be A+10 when workload is T+100 and we have system resources usage above 80%… I guess. Or maybe let’s just up a queue in front of this component, queues solve everything, right?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now multiply this scenario by tens or hundreds of services. Think for a second about the cognitive burden resulting from these configurations.&lt;/p&gt;

&lt;p&gt;This is not a new concern. In 2003, Ganek and Corbi &lt;a href=&quot;http://ieeexplore.ieee.org/document/5386835/?reload=true&quot;&gt;discussed&lt;/a&gt; the need for autonomic computing to handle the complexity of managing software systems. They noted that managing complex systems became too costly, labor-intensive, and prone to error due to the pressure engineers felt while maintaining them. This increased the potential of system outages with a concurrent impact on business.&lt;/p&gt;

&lt;p&gt;Even nowadays, most of the configurations and tuning of the systems are performed manually, often in run-time, which is known to be a very time-consuming and risky practice. Check out these two links (&lt;a href=&quot;https://link.springer.com/book/10.1007/978-3-642-35813-5&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.8651&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;here&lt;/a&gt;) to read more about it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2000/1*6Kh0EXVHQ9zmau8kLs4pzQ.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-need-for-autonomic-computing&quot;&gt;The need for autonomic computing&lt;/h3&gt;

&lt;p&gt;Most decisions to configure and tune the system are made based on the context — there are many different variables such as workload, number of instances of some services, resources usage, and more. So why not delegate these tasks to something that excels at exactly that? &lt;em&gt;Machine learning sounds like a feasible tool for the job.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;After starting my Masters at the University of British Columbia, I kept working on this idea. It seemed interesting although quite weird, and, sometimes, unpractical and impossible to implement.&lt;/p&gt;

&lt;p&gt;To my surprise, I realized I wasn’t alone. Some very interesting people were working on these ideas — so it might not be that weird, unpractical, and impossible.&lt;/p&gt;

&lt;p&gt;Recently, Jeff Dean — a man that I admire a lot — &lt;a href=&quot;https://news.ycombinator.com/item?id=15892956&quot;&gt;gave a talk at NIPS 2017 talking about machine learning for systems&lt;/a&gt;, where he stated:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Learning should be used throughout our computing systems. Traditional low-level systems code (operating systems, compilers, storage systems) does not make extensive use of machine learning today. This should change!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Computer Systems are filled with heuristics: compilers, networking code, operating systems. Heuristics have to work well “in general case”. [They] generally don’t adapt to actual pattern of usage and don’t take into account available context&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Learning in the core of all of our computer systems will make them better/more adaptive.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I was in complete awe when I read this. One of the engineers I admire the most was talking about the very same ideas I’ve been thinking about and working on.&lt;/p&gt;

&lt;p&gt;This led me to think that it’s not only interesting but &lt;strong&gt;natural to think about enhancing software systems with machine learning.&lt;/strong&gt; Throughout the whole software stack, we have many heuristics that, although they work well, could be improved by machine learning.&lt;/p&gt;

&lt;p&gt;Is it challenging and potentially risky? Yes, most definitely. Especially given that interpretability, apparently, has become a secondary goal in the machine learning community. How can we interpret and explain the decisions made by neural nets?&lt;/p&gt;

&lt;p&gt;However, with that said, these obstacles shouldn’t hinder scientific and technological progress. &lt;a href=&quot;https://arxiv.org/pdf/1712.01208.pdf&quot;&gt;Yes, we should question old paradigms &lt;/a&gt;and try to improve things.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2000/1*puiL2EVDE6Ztlocw3JD1uQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;towards-machine-learning-enhanced-software-systems&quot;&gt;Towards machine learning-enhanced software systems&lt;/h3&gt;

&lt;p&gt;As Jeff Dean pointed out: we need to find &lt;strong&gt;practical&lt;/strong&gt; ways to make systems data-aware. We need systems that collect metrics and metadata about themselves. To achieve this, we could learn a thing a two from the ideas in systems observability and instrumentation. We have been instrumenting systems for decades, and the data is already there.&lt;/p&gt;

&lt;p&gt;We also need to find &lt;strong&gt;practical&lt;/strong&gt; and &lt;strong&gt;clean&lt;/strong&gt; ways to &lt;strong&gt;integrate&lt;/strong&gt; machine learning components into software systems, making learning a first-class citizen in the system. This will lead to &lt;strong&gt;systems that learn how to improve themselves,&lt;/strong&gt; beating heuristics and manually-performed operations. Think about this for a second. It does sound cool &lt;em&gt;and&lt;/em&gt; feasible.&lt;/p&gt;

&lt;p&gt;I would also add that we need &lt;strong&gt;practical&lt;/strong&gt; and &lt;strong&gt;clean&lt;/strong&gt; ways to propagate the decisions made by the learned models to the rest of the system. This would allow the system to have self-adaptive capabilities. Here, we could learn something from the control theory community.&lt;/p&gt;

&lt;p&gt;The general idea is fairly simple: make a system learn about its behavior by training a model on its context. Then allow it to change its structures and configurations in order to optimize for a certain scenario. Now implement this idea in such a way that it could be possible to integrate it into many kinds of systems.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;The most interesting questions I have in mind are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Can self-adaptation by learned models lead to more stable, faster, safer software systems? Can it reduce the need for manually configuring and tuning systems, allowing engineers to focus on more important tasks?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Can this be easily integrated into software systems, requiring only small changes to the codebase?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Can this work with low overhead?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is worth noting that this &lt;strong&gt;would not&lt;/strong&gt; replace good engineers, but would rather free the engineers’ cognitive abilities to focus on what matters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.sysml.cc/&quot;&gt;I genuinely believe that this will become a trend in the next few years&lt;/a&gt;. I myself am working on these ideas as part of my graduate studies, and I will be posting the results of my research, so &lt;a href=&quot;https://twitter.com/digorithm&quot;&gt;stay tuned&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/machine-learning-enhanced-software</link>
        <guid isPermaLink="true">http://localhost:4000/post/machine-learning-enhanced-software</guid>
        
        <category>architecture</category>
        
        <category>software</category>
        
        <category>machine-learning</category>
        
        <category>thoughts</category>
        
        <category>systems</category>
        
        
        <category>software-architecture</category>
        
        <category>systems</category>
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>Thoughts on visualizing software architecture</title>
        <description>&lt;p&gt;The more I think about the advantages and disadvantages of upfront design, the more I find it complex, I mean, the whole software architecture diagramming in general. I’ve been following the development of the &lt;a href=&quot;http://www.codingthearchitecture.com/2014/08/24/c4_model_poster.html&quot;&gt;C4 model&lt;/a&gt; for a considerable time, I find it interesting, but it still brings me the old struggles about how much detail we should put into the diagrams and whether we should do it while planning the software, while developing it, or after.&lt;/p&gt;

&lt;p&gt;I believe that when drawing architecture diagrams we should focus on getting the right purpose and the right level of abstraction.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;I do think that the diagram should reflect the code in certain level of abstraction and that this could be really useful for a new engineer in a large project. Tackling a huge codebase is not an easy task, and the diagrams should act as a map of the code, saying how things communicate with each other.&lt;/p&gt;

&lt;p&gt;Perhaps what we need is different types of architecture diagrams, each one for a specific development phase. What comes to my mind is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Project planning phase&lt;/strong&gt;: we can’t code without any upfront planning, at this level I feel necessary to draw the most &lt;em&gt;visible&lt;/em&gt; containers and components, this way we can plan what to do next, how to split up teams to each part of the system. It’s when a deployment architecture can be written as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Project evolution and maintenance&lt;/strong&gt;: as we reach this phase of the project, it’s interesting to see the Big Picture, the Not-So-Big Picture and the Small Picture. I can see many cases where this could be helpful, for instance: new engineers coming to the project, debugging something that crosses many components and containers, and I’m sure there are many more.&lt;/p&gt;

&lt;p&gt;Given this task, I’d prefer taking a top-down approach, going from Big Picture architecture to a more detailed one. As an example, I’m going to use a side project I was working on, which is a simple webapp to search for simple food recipes.&lt;/p&gt;

&lt;p&gt;I followed a backend + client side app approach: in the backend is where the main logic happens, the core business classes and everything, in addition to exposing a simple REST API. Client side just consumes the backend data through its API.&lt;/p&gt;

&lt;p&gt;Each of these 2 parts will be a different application, which can be dockerized and run in different plataforms – two separate amazon EC2, for example. So if we’re planning it, we could start by saying:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/gdd20overview.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The client will talk to the backend’s api over HTTP, the backend will communicate to the client and to a database instance. Ok, that’s a start. We can easily separate a team for the backend and its api and another team for the web client.&lt;/p&gt;

&lt;p&gt;Still, it doesn’t say much about the inner details of each container, and now we need something more detailed. So we break it down a bit more:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/gdd20arch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, it’s the same structure, but with way more details, which makes it clearer to everybody how we can tackle the problem with actual coding. An extra level of detail would be going down one more level of abstraction and describe the classes through a class diagram, explaining how the core backend would be implemented, for example.&lt;/p&gt;

&lt;p&gt;Now, this diagram, plus all requirements gathered can be a nice start for the implementation phase. We can add now how we would deploy it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/images/gdd20deploy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I believe those 3 diagrams plus the codebase could be helpful for efficient communication.&lt;/p&gt;

&lt;p&gt;Though I still have many open questions about this, e.g: the cost of this upfront design is a concern,so I’m open to discussions.&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/thoughts-visualizing-software-arch</link>
        <guid isPermaLink="true">http://localhost:4000/post/thoughts-visualizing-software-arch</guid>
        
        <category>architecture</category>
        
        <category>software</category>
        
        <category>visualization</category>
        
        <category>thoughts</category>
        
        
        <category>software-architecture</category>
        
      </item>
    
      <item>
        <title>Neural Computation: Toward an Intuitive Understanding of the Perceptron</title>
        <description>&lt;p&gt;Artificial Neurons is one of the most beautiful ways to simulate a biological behavior through computation, despite the fact that it’s not very close to the level of details of a real neuron. But it captured the core of what a neuron is doing.&lt;/p&gt;

&lt;p&gt;And I find that trying to understand this mathematical method by first understanding the concept through a metaphor &lt;em&gt;(which, in this case, I think that the metaphor is our mathematical side, and not the biological one, the biological is just, you know, the real thing)&lt;/em&gt; is really valuable to gain an intuition on this topic.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;If you were about to make a decision, given many variables, how would you make this decision? It may not look obvious, due the fact that, on normal occasions, we’re not thinking about our thinking process, but the truth is, we are collecting those variables that affect the future decision, and we’re giving weights to them. Weights? how? Simple, some variables are more crucial than other while making decision, isn’t it? the word &lt;em&gt;‘crucial’&lt;/em&gt;, in this case, means a huge weight on this variable, and it will strongly affect the final decision.&lt;/p&gt;

&lt;p&gt;Suppose a scenario: we’re deciding whether we should go to beach or not. Let’s put a few binary variables on table:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Are we in the mood to go to the beach?&lt;/li&gt;
  &lt;li&gt;Is it raining?&lt;/li&gt;
  &lt;li&gt;Our other friends are going?&lt;/li&gt;
  &lt;li&gt;Do we have money?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, depending on those variables, it will be more likely that we will go to the beach… or not. Let’s think about the variable &lt;em&gt;‘is it raining?’&lt;/em&gt;, if it’s raining, we can say that it’s a deal breaker, so we can conclude that this variable has more weights than the other. The fact is that we already trained those weights inside our brain, long time ago.&lt;/p&gt;

&lt;p&gt;That’s what an Artificial Neuron try to do, the AN try to learn the weights of things so it can make decisions. So, drawing back to the mathematical and computational aspect of it, the perceptron is a single unit that will receive external inputs&lt;em&gt;(variables)&lt;/em&gt;, it will have a weight for each input, it will do some computation, send this result to a decision function, and, finally, output the final decision.&lt;/p&gt;

&lt;p&gt;Those inputs are the variables we’re talking before, and like our process to make a decision, the Perceptron will weight each input to make a decision.&lt;/p&gt;

&lt;p&gt;Now, how the Perceptron knows how the weights should be for each input? Well… it doesn’t. At least, not from the beginning of the learning process, just like you and me, when trying to learn something new.&lt;/p&gt;

&lt;p&gt;That’s why this case is a case of supervised learning, what the Perceptron will do is: start with a random small weights, take one previously trained example &lt;em&gt;(e.g: (1) yes to mood to beach, (2) not raining, (3) yes to friends going to beach and (4) yes to money, the output: 1 - yes, we shall go to the beach)&lt;/em&gt; do some computation taking in consideration the random weights we defined before, send it to a decision function, output something &lt;em&gt;(1 - yes, 0 - no)&lt;/em&gt; and check if this output is equal to the expected &lt;em&gt;(we’re using a trained example, remember?)&lt;/em&gt;, of course that at the first try, it won’t be equal. So the Perceptron calculate the error rate and update its weights… after this, guess what? it repeat the process of trying to predict, but now, with the updated weights, after a few tries, the error rate tends to reduce and it starts predicting correctly. So it learned to predict a behavior, by training with previously trained examples.&lt;/p&gt;

&lt;p&gt;So, a nice learning behavior would be something like this:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;/content/images/images/perceptron_0.1.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Which means, at every iteration &lt;em&gt;(we call it epochs)&lt;/em&gt;, the error rate tend to decrease and, as consequence, the Perceptron starts to predict correctly!&lt;/p&gt;

&lt;p&gt;As you may have noted, I omitted a few details of the process so you could see the whole picture of the process: we take an example, we practice on it, we see what we missed, we try again, we learn. That’s the core process.&lt;/p&gt;

&lt;p&gt;Now, to the details:&lt;/p&gt;

&lt;h3 id=&quot;the-computation&quot;&gt;The computation&lt;/h3&gt;

&lt;p&gt;The first step that the Perceptron does it’s a computation that I referred as &lt;em&gt;“some computation”&lt;/em&gt;, this is a simple computation, it’s just a simple Linear Combination of the feature vector &lt;em&gt;(the variables)&lt;/em&gt; and the weight vector, which is just the sum of the products between feature/input and its respective weight:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}+ \cdots +x_{n}w_{n}&lt;/script&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;the-decision-function&quot;&gt;The Decision Function&lt;/h3&gt;
&lt;p&gt;So, after the Linear Combination, we send the result of it to a decision function, which will, somehow, based on some threshold &lt;em&gt;(or without a threshold as we’re going to see)&lt;/em&gt;, give us the output, that will be checked with the correct output, in case if it’s correct, fine, keep the weights like this, otherwise, it will calculate the error rate, adjust the weights and restart the process.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;binary-output-with-threshold&quot;&gt;Binary output with threshold&lt;/h4&gt;

&lt;p&gt;This technique is very simple, after the linear combination, if the output is bigger than some threshold, it outputs 1 and we say that the neuron was activated, otherwise, output 0.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
    X=
    \begin{cases}
      1, &amp; \text{if}\ Linear\,Combination&gt;0 \\
      0, &amp; \text{otherwise}
    \end{cases}
  \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;using-sigmoid-function&quot;&gt;Using sigmoid function&lt;/h4&gt;

&lt;p&gt;Now, that a interesting one, it won’t use a threshold anymore, we’ll send the output of the Linear Combination to a sigmoid function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(\vec{w}\vec{x}) = \dfrac{1}{1+e^{-\vec{w}\vec{x}}}&lt;/script&gt;

&lt;p&gt;which will, then, smooth the output, making it in the range of 0 and 1. Which is the most used in many Machine Learning Algorithms.&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;adjusting-the-weights-and-learning&quot;&gt;Adjusting the weights and learning&lt;/h3&gt;

&lt;p&gt;After the perceptron fails to predict correctly, it’s time to adjust the weight, using some rule, the classic perceptron has 2 rules:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;perceptron-learning-rule&quot;&gt;Perceptron Learning Rule&lt;/h4&gt;
&lt;p&gt;This is very simple, when the perceptron has the incorrect output, it update its weights following this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{i} = w_{i} + \eta (t_{i} - o_{i})x_{i}&lt;/script&gt;

&lt;p&gt;Which is simply multiplying a learning rate &lt;em&gt;(usually 0.001)&lt;/em&gt; by the difference between the correct output and the wrong output and then multiplying it by its original input, after this, we add this value to the previous weight and then we have the value of the new weight. 
It’s fair simple, isn’t it? The problem arises when the data isn’t linearly separable, like this:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/DBSCAN-density-data.svg/2000px-DBSCAN-density-data.svg.png&quot; width=&quot;400&quot; /&gt;
&lt;/div&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;With this scenario, the result just won’t converge. So we adopt another rule!&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;delta-rule&quot;&gt;Delta Rule&lt;/h4&gt;

&lt;p&gt;Now we must find a way to have a non-linear output, and what’s the best for this if not the classic Gradient Descent algorithm? GD will simply search through hypothesis spaces and try to minimize the cost function, I wrote about this &lt;a href=&quot;http://rodrigoaraujo.me/general/setup/demo/2015/01/09/Making-Your-Machine-Think-Learn-And-Predict--Gradient-Descent-Algorithm-in-Java.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;http://www.yaldex.com/game-development/FILES/17fig06.gif&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;So, it’s just a technique to solve our previous problem, but still, the weights will be updated &lt;em&gt;(now, using the GD)&lt;/em&gt; and then the process start over!&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3 id=&quot;code-of-a-perceptron&quot;&gt;Code of a Perceptron&lt;/h3&gt;

&lt;p&gt;Now, here’s a code of a Perceptron that will behave like a simple Boolean function. I didn’t use the Delta Dule (Gradient Descent) to optimize the weights, as this problem (binary Boolean functions) is linearly separable.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;
  &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;
  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
      
      &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
      this is the thereshold to activate the unit 
      &quot;&quot;&quot;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear_combination&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;
      
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;unit_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_combination&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;# initializing weights
&lt;/span&gt;          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
          
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# &amp;gt; calculate output with current weight
&lt;/span&gt;              &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unit_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# &amp;gt; calculate error rate (t - o)
&lt;/span&gt;              &lt;span class=&quot;n&quot;&gt;error_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
              &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'the X: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'output: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'correct target: '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# &amp;gt; apply learning rule which will update weights
&lt;/span&gt;              &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unit_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
                  

          &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                  
          &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'errors'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'epochs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;conclusion-and-next-steps&quot;&gt;Conclusion and next steps&lt;/h3&gt;

&lt;p&gt;With this we conclude what a simple single Perceptron is doing! Of course there are many improvements on it and we can connect many of these Artificial Neurons in many layers… that’s what is called Artificial Neural Network, I’ll be writing about it soon!&lt;/p&gt;

</description>
        <pubDate>Thu, 16 Jul 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/neural-computation-pt1</link>
        <guid isPermaLink="true">http://localhost:4000/post/neural-computation-pt1</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>engineering</category>
        
        <category>tutorial</category>
        
        <category>text classification</category>
        
        <category>NLP</category>
        
        <category>Sentiment Analysis</category>
        
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>Case Study: Sentiment Analysis On Movie Reviews</title>
        <description>&lt;p&gt;Well, this case was a fun one, sentiment analysis is a sub type of NLP &lt;em&gt;(natural language processing)&lt;/em&gt; in which you have to train a model to analyze a text, and classify it was a negative sentiment or a positive sentiment. In this case, I wanted to apply it on movies reviews, to see if a review is a negative one or a positive one. So, basically, teaching this kind of sentiment to the machine.&lt;/p&gt;

&lt;p&gt;There’s a lot of challenges in doing this kind of NLP, experience has told me that when the text is big, it is usually hard to predict things about it… and most of the times, movie reviews are big chunk of text.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;That’s the case where I needed to learn and use a few &lt;em&gt;extremely&lt;/em&gt; useful things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pipeline&lt;/li&gt;
  &lt;li&gt;Grid Search&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These two techniques made total difference in my results. Let’s go through the concept of these two:&lt;/p&gt;

&lt;h3 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h3&gt;

&lt;p&gt;In a &lt;a href=&quot;http://rodrigoaraujo.me/general/setup/demo/2015/06/30/A-Generic-Architecture-for-Text-Classification.html&quot; target=&quot;_blank&quot;&gt;previous post&lt;/a&gt; I talked about a generic Architecture or Flow to achieve basic text classification tasks, so, Pipeline is a way to automate all those tasks, so, instead of explicitly and separately select features, extract features, train the learning algorithms, you can do all of these things inside the pipeline, and, the amazing scikit learn API gives you an awesome Pipeline interface, pretty similar to a basic estimator’s interface, so you can call methods such as &lt;em&gt;fit, fit_transform, predict&lt;/em&gt;, etc…&lt;/p&gt;

&lt;p&gt;So you can do even more inside a pipeline, such as new methods for feature extraction/selection, FeatureUnion, use other estimators to select better features for your estimator &lt;em&gt;(do you even neural netception? LOL).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here’s an example of a simple pipeline that does PCA to reduce the dimension of the features and creates a SVC:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    from sklearn.pipeline import Pipeline
    from sklearn.svm import SVC
    from sklearn.decomposition import PCA
    estimators = [('reduce_dim', PCA()), ('svm', SVC())]
    clf = Pipeline(estimators)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;grid-search&quot;&gt;Grid Search&lt;/h3&gt;

&lt;p&gt;This was the answer for many hours of pure pain tweaking gazillions of parameters to improve the algorithm’s performance. Grid Search is a way to automate parameters changes, so it can run many times with many different parameters… and choose the best for you, magical, right?&lt;/p&gt;

&lt;p&gt;And, once again, the Grid Search object has an interface similar to the basic estimator’s interface, so you can call all the same methods. But, when you create the object grid search, you gotta pass a estimator as parameter… and here’s when things get interesting:&lt;/p&gt;

&lt;p&gt;You can pass a pipeline to the grid search. &lt;strong&gt;so much win!&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
	&lt;img src=&quot;http://cdn.meme.am/instances2/500x/581044.jpg&quot; /&gt;
&lt;/div&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;But, the truth is: this operation is &lt;em&gt;extremely&lt;/em&gt; expensive, and it’s recommended to use it only on the few first tries, where you don’t know exactly the best parameters values for the task.&lt;/p&gt;

&lt;p&gt;Another interesting thing is, after the model inside the grid search is trained, methods like &lt;em&gt;predict&lt;/em&gt; are computed using the best features. If we look inside the SKlearn’s grid search’s code, we can see how easily and elegant it’s done:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;grid_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_start&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_fits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_folds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_test_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;all_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_n_test_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_folds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;all_scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;this_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_n_test_samples&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;n_test_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_n_test_samples&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this_score&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_test_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_folds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# TODO: shall we also store the test_fold_sizes?
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;grid_scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_CVScoreTuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Store the computed scores
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_scores_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_scores&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Find the best parameters by comparing on the mean validation score:
# note that `sorted` is deterministic in the way it breaks ties
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_validation_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_params_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;best_score_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_validation_score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, in the end it keeps the best parameters and scores, so it can be used when it needs to predict or output its &lt;em&gt;predict_proba&lt;/em&gt; or &lt;em&gt;decision_function&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;correctly-mixing-pipeline-grid-search-and-cross-validation-correctly-the-hidden-wizardry&quot;&gt;Correctly mixing Pipeline, Grid Search and Cross Validation correctly: the hidden wizardry&lt;/h3&gt;

&lt;p&gt;It’s tricky to mix all these things up, the most dangerous mistake that everybody does sometime is: &lt;strong&gt;using the same dataset to the grid search AND the cross validation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So, here’s a nice technique to avoid this: Split the initial dataset into Development Dataset, which will be used to train the algorithm and to execute the grid search, and the Validation Dataset, which will be passed to the cross_val_score and internally splitted again to avoid bias (CV parameter can configure this).&lt;/p&gt;

&lt;p&gt;So, in the end, you train the grid search with a subset of the dataset, and validate it with another subset, avoiding many kind of undesired effects.&lt;/p&gt;

&lt;h3 id=&quot;the-movie-review-problem&quot;&gt;The Movie Review Problem&lt;/h3&gt;

&lt;p&gt;Ok, so now let’s head to the real problem. The dataset can be downloaded with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz
tar xzf review_polarity.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What I did to solve it was: I used grid search to search a few parameters, such as &lt;em&gt;TfidfVectorizer’s max_features&lt;/em&gt;, that in the end I kept with max_features=10000, also its ngram_range, searching between (1,1) and (1,2) and &lt;em&gt;the LinearSVC()’s C parameter&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I used a few other algorithms, but I found the best performance with LinearSVC, though I did not try all the other possibilities &lt;em&gt;(I could create many pipelines to test many algorithms, but my computer would be unusable during many hours, probably, which I couldn’t afford)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Why Linear SVC?&lt;/p&gt;

&lt;p&gt;The Linear SVC is very similar to the SVC object but using the &lt;em&gt;kernel = “linear”&lt;/em&gt;. LinearSVC is a type of &lt;em&gt;Support Vector Machines&lt;/em&gt;, it’s known that SVMs are effective in &lt;strong&gt;high dimensional spaces&lt;/strong&gt;, which is our case here. Also, it can give weights to classes, helping to go through unbalanced datasets
&lt;img src=&quot;http://scikit-learn.org/stable/_images/plot_separating_hyperplane_unbalanced_0011.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And, basically, what the SVMs does is create a hyper-plane (or a set of it) in a high or even infinite dimensional space. And it solves the primal problem:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;http://scikit-learn.org/stable/_images/math/396704acdf11cc18d2d02b32275c0ee42d76b95e.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Where Xi are the training vectors and Y is a vector in {1,-1}^n.&lt;/p&gt;

&lt;p&gt;So, here’s the code where I built the pipeline, the gridsearch, use a subset of the dataset to develop the grid search and another subset to validate using the &lt;em&gt;cross_val_scores&lt;/em&gt;, then, I call &lt;em&gt;predict&lt;/em&gt; to get the vector of predictions to build a report &lt;em&gt;(or, with this, I can start to build a ROC curve, confusion matrix and other statistics debug tools)&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from sklearn.datasets import load_files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.grid_search import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC
from sklearn.cross_validation import train_test_split
from sklearn.cross_validation import cross_val_score
from sklearn.metrics import classification_report

data = load_files('../txt_sentoken/')

X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.5, random_state=0)

pipeline = make_pipeline(TfidfVectorizer(sublinear_tf=True, max_features=10000), LinearSVC())

parameters = {
        'tfidfvectorizer__ngram_range': [(1,1), (1,2)],
        'linearsvc__C':(.01,.1,1),
        }

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1) 

grid_search.fit(X_train, y_train)

print(&quot;Best score: %0.3f&quot; % grid_search.best_score_)
print(&quot;Best parameters iset:&quot;)
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameters.keys()):
    print(&quot;\t%s: %r&quot; % (param_name, best_parameters[param_name]))

print &quot;The model was trained on the full development set&quot;
print &quot;the scores are going to be computed with the evaluation set&quot;
scores = cross_val_score(grid_search, X_test, y_test, cv=5)

print scores.mean(), scores.std()

y_pred = grid_search.predict(X_test)

print classification_report(y_test, y_pred)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and the output:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Best score: 0.849

Best parameters iset:
	linearsvc__C: 1
	tfidfvectorizer__ngram_range: (1, 2)

The model was trained on the full development set
the scores are going to be computed with the evaluation set
0.865933623341 0.0281523948704

Classification report:
             precision    recall  f1-score   support

          0       0.88      0.86      0.87       496
          1       0.87      0.88      0.87       504

avg / total       0.87      0.87      0.87      1000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, around 86%~87% of precision/f1-score, not &lt;em&gt;that&lt;/em&gt; bad.&lt;/p&gt;

&lt;h3 id=&quot;further-improvements&quot;&gt;Further Improvements&lt;/h3&gt;

&lt;p&gt;There are many things I could to do elevate this performance, but due the lack of time to play with this dataset, those improvements will be in a next post.&lt;/p&gt;

&lt;p&gt;Possible improvements could be: find and extracting new features, to do this, we gotta understand better this data, extract more information about it. Trying new algorithms and techniques such as Ensemble, Classifiers Combination or Fusion. Stemming and Stop Words could improve it too. If you solved this problem and got a better result, share it with me, I’m very curious about how to improve this!&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jul 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/case-study-sentiment-analysis-movie-reviews</link>
        <guid isPermaLink="true">http://localhost:4000/post/case-study-sentiment-analysis-movie-reviews</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>engineering</category>
        
        <category>tutorial</category>
        
        <category>text classification</category>
        
        <category>NLP</category>
        
        <category>Sentiment Analysis</category>
        
        
        <category>machine-learning</category>
        
        <category>case-study</category>
        
      </item>
    
      <item>
        <title>A Generic Architecture for Text Classification with Machine Learning</title>
        <description>&lt;p&gt;One of the most commons tasks in Machine Learning is text classification, which is simply teaching your machine how to read and interpret a text and predict what kind of text it is.&lt;/p&gt;

&lt;p&gt;The purpose of this essay is to talk about a simple and generic enough Architecture to a supervised learning text classification. The interesting point of this Architecture is that you can use it as a basic/initial model for many classifications tasks.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;supervised-learning&quot;&gt;Supervised Learning&lt;/h2&gt;

&lt;p&gt;If you’re already familiar with this concept, just jump this step, but I feel it’s important to beginners to know.&lt;/p&gt;

&lt;p&gt;Supervised Learning is when you have to first train your model with already existing labeled dataset, just like teaching a kid how to differentiate between a car and a motorcycle, you have to expose its differences, similarities and such. Whereas unsupervised learning is about learning and predicting without a pre-labeled dataset.&lt;/p&gt;

&lt;h2 id=&quot;starting-to-sketch-the-architecture&quot;&gt;Starting to sketch the Architecture&lt;/h2&gt;

&lt;p&gt;With the dataset in hands, we start to think about how is going to be our architecture to achieve the given goal, we can resume the steps in:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Cleaning the dataset&lt;/li&gt;
  &lt;li&gt;Partitioning the dataset&lt;/li&gt;
  &lt;li&gt;Feature Engineering&lt;/li&gt;
  &lt;li&gt;Chosing the right Algorithms, Mathematical Models and Methods&lt;/li&gt;
  &lt;li&gt;Wrapping everything up&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;cleaning-the-dataset&quot;&gt;Cleaning the dataset&lt;/h2&gt;

&lt;p&gt;Cleaning the dataset is a crucial initial step in Machine Learning, many Toy Datasets don’t need to be cleaned, because it’s already clean, peer-reviewed and published in a way you can use it exactly to work on the learning algorithms.&lt;/p&gt;

&lt;p&gt;The problem is:&lt;/p&gt;

&lt;h4 id=&quot;the-real-world-is-full-of-painful-and-noisy-datasets&quot;&gt;The real world is full of painful and noisy datasets&lt;/h4&gt;

&lt;p&gt;If there’s one thing I learned while working with Machine Learning is, there’s no such thing as shiny and perfect dataset in the real world, so we have to deal with this beforehand. Situations where there are many empty fields, wrong and non-homogeneous formats, broken characters, is very common. I won’t talk about such techniques now, but I will write something about it in another post.&lt;/p&gt;

&lt;h2 id=&quot;partitioning-the-dataset&quot;&gt;Partitioning the Dataset&lt;/h2&gt;

&lt;p&gt;We always need to partition the dataset in, at least, 2 partitions: the training dataset and the test/validation dataset. Why?&lt;/p&gt;

&lt;p&gt;Suppose we fed the learning algorithm with a training data X and it already known the output Y (because it’s a training data pair (X,Y)), which is, for given text X, Y is its classification, the algorithm will learn it.&lt;/p&gt;

&lt;p&gt;Great, the algorithm learned this. But now, we’re going to validate the learning, so we use the same data X, I mean, we pass X to the model and ask what’s its classification…&lt;/p&gt;

&lt;p&gt;Do you see the problem here?&lt;/p&gt;

&lt;p&gt;Of course the algorithm will output Y, the same Y we passed to its training. So, if we pass the complete dataset D in the training phase, then we validate the model using the SAME D dataset, we will be steping onto this very same situation. It’s like cheating, it’s like we point to a car and say “this is a car”, then, at the same time and with the same car, we ask to the kid “is this a car?”, it will be highly probable that the kid will answer correctly, though it may not learned correctly. What we must do is, point to a car and say “this is a car”, and then, point to a different car and ask “is this a car?”.&lt;/p&gt;

&lt;p&gt;Stepping out of the metaphor, we must check if the machine learned correctly by using a diferent portion of the dataset.&lt;/p&gt;

&lt;p&gt;So, at the end of this step, we’ll have the training dataset and the test dataset, both are subset of the same initial dataset.&lt;/p&gt;

&lt;p&gt;With Python’s Scikit Learn you can do this easily using &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html&quot; target=&quot;_blank&quot;&gt;Train Test Split&lt;/a&gt; &lt;em&gt;(read the docs, it’s very simple to use it.)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/ml1.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;##Feature Engineering&lt;/p&gt;

&lt;p&gt;This is one of the most important steps when doing Machine Learning. Briefly, Features are the data that the learning algorithm will use as the “X”, which will be used to compute and understand the patterns. A simpler example would be a non-text classification or a regression, e.g: house pricing predictions, where our Features could be: number of rooms, size in squared meters, location and more. As you can see, these feature will describe the patterns of each data point and will affect how the house will be priced.&lt;/p&gt;

&lt;p&gt;The crucial point is, features can vary, you can identify new features that can level up your model’s predictions in huge proportions, a simple example would be, in a text classification you count the frequency of each word, this is one feature… you feed your learning algorithm with it… and the result isn’t enough: 50% of precision. But then you see that, in this case, the length of the whole data point (the text) plays a huge role determining a pattern for each class. Now that you &lt;strong&gt;identified&lt;/strong&gt; this feature, you do something to &lt;strong&gt;extract&lt;/strong&gt; that feature and then add this new feature and feed your learning algorithm with it, then, the result: 95% of precision.&lt;/p&gt;

&lt;p&gt;Ok, so you know the importance of the feature engineering phase, now it’s time to understand the most common technique to extract feature in texts: &lt;strong&gt;TF-IDF&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;tf-idf&quot;&gt;TF-IDF&lt;/h4&gt;

&lt;p&gt;Though the most intuitive way to look for patterns in texts is to count each word in the text (and use it as a Feature), it may not be the best way to do it. A few reasons for it is, larger texts will have higher averages than the shorter texts, these discrepancies can hurt the learning algorithm, and this Feature &lt;strong&gt;doesn’t say much about the importance of the words&lt;/strong&gt;, which is very important to find patterns.&lt;/p&gt;

&lt;p&gt;So, instead of computing the occurrence, it’s better to compute the importance of the words. to accomplish this we can use 2 statistic’s techniques:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TF&lt;/strong&gt; - which is basically the raw frequency of the word given the document, raw frequency of t by f(t,d), then the simple tf scheme is tf(t,d) = f(t,d).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;IDF&lt;/strong&gt; - Inverse Document Frequency, which is a technique to give emphasis to words that &lt;em&gt;don’t appear with high frequency&lt;/em&gt;, because these are the words that can differentiate the texts, which means, in this case, these are the most important words, so we inverse its frequencies. So, an example, if the word &lt;em&gt;“the”&lt;/em&gt; happens to appear very often in a text, it will weight &lt;strong&gt;less&lt;/strong&gt;, because it’s a common word, thus, don’t cause very impact when fiding patterns to differentiate the texts.&lt;/p&gt;

&lt;p&gt;So the whole TF-IDF can be computed by&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/e/8/1/e81492e44713270fd230d821ccebd100.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scikit Learn gives us a great API to use the TF-IDF method, it’s really simple.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    from sklearn.feature_extraction.text import TfidfVectorizer

    vectorizer = TfidfVectorizer()
    vectorized_x = vectorizer.fit_transform(X)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which will do the TF-IDF on the X data, and then vectorize this data, in other words, transform the whole thing into an array of inverse frequencies.
So, extracting those features, &lt;strong&gt;this&lt;/strong&gt; will be our training and test data, that will feed the algorithm (along with the Y, which is the output, the class of each training data).&lt;/p&gt;

&lt;p&gt;Re-thinking our Architecture, now we have:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/ml2.svg&quot; /&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Remembering that X is the set of features (e.g: vector with the TF-IDF of each data point) and Y is the output, which is, the labels/class (e.g: Spam or not-spam).&lt;/p&gt;

&lt;h2 id=&quot;chosing-the-right-algorithms-mathematical-models-and-methods&quot;&gt;Chosing the right Algorithms, Mathematical Models and Methods&lt;/h2&gt;

&lt;p&gt;With the data prepared, features selected and extracted, it’s time to feed the algorithm with this data, this topic &lt;em&gt;per se&lt;/em&gt; could go pages and pages, as learning algorithms is such a huge fields, with many publications and ideas to solve every kind of problem.&lt;/p&gt;

&lt;p&gt;To not extend it very much, and as the purpose of this essay is to discuss the architecture, I’ll use a few commons algorithms, such as Logistic Regression, Decision Trees, SVM and Neural Networks.&lt;/p&gt;

&lt;p&gt;So, at this point, we’ll treat the learning algorithm as a black-box algorithm, where it:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;receive a training data X which is a vector of features, and training data Y, which is the label/class/output (we can binarize it, such as spam=1, ham=0)&lt;/li&gt;
  &lt;li&gt;return a model, where given an text X, can output its predicted class Y.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After generating this model, we will test it with our test dataset and check its performance, if it can predict correctly &lt;em&gt;(the test dataset has output values (Y) so we can check them)&lt;/em&gt;, it is ready to predict new data &lt;em&gt;(data completely outside our initial dataset)&lt;/em&gt;, so we say that the machine learned the task.&lt;/p&gt;

&lt;p&gt;Our architecure now:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/ml3.svg&quot; height=&quot;800&quot; /&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;wrapping-everything-up&quot;&gt;Wrapping everything up&lt;/h2&gt;

&lt;p&gt;With this architecture, we should be able to do most of the simple text classification tasks, as the main flow is: get data, clean data, identify and extract features, train your algorithm/mathematical model of choice, validate it and then, use the generated model to do the estimations.&lt;/p&gt;

&lt;p&gt;Of course there are many improvements that can be made on this architecture and many, many, many lower level details, but you can see this architecture as a &lt;em&gt;“boilerplate code”&lt;/em&gt; to get you started with the machine learning engineering task.&lt;/p&gt;

&lt;p&gt;A few tips:&lt;/p&gt;

&lt;p&gt;Learn the underlying mathematical models of the most commons learning algorithms, this will teach you the trade offs of each one, so you can apply the correct algorithms to the given dataset and scenario. For example, SVM can be good for unbalanced dataset, but why? You gotta know this. Neural Networks can be slow to be trained, but, if training time is not critical, it’s okay to use Neural Networks.&lt;/p&gt;

&lt;p&gt;Master the skills to clean data, if using Python, learn to use Pandas. This will be an extremely important skill.&lt;/p&gt;

&lt;p&gt;Master the skills to understand data, this will be crucial to make you see what algorithm to use, you can’t make something learn if you don’t know about what you are teaching.&lt;/p&gt;

</description>
        <pubDate>Tue, 30 Jun 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/generic-architecture-text-classification</link>
        <guid isPermaLink="true">http://localhost:4000/post/generic-architecture-text-classification</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>engineering</category>
        
        <category>tips</category>
        
        <category>architecture</category>
        
        <category>text classification</category>
        
        
        <category>machine-learning</category>
        
        <category>software-architecture</category>
        
      </item>
    
      <item>
        <title>Case Study: Python Performance on rotating one-dimensional vectors</title>
        <description>&lt;p&gt;I’m a big fan of a nice challenge, therefore, I like books like Programming Pearls, I like to dive into many kinds of solutions to the same problem and try to differentiate them by novelty, performance, elegance, etc…&lt;/p&gt;

&lt;p&gt;This time I was playing with a fun problem, from the column 2:&lt;/p&gt;

&lt;!--more--&gt;

&lt;blockquote&gt;
  &lt;p&gt;“rotate a one dimensional array of &lt;em&gt;N&lt;/em&gt; elements left by &lt;em&gt;I&lt;/em&gt; positions”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The author says it should consume little space and time, so, there are many solutions, obviously. The fun thing is that I was doing it in Python, so you can solve it in many ways, but you can decrease the performance a lot if you choose poorly. In C we could just swap pointers in a doubly linked list, which is what happens in the real implementation of CPython.&lt;/p&gt;

&lt;p&gt;The idea that the Author exposed is pretty clever, which is based in reversing the vector only 3 times:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reverse the vector, from the first position to the &lt;em&gt;I&lt;/em&gt;-th position, where &lt;em&gt;I&lt;/em&gt; is the number of position that is wanted to move to the left&lt;/li&gt;
  &lt;li&gt;Reverse the vector, from &lt;em&gt;I&lt;/em&gt;-th + 1 position to &lt;em&gt;N&lt;/em&gt;, where &lt;em&gt;N&lt;/em&gt; is the size of this vector&lt;/li&gt;
  &lt;li&gt;Reverse the whole vector, again.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In other words, we have the vector &lt;em&gt;AB&lt;/em&gt; (Where &lt;em&gt;A&lt;/em&gt; is the first part, &lt;em&gt;B&lt;/em&gt; is the second), we reverse &lt;em&gt;A&lt;/em&gt; so we have &lt;em&gt;Ar&lt;/em&gt;, reverse &lt;em&gt;B&lt;/em&gt; so we have &lt;em&gt;Br&lt;/em&gt;, now we have &lt;em&gt;ArBr&lt;/em&gt;, then we reverse the whole thing &lt;em&gt;(ArBr)r&lt;/em&gt;, after that, we have the rotated vector. It’s quite beautiful. A picture can help the visualization:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/img1.jpeg&quot; height=&quot;450&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;here’s an example with a simple vector:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/img2.jpeg&quot; height=&quot;350&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;Now, searching for other approaches, I looked inside the code from CPython, and there’s a nice comment:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &quot;Conceptually, a rotate by one is equivalent to a pop on one side and an append on the other&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Quite elegant solution as well. So, an example:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/img4.jpeg&quot; height=&quot;350&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;And the last example I found while searching through Pythonic ways to solve this problem (Even though may not be very fast)&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
def rotate_pythonic_way(arr, i):
         return arr[i:] + arr[:i] 
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Which is super simple and elegant, what it’s doing is the following:&lt;/p&gt;

&lt;div class=&quot;imgcenter&quot;&gt;
&lt;img src=&quot;/content/images/images/img3.jpeg&quot; height=&quot;350&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;At first I thought this solution would be the slowest, but we’ll get to that.&lt;/p&gt;

&lt;p&gt;One thing that took my attention was the internal method to reverse a list that Python offers, there are two of them, one returns a reversed iterator and you can cast it into a list and the other reverse the exactly same list that is passed as parameter, it shouldn’t take you long to realize which is faster. So I wrote a code with the many solutions to it and I profiled the functions, which gave us a interesting result when ran over a vector with 10 millions integers.&lt;/p&gt;

&lt;p&gt;First, the original idea (&lt;em&gt;reverse the vector 3 times&lt;/em&gt;), but using the reversed(array) method:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
def rotate_original_solution_with_reversed(arr, i):
    n = len(arr)
    array_A_reversed = list(reversed(arr[0:i]))
    array_B_reversed = list(reversed(arr[i:n]))
    ArBr = array_A_reversed + array_B_reversed
    rotated_array = list(reversed(ArBr))
    return rotated_array
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;and the result:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    Original Idea using reversed(arr): 

    4 function calls in 0.611 seconds

    Ordered by: standard name

    ncalls tottime percall cumtime percall filename:lineno(function)
    1 0.440 0.440 0.440 0.440 2_1b.py:21(rotate_original_solution_with_reversed)
    1 0.171 0.171 0.611 0.611 string:1 module
    1 0.000 0.000 0.000 0.000 {len}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;####Damn, 0.6s, that’s too slow!&lt;/p&gt;

&lt;p&gt;After this one, I changed the way I was reversing the vector, using the other internal method from Python:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
def rotate_original_solution_with_reverse(arr, i):
    n = len(arr)
    
    array_A = arr[0:i]
    array_B = arr[i:n]
    
    array_A.reverse()
    array_B.reverse()

    ArBr = array_A + array_B
    ArBr.reverse()
    return ArBr
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;And the result:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    Original Idea using arr.reversed(): 
    7 function calls in 0.322 seconds

    Ordered by: standard name

    ncalls tottime percall cumtime percall filename:lineno(function)
    1 0.187 0.187 0.210 0.210 2_1b.py:30(rotate_original_solution_with_reverse)
    1 0.113 0.113 0.322 0.322 string:1 module
    1 0.000 0.000 0.000 0.000 {len}
    1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}
    3 0.023 0.008 0.023 0.008 {method 'reverse' of 'list' objects}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;####From 0.6s to 0.3s, that’s a great improvement. lesson: choose your built-ins/Data Structures/Algorithms carefully.&lt;/p&gt;

&lt;p&gt;And now, using the Pythonic Way, which I was thinking that would be the slowest:&lt;/p&gt;

&lt;p&gt;And the result:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;bash hljs&quot;&gt;
Pythonic Way: 

3 function calls in 0.295 seconds

Ordered by: standard name

ncalls tottime percall cumtime percall filename:lineno(function)
1 0.238 0.238 0.238 0.238 2_1b.py:45(rotate_pythonic_way)
1 0.056 0.056 0.295 0.295 string:1(module)
1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}

&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;It was faster than the other methods! Well played, Python.&lt;/p&gt;

&lt;p&gt;And the last one, using the pop/append technique, repeated i times:&lt;/p&gt;

&lt;p&gt;And the result:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;bash hljs&quot;&gt;

Pop and append way: 

5 function calls in 0.013 seconds

Ordered by: standard name

ncalls tottime percall cumtime percall filename:lineno(function)
1 0.000 0.000 0.013 0.013 2_1b.py:49(rotate_pop_append)
1 0.000 0.000 0.013 0.013 string:1(module)
1 0.000 0.000 0.000 0.000 {method 'append' of 'list' objects}
1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects}
1 0.013 0.013 0.013 0.013 {method 'pop' of 'list' objects}
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Ok. That was weird. 0.013 seconds?  13 milliseconds? Crazy, right? I thought the fact of repeating it many times would make it go way slower, but I guess I was wrong!&lt;/p&gt;

&lt;p&gt;So, as you can see, an interesting problem can have many solutions, one more elegant, others faster. In the end you may pick the simplest and most intuitive solution, this may be the best for the situation. Some times you gotta go with the strangest and most non-intuitive solution (which reminds me the first time I saw the Quick Sort and all its non-intuitive way to think)&lt;/p&gt;

&lt;p&gt;So, if you’re willing to reverse a list with Python, go with List.reverse() method (unless you want the iterators to do something), and if you want to rotate a vector, go with pop/append, it looks faster.&lt;/p&gt;
</description>
        <pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/case-study-python-performance-rotating-vector</link>
        <guid isPermaLink="true">http://localhost:4000/post/case-study-python-performance-rotating-vector</guid>
        
        
        <category>algorithms</category>
        
        <category>case-study</category>
        
        <category>python</category>
        
        <category>profiling</category>
        
      </item>
    
      <item>
        <title>Web Scaling - using Redis as Cache</title>
        <description>&lt;p&gt;&lt;strong&gt;Redis is such a great technology.&lt;/strong&gt; Unfortunately, there’s still people who don’t know Redis or don’t know that Redis can be used as a Cache System to improve the speed of responses.&lt;/p&gt;

&lt;h3 id=&quot;why-redis&quot;&gt;Why Redis&lt;/h3&gt;

&lt;p&gt;Well, let’s start this discussion remembering how a common Relational Database basically works: Suppose we’re using a MySQL, every time your app sends a request to the MySQL client, the MySQL client gotta make a trip to the hard drive to get the data asked in the request, this can become a problem if the data asked in request is big… and if there are many requests at the same time, this can generate a huge latency, annoying users or worse.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;This is where Redis comes into play, Redis is a key-value database that will be running and storing data inside your memory, if you remember the basic of computers architecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/memchart.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s way faster to access data in memory (Physical RAM, main memory) than to access data in the Hard Drive, so it’s easy to notice that if the data that the application wants to access is inside the main memory, it’s way easier to reach to that data than if it was stored in the Hard Drive.&lt;/p&gt;

&lt;p&gt;So, like I said, Redis will be storing its data inside the memory, but you may ask yourself: “but what if I turn off the machine?? isn’t the ram memory volatile? “ Yes, that’s why Redis will be flushing the data to the hard drive from time to time, it’s up to you to choose this time between flushes, it’s all about Performance vs. Security.&lt;/p&gt;

&lt;p&gt;So, What we’ll be doing is just:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/atv1.png&quot; width=&quot;350&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;note that this print is taken from a talk I gave in my country, so it’s in portuguese. aplicação = application, Não acha key = key don’t found, retorna dados = return data&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As you can see: So much win. We avoided redundant trips to the disk.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now that we understand the concept of what we’ll be doing, the code becomes very easy to implement, here’s a simple idea &lt;em&gt;(thougt it can be improved and extended in many ways, but it can demonstrate the idea we’re working here)&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
    from flask import Flask
    from flask.ext.sqlalchemy import SQLAlchemy
    from sqlalchemy import create_engine
    import redis

    app = Flask(__name__)
    url = 'mysql+pymysql://username:password@ip/dbname'
    app.config['SQLALCHEMY_DATABASE_URI'] = url
    db = SQLAlchemy(app)
    cache = redis.StrictRedis(host='localhost', port=6379, db=0)

    #improvement: model could stay in a different file
    class User(db.Model):
        id = db.Column(db.Integer, primary_key=True)
        username = db.Column(db.String(80))
        email = db.Column(db.String(120))

        def __init__(self, username, email):
            self.username = username
            self.email = email

        def __repr__(self):
            return '' % self.username

    def createUsers():
            for x in xrange(0,100000):
                user = User('test', 'test')
                db.session.add(user)
            db.session.commit()

    def getUsers():
            users = cache.get('users')
            if not users:
                users = User.query.all()
                cache.set('users', users)

    # improvement: views/routing should stay in a different file
    @app.route('/')
    def hello_world():
        getUsers()
        return 'Hello Worldd!'

    if __name__ == '__main__':
        app.debug=True
        app.run(host='0.0.0.0')
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;So, after you create the correct database, populate the database (there’s a function in the code for that) and change username/password/dbname in the code, we’ll run the app.py and go to localhost:port-you-exposed.&lt;/p&gt;

&lt;h3 id=&quot;what-will-happen&quot;&gt;&lt;em&gt;What will happen?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;1. The first time you access it, it will take a few seconds to get the data from the MySQL&lt;/p&gt;

&lt;p&gt;2. The second time you access it, it will take just a few milliseconds to get the same data from redis&lt;/p&gt;

&lt;p&gt;In my computer the result was:&lt;/p&gt;

&lt;p&gt;first access: 45861 milliseconds&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;second access: 5ms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I know, right? That’s just blazing fast!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;And it can save a lot of computational resources and human time. Now, with this logic applied to one method (the method to get users), we can apply it to whichever method we want, or we can even create a generic decorator and annotate the methods that we want do the caching!!&lt;/p&gt;
</description>
        <pubDate>Mon, 25 May 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/web-scaling-using-redis-as-cache</link>
        <guid isPermaLink="true">http://localhost:4000/post/web-scaling-using-redis-as-cache</guid>
        
        <category>redis</category>
        
        <category>web</category>
        
        <category>scalability</category>
        
        <category>programming</category>
        
        <category>python</category>
        
        <category>programming</category>
        
        <category>engineering</category>
        
        
        <category>python</category>
        
        <category>software-architecture</category>
        
        <category>software-scalability</category>
        
        <category>caching</category>
        
      </item>
    
      <item>
        <title>Building your first REST API with Python</title>
        <description>&lt;p&gt;Are you total lost in this world full of jargons like: API, Rest API, microservices and stuffs? Come here, sit, grab a cup of coffee, and let’s talk briefly about it.&lt;/p&gt;

&lt;p&gt;Today a lot is said about APIs. Everything has an API, every programmer (newbie to expert) uses tons of API. Also, today we can see a lot of people talking about microservices and the idea of total separation of backend, frontend, web services or &lt;em&gt;whatever&lt;/em&gt;. So we can &lt;em&gt;(and we do)&lt;/em&gt; hear a lot about REST/RESTful APIs. We have 3 current problems with it:&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;1. Many new programmers don’t have a single clue of what a Rest API is.&lt;/p&gt;

&lt;p&gt;2. It’s probable that they’re using Rest APIs and don’t know about it.&lt;/p&gt;

&lt;p&gt;3. When they feel that they should learn more about it, there’s hardly any good and accessible material to learn it.&lt;/p&gt;

&lt;h3 id=&quot;so-heres-a-extremely-simplistic-approach-to-try-to-explain-what-is-a-rest-api&quot;&gt;So here’s a extremely simplistic approach to try to explain what is a Rest API&lt;/h3&gt;

&lt;p&gt;Starting with the concept of API, which stands for &lt;strong&gt;A&lt;/strong&gt;pplication &lt;strong&gt;P&lt;/strong&gt;rogramming &lt;strong&gt;I&lt;/strong&gt;nterface, it’s just an interface which you, &lt;em&gt;dear programmer&lt;/em&gt;, will be dealing with to extract whatever you &lt;em&gt;(or your program)&lt;/em&gt; want.&lt;/p&gt;

&lt;p&gt;Suppose your program needs to create a connection with a given database, normally you do:&lt;/p&gt;

&lt;p&gt;1. You import the &lt;em&gt;library&lt;/em&gt; that will abstract the connection with the database&lt;/p&gt;

&lt;p&gt;2. You create an object to represent a Connection with the database&lt;/p&gt;

&lt;p&gt;3. You call functions that this object provide to you, so you can do whatever you want (and whatever they provide)&lt;/p&gt;

&lt;p&gt;Here’s a silly example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from database_library import Connection

connection = Connection()
connection.OpenConnection()
… do whatever you want
connection.CloseConnection()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When you called those function, you were dealing with the interface that the object provided to you, sure the object may be doing thousand of things at the moment you call its functions, but, it doesn’t matter to you, does it? You just want the &lt;em&gt;damn&lt;/em&gt; connection open and then closed.&lt;/p&gt;

&lt;p&gt;So the library that you imported is giving you something you want, offering a service or a resource. That’s the sole purpose of an API. It’s a layer that you can use to get things from other(s) component(s).&lt;/p&gt;

&lt;p&gt;So, it’s easy to deduct that, the better the engineer planned the API, the easier it will be to deal with and extract what you want, and the contrary is true.&lt;/p&gt;

&lt;p&gt;But still, you’re processing the core of this API in your own machine, which isn’t that great, here’s when the API evolves to whole web hosted services, so you can request this API something, and this API can give you something, through the WEB, via HTTP request. And that’s amazing.&lt;/p&gt;

&lt;p&gt;And the community, recently, decided that the request and response that happen between applications and APIs, should be done with JSON, so this communication can become uniform and APIs can talk to other APIs effortlessly.&lt;/p&gt;

&lt;p&gt;So, basically, what’s going on is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/json-rest3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So… yes, you make your function calls, now, with just a simple URL + HTTP methods, wanna see a real example? We can just send a request to the Facebook’s API by accessing this URL: &lt;a href=&quot;http://graph.facebook.com/contatodigo&quot;&gt;http://graph.facebook.com/contatodigo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Which will request my profile’s data, and the Facebook’s API will return:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
   &quot;id&quot;: &quot;100001638888259&quot;,
   &quot;first_name&quot;: &quot;Rodrigo&quot;,
   &quot;gender&quot;: &quot;male&quot;,
   &quot;last_name&quot;: &quot;Ara\u00fajo&quot;,
   &quot;link&quot;: &quot;https://www.facebook.com/contatodigo&quot;,
   &quot;locale&quot;: &quot;pt_BR&quot;,
   &quot;name&quot;: &quot;Rodrigo Ara\u00fajo&quot;,
   &quot;username&quot;: &quot;contatodigo&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As simple as that.&lt;/p&gt;

&lt;p&gt;Now with this basic idea explained, we can do a simple Rest API using Python and Flask. (I’m assuming you’re already familiar with both technologies).&lt;/p&gt;

&lt;p&gt;All we’re gonna do is, using the flask routing, create routes to the users so they can interact with the resources of our API. Let’s suppose our API will serve and receive only Books. So this is the resource we’re dealing with, users may use our API to get Books and insert new Books, so our only URIs will be:&lt;/p&gt;

&lt;p&gt;1. &lt;em&gt;/bookapi/v1.0/books&lt;/em&gt; with a GET method, which will just return the list of books&lt;/p&gt;

&lt;p&gt;2. &lt;em&gt;/bookapi/v1.0/books&lt;/em&gt; with a POST method, which will insert a book&lt;/p&gt;

&lt;p&gt;So after the API is built, you or you program can get books or insert books by sending HTTP request to the API’s URIs, simple as that. I’ll be very straight forward, here’s the code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#!flask/bin/python
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;books&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'Game of Thrones'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'description'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'Cool dragons'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;s&quot;&gt;'finished'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'50 shadows of grey'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'description'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;u'It start as bullshit, end as a huge bullshit'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;s&quot;&gt;'finished'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/bookapi/v1.0/books/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'GET'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_books&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'books'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;books&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/bookapi/v1.0/books'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'POST'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;abort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;404&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;book&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;books&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'description'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'description'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'finished'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;books&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'book'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;201&lt;/span&gt; 

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that we’re creating a simple in-memory &lt;em&gt;database&lt;/em&gt;, which is a simple python’s dict. This could be a database. But for the sake of simplicity, I’m using just a dict.&lt;/p&gt;

&lt;p&gt;When the requests come, it verifies the route that the user is asking for, which is: what resources is he/she wanting? and then, the code do whatever it must do (&lt;em&gt;remember the API idea of hiding the complexity, the user requesting just want the result&lt;/em&gt;), and then it put everything in a JSON and returns it. That simple!&lt;/p&gt;

&lt;p&gt;Of course, many improvements and extensions (&lt;em&gt;there are infinity possibilities&lt;/em&gt;) can be made to this code, but, here’s a skeleton of the idea of an API, it’s just a start. One good practice it’s to &lt;em&gt;not&lt;/em&gt; return the ID of the resource, but return just its URI, which is surely a great idea. Another good practice is to ask authentication in every HTTP request, so your API won’t be exposed to everybody. There are, indeed, endless improvements, but in this code, you, that are completely beginner to the API’s concept, can now understand what’s going on underneath and start planning and building your own API using Flask.&lt;/p&gt;
</description>
        <pubDate>Wed, 22 Apr 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/build-your-first-rest-api-python</link>
        <guid isPermaLink="true">http://localhost:4000/post/build-your-first-rest-api-python</guid>
        
        <category>api</category>
        
        <category>python</category>
        
        <category>programming</category>
        
        <category>engineering</category>
        
        <category>rest</category>
        
        <category>flask</category>
        
        
        <category>python</category>
        
        <category>software-architecture</category>
        
        <category>api</category>
        
      </item>
    
      <item>
        <title>Using Python and AI to predict types of wine</title>
        <description>&lt;p&gt;I’ve been working with AI/Machine Learning at &lt;a href=&quot;http://www.jusbrasil.com.br/&quot;&gt;Jusbrasil&lt;/a&gt; recently, and it’s being pretty challenging due to the &lt;em&gt;huge&lt;/em&gt; amount of data that we have to deal with, so cleaning this data and making predictions and classifications in an acceptable time demands a nice AI architecture.&lt;/p&gt;

&lt;p&gt;That said I can say that I’m extremely thankful for a few technologies that are helping me go through this challenge &lt;em&gt;(and the pain of cleaning this amount of data)&lt;/em&gt;: Python, Scikit Learn, Pandas, and the whole stack that the Scikit Learn use, such as NumPy, SciPy, matplotlib and few others.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;So, this inspired me to &lt;em&gt;spread the word&lt;/em&gt;, so I’ll be showing here a simple example of Machine Learning using Python, Pandas and Scikit Learn to predict, given a great amount of data/features about wines, if a wine is white or red.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;disclaimer:&lt;/strong&gt; &lt;em&gt;I’m assuming that you already have a small knowledge on the ideas of the machine learning and its mathematical aspects (although not necessary to implement the code that I’ll show here), this is just a simple introduction to scikit learn and its power, so the example is pretty simple and straight forward, if you just want the code, here it is: &lt;a href=&quot;https://gist.github.com/digorithm/ad742f9314f76e732888&quot;&gt;github link&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-pandas&quot;&gt;What’s Pandas?&lt;/h2&gt;

&lt;p&gt;Pandas is an amazing library for data manipulation, it makes the process of dealing with data very easy and straight forward, we can work with CSV, JSON and plenty other formats without struggling to manipulate the data, &lt;a href=&quot;https://signalvnoise.com/posts/3124-give-it-five-minutes&quot;&gt;give it five minutes&lt;/a&gt; and skim their &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/dev/&quot;&gt;docs&lt;/a&gt;, it will definitely worth it!&lt;/p&gt;

&lt;h2 id=&quot;fetching-the-data&quot;&gt;Fetching the data&lt;/h2&gt;

&lt;p&gt;Let’s start fetching the data with Pandas, &lt;em&gt;(you can download the data &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/&quot;&gt;here&lt;/a&gt;)&lt;/em&gt; to do so, just import Pandas and read the CSV file, just like that:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
import pandas as pd
reds = pd.read_csv('winequality-red.csv', sep=';')
whites = pd.read_csv('winequality-white.csv', sep=';')
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;We can see how the data is structured by doing a few commands:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
reds.values[0:6]
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;As output, we have the first 5 rows of the red wine’s data&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
array([[7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4, 5],
       [7.8, 0.88, 0.0, 2.6, 0.098, 25.0, 67.0, 0.9968, 3.2, 0.68, 9.8, 5],
       [7.8, 0.76, 0.04, 2.3, 0.092, 15.0, 54.0, 0.997, 3.26, 0.65, 9.8, 5],
       [11.2, 0.28, 0.56, 1.9, 0.075, 17.0, 60.0, 0.998, 3.16, 0.58, 9.8,6],
       [7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4, 5],
       [7.4, 0.66, 0.0, 1.8, 0.075, 13.0, 40.0, 0.9978, 3.51, 0.56, 9.4, 5]], 
dtype=object)
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Or we just use a function from Pandas that describe very well our data&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
reds.head()
&lt;/code&gt;
&lt;/pre&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;

&amp;lt;class 'pandas.core.frame.dataframe'=&quot;&quot;&amp;gt;Int64Index: 5 entries, 0 to 4
Data columns:
fixed acidity           5  non-null values
volatile acidity        5  non-null values
citric acid             5  non-null values
residual sugar          5  non-null values
chlorides               5  non-null values
free sulfur dioxide     5  non-null values
total sulfur dioxide    5  non-null values
density                 5  non-null values
pH                      5  non-null values
sulphates               5  non-null values
alcohol                 5  non-null values
quality                 5  non-null values
dtypes: float64(11), int64(1), object(1)&amp;lt;/class&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;

&lt;h2 id=&quot;understanding-our-data&quot;&gt;Understanding our data&lt;/h2&gt;

&lt;p&gt;Matplotlib gives you many ways to plot our data into graphs so we can understand what is going on with the data so we can choose the best model/algorithm for the given scenario,&lt;/p&gt;

&lt;p&gt;For instance, let’s take a look at the relation between the red wines and its fixed acidity&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
x = plt.subplots(figsize=(10, 5))
plt.plot(reds.index, reds.get(&quot;fixed acidity&quot;), 'ro')
ax.set_title('Wines vs fixed acidity')
ax.set_xlabel('red wine index')
ax.set_ylabel('Fixed Acidity')
plt.show()
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/mlproblem.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;preparing-the-data-for-classification&quot;&gt;Preparing the Data for classification&lt;/h2&gt;

&lt;p&gt;Now we’re going to add a new feature/variable to our data, which is our target variable, the &lt;em&gt;Y&lt;/em&gt;, that will be telling if the wine from our dataset is white or red&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
reds['kind'] = 'red'
whites['kind'] = 'white'
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;We need to get all of our feature into a vector called X, that will be set into our algorithm, right? And, we need to get all our targets (white or red) and set into a Y variable&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
wines = reds.append(whites, ignore_index=True)
X = wines.ix[:, 0:-1]
y = wines.kind
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Notice that we’re merging both datasets together, the one with the red wines and the one with the white wines, so we can send them together to the algorithm. Now we’re going to binarize the labels ‘white’ and ‘red’, so the mathematical model can use it. It’s pretty simple&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
y = y.apply(lambda val: 0 if val == 'white' else 1)
&lt;/code&gt;
&lt;/pre&gt;

&lt;h2 id=&quot;the-algorithm&quot;&gt;The algorithm&lt;/h2&gt;

&lt;p&gt;Now that we have our data well structured and we do understand it, we can start looking for an algorithm to use. A good algorithm for our scenario is a simple &lt;strong&gt;Logistic Regression&lt;/strong&gt;, that will return a model that we’ll use to make our predictions/classification. The mathematical linear model that will use is the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(w) := \lambda\, R(w) + \frac1n \sum_{i=1}^n L(w;x_i,y_i) \label{eq:regPrimal}\&lt;/script&gt;

&lt;p&gt;In addition with the loss function defined by the logistic loss&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w;x,y) := \log(1+\exp( -y w^T x ))&lt;/script&gt;

&lt;p&gt;The Scikit learn provides an awesome library with an amazingly ease of use, so that we don’t have to implement the whole model from scratch. All we have to do is create a object from the model we want to use, understand how it works &lt;em&gt;(at least understand how to use its interface to do what we want)&lt;/em&gt;. In this case, we will be using the &lt;em&gt;cross validation&lt;/em&gt; object, which is another discussion for another time, but in a few words, it will divide our dataset and test it against all parts of the divided dataset, this way we make sure that we’re validating the quality of the result.&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
clf = LogisticRegression()
scores = cross_val_score(clf, X, y, cv=5)
print scores.mean(), scores.std()
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;And as the result we get:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;python hljs&quot;&gt;
0.981376321334 0.00638795038332
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;And that’s the exactly the precision of the algorithm over the given dataset, &lt;strong&gt;98% of precision&lt;/strong&gt;, which is quite good! Now, for example, we can save this trained classifier and use it for future classifications of incoming data about wines that need to be classified as white or red, to do so, we just call the method &lt;em&gt;clf.predict(X)&lt;/em&gt; where this X will be the new wine’s data. simplicity at its best!&lt;/p&gt;
</description>
        <pubDate>Tue, 31 Mar 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/post/using-python-and-ai-to-predict-wine</link>
        <guid isPermaLink="true">http://localhost:4000/post/using-python-and-ai-to-predict-wine</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>python</category>
        
        <category>sklearn</category>
        
        <category>programming</category>
        
        <category>engineering</category>
        
        <category>tutorial</category>
        
        
        <category>python</category>
        
        <category>case-study</category>
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>Making your machine think, learn and predict - Gradient Descent Algorithm in Java</title>
        <description>&lt;h3 id=&quot;how-can-we-make-a-machine-learn-from-data&quot;&gt;How can we make a machine learn from data?&lt;/h3&gt;

&lt;p&gt;Then, how can we make the machine predicts things based on that learned data? Those are the question answered by one of the most classic Machine Learning Algorithms, the &lt;strong&gt;Gradient Descent Algorithm&lt;/strong&gt;, from a Mathematical-Statistical side it’s called &lt;strong&gt;Univariate Linear Regression&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is one of the tools of the Machine Learning toolbox, and what it tries to do is to model a relationship between a scalar dependent variable Y and a explanatory variable X.&lt;/p&gt;

&lt;h3 id=&quot;in-laymans-term&quot;&gt;In Layman’s term…&lt;/h3&gt;

&lt;p&gt;Let’s suppose you have a few points distributed in a Graph, so you already know that in a point A you have a well defined X and Y, which means, if you input X, your output will be Y, and in a point B you have a well defined X’ and Y’ as well. But, thing is, if a point emerge between A and B, and you only have the X… what will be the Y &lt;em&gt;(the output)&lt;/em&gt;?&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;What this algorithm does is: &lt;strong&gt;It tries to predict this Y value, based on the previous data&lt;/strong&gt;! Amazing, right?&lt;/p&gt;

&lt;p&gt;At the end of the execution, you’ll have a full trend line that you can use to predict values! just like the image below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/linearRegression.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-theory-behind-it&quot;&gt;The Theory Behind It&lt;/h3&gt;

&lt;p&gt;I’ll cover a few theories about this algorithm here, but, it won’t be complete, as this demand a great coverage of mathematical material that if I would write it all here, It would be a &lt;em&gt;long, long&lt;/em&gt;, &lt;strong&gt;very&lt;/strong&gt; &lt;em&gt;long&lt;/em&gt; post. So I’m assuming that you’re already familiar with Calculus &lt;em&gt;(Sums, Partial Derivatives)&lt;/em&gt;, Statistics and Discrete Mathematics.&lt;/p&gt;

&lt;p&gt;So, our goal here is to &lt;strong&gt;fit the best straight line in our initial data&lt;/strong&gt;, right?&lt;/p&gt;

&lt;p&gt;Thus, we need something to represent this straight line, which will be our hypothesis function:&lt;/p&gt;

&lt;div id=&quot;math&quot;&gt; 
$ h_{\Theta}(x) = \Theta_{0} + \Theta_{1}x $ 
&lt;/div&gt;

&lt;p&gt;Where this \( \Theta_{0} \) and \( \Theta_{1} \) are the parameters of the function, and &lt;strong&gt;finding the best parameters for this function is what is going to give us the correct straight line to plot on our data&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Here’s an example of what we’re trying to do, which is, fit the best straight line in the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/Linear-regression.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So what we want to do is to find a \( \Theta_{0} \) and \( \Theta_{1} \) so our Hypothesis outputs can be very close to the real Y output.&lt;/p&gt;

&lt;p&gt;Formally we want:&lt;/p&gt;

&lt;div id=&quot;math&quot;&gt;$ \underset{\Theta_{0}\Theta_{1}}{min}(h_{\Theta}(x) - y)^2 $&lt;/div&gt;

&lt;p&gt;So we want &lt;strong&gt;minimize&lt;/strong&gt; \( \Theta_{0} \) and \( \Theta_{1} \) so the difference between the &lt;strong&gt;Hypothesis&lt;/strong&gt; and the &lt;strong&gt;real output&lt;/strong&gt; is minimal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But we want it for every point in our data&lt;/strong&gt;, which is \(x(i)\) &lt;em&gt;(which is the i-th x of our data)&lt;/em&gt;, so we want the &lt;strong&gt;sum of this average&lt;/strong&gt;, which is, formally:&lt;/p&gt;

&lt;div id=&quot;math&quot;&gt; 
$ \underset{\Theta_{0}\Theta_{1}}{min}\,\frac{1}{2m} \sum_{i=1}^{m} (h_{0}(x^{(i)}) - y^{(i)})^2 $
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;And we’re going to call this function &lt;strong&gt;Cost Function&lt;/strong&gt;, with the following notation:&lt;/p&gt;

&lt;div id=&quot;math&quot;&gt; 
$ J(\Theta_{0}, \Theta_{1}) = \,\frac{1}{2m} \sum_{i=1}^{m} (h_{0}(x^{(i)}) - y^{(i)})^2 $
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;So, our goals is to &lt;strong&gt;minimize this cost function&lt;/strong&gt;:&lt;/p&gt;

&lt;div id=&quot;math&quot;&gt; 
$ \underset{\Theta_{0}\Theta_{1}}{min}\,\, J(\Theta_{0}, \Theta_{1}) $
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This cost function is also called &lt;a href=&quot;http://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;Square Error Function&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;now-the-gradient-descent-algorithm&quot;&gt;Now, the gradient descent algorithm&lt;/h3&gt;

&lt;p&gt;With our cost function built, we need to “keep” finding values for \( \Theta_{0} \) and \( \Theta_{1} \) so we can reach our ideal trend line. So, basically:&lt;/p&gt;

&lt;p&gt;1. We start with some \( \Theta_{0} \) and \( \Theta_{1} \)&lt;/p&gt;

&lt;p&gt;2. keep changing \( \Theta_{0} \) and \( \Theta_{1} \) to reduce our cost function \( J(\Theta_{0}, \Theta_{1}) \), until we find the minimum.&lt;/p&gt;

&lt;p&gt;That’s quite simple and intuitive, right? There’s a lot of intuitive explanation and more visual examples of what this algorithm is doing in the &lt;a href=&quot;https://class.coursera.org/ml-007/&quot;&gt;machine learning course taught by Andrews Ng (From Stanford)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What this algorithm will be doing is: partially derive our cost function for \( \Theta_{0} \) and \( \Theta_{1} \) simultaneously, so we can find the minimum value for them, with every iteration updating our \( \Theta_{0} \) and \( \Theta_{1} \) with their new value!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;- Meh, talk is cheap show me the math!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Formally, the algorithm is:&lt;/p&gt;

&lt;div id=&quot;math&quot;&gt;
$ repeat \, until \, convergence\,\{ \\ \,\,\,\,\,\,\,\,\,\,\,\, \Theta_{j} := \Theta_{j} - \alpha \frac{\partial }{\partial \Theta_{j}} \,\, J(\Theta_{0}, \Theta_{1}) \\ \} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, $
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Make sure that this will run for j = 0 and j = 1.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;But, pay attention, this is the generic version, the \( \Theta_{j} \) represent both \( \Theta_{0} \) and \( \Theta_{1} \).&lt;/p&gt;

&lt;p&gt;What the algorithm is saying is that we’ll be doing this procedure to \( \Theta_{0} \) and \( \Theta_{1} \) at the same time! So, we can put it in this way:&lt;/p&gt;

&lt;div id=&quot;math&quot;&gt;
$ repeat \, until \, convergence\,\{ \\ \Theta_{0} := \Theta_{0} - \alpha \frac{\partial }{\partial \Theta_{0}} \,\, J(\Theta_{0}, \Theta_{1}) \\ \Theta_{1} := \Theta_{1} - \alpha \frac{\partial }{\partial \Theta_{1}} \,\, J(\Theta_{0}, \Theta_{1}) \\ \} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\ $
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Also, we can expand the &lt;strong&gt;Cost Function&lt;/strong&gt; that is being derived, doing this, it will be exactly what I’ll be putting into code soon, so, our &lt;strong&gt;final algorithm&lt;/strong&gt; is:&lt;/p&gt;

&lt;div id=&quot;math&quot;&gt;
$ repeat \, until \, convergence\,\{ \\ \Theta_{0} := \Theta_{0} - \alpha \frac{\partial }{\partial \Theta_{0}} \,\, \Big (\frac{1}{2m} \sum_{i=1}^{m} (h_{\Theta}(x^{(i)}) - y^{(i)})^2 \Big) \\ \Theta_{1} := \Theta_{1} - \alpha \frac{\partial }{\partial \Theta_{1}} \,\, \Big (\frac{1}{2m} \sum_{i=1}^{m} (h_{\Theta}(x^{(i)}) - y^{(i)})^2 \Big) \\ \}
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\ $
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Now it’s time to code all of it!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First things first, we’ll be using &lt;a href=&quot;http://code.google.com/p/jmathplot/&quot;&gt;Google’s JMathPlot&lt;/a&gt; to plot graphs using Java and Swing to use its JFrame, we shall start with our class to represent our &lt;strong&gt;Initial Data&lt;/strong&gt;, which will be the &lt;strong&gt;Training Set&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;java hljs&quot;&gt;
    import javax.swing.JFrame;
    import org.math.plot.*;

    public class InitialData{
            public static double[] x = {2, 4, 6, 8};
            public static double[] y = {2, 5, 5, 8};

            public void plotData(){
                    Plot2DPanel plot = new Plot2DPanel();
                    plot.addScatterPlot(&quot;X-Y&quot;, this.x, this.y);
                    JFrame frame = new JFrame(&quot;Original X-Y Data&quot;);
                    frame.setContentPane(plot);
                    frame.setSize(600, 600);
                    frame.setVisible(true);
            }
    }
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
To try out the data plotting, create a main class and call &lt;em&gt;plotData()&lt;/em&gt; from &lt;strong&gt;InitialData.java&lt;/strong&gt;, so we can have this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/example-plot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we’re going to take our first steps on writing the &lt;strong&gt;GradientDescent.java&lt;/strong&gt;, we must be very careful here. Let’s start with the main settings and parameters of it:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;java hljs&quot;&gt;
    import javax.swing.JFrame;
    import org.math.plot.*;

    public class GradientDescent{
            private double theta0;
            private double theta1;

            private int trendline;

            // Algorithm settings
        double alpha = 0.01;  // learning rate
        double tol = 1e-11;   // tolerance to determine convergence
        int maxiter = 9000;   // maximum number of iterations in case convergence is not reached
        int dispiter = 100;   // interval for displaying results during iterations
        int iters = 0;

        //track of results
        double[] theta0plot = new double[maxiter+1];
        double[] theta1plot = new double[maxiter+1];
        double[] tplot = new double[maxiter+1];

        InitialData initial_data;

        Plot2DPanel plot;

        public GradientDescent(InitialData id){
                    //initial guesses
            this.theta0 = 0;
            this.theta1 = 0;
            this.initial_data = id;

            plot = new Plot2DPanel();
            plot.addScatterPlot(&quot;X-Y&quot;, initial_data.x, initial_data.y);
            JFrame frame = new JFrame(&quot;Final X-Y Data&quot;);
            frame.setContentPane(plot);
            frame.setSize(600, 600);
            frame.setVisible(true);
        }
    [...]
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Now let me explain a few details of this part:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alpha&lt;/strong&gt; is the &lt;strong&gt;Learning Rate&lt;/strong&gt;, it’s a &lt;em&gt;dangerous variable&lt;/em&gt;, it’s used to set the size of the step that the algorithm will take while trying to find the \( \Theta_{0} \) and \( \Theta_{1} \), that’s the learning rate of the algorithm. If Alpha is &lt;strong&gt;too low&lt;/strong&gt;, the algorithm can be very slow, although very precise, if Alpha is &lt;strong&gt;higher&lt;/strong&gt;, it will be taking &lt;strong&gt;larger steps&lt;/strong&gt;, which can be &lt;strong&gt;faster&lt;/strong&gt;, or &lt;strong&gt;dangerous&lt;/strong&gt;, causing the algorithm to &lt;strong&gt;DIVERGE&lt;/strong&gt;, which, &lt;em&gt;trust me&lt;/em&gt;, you don’t want this! &lt;em&gt;(Andrew Ng explain this part very well in its course)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The variable &lt;strong&gt;TrendLine&lt;/strong&gt; is what we’ll use to plot the straight line which is our main goal.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;tol&lt;/strong&gt; variable is our safe move in case of a dangerous convergence, which means, in case of convergence, it will stop the execution.&lt;/p&gt;

&lt;p&gt;The other variables and objects in this part are very intuitive to understand, it’s auto explainable! &lt;em&gt;(forgive if i’m wrong, just say something and I’ll put more detail on that)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;About the Constructor, we’re saying that our initial guesses for \( \Theta_{0} \) and \( \Theta_{1} \) is 0. The rest is just data plotting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;next: our Hypothesis Function&lt;/strong&gt; &lt;em&gt;(that will be doing exactly as the model that I did show above)&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;java hljs&quot;&gt;
    public double hypothesisFunction(double x){
            return this.theta1*x + theta0;
        }
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Now, our two function to derive our \( \Theta \), again, it will be doing exactly the same as the mathematical model, there’s no magic!&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;java hljs&quot;&gt;
    public double deriveTheta1(){
            double sum = 0;

            for (int j=0; j&amp;lt;initial_data.x.length; j++){
                    sum += (initial_data.y[j] - hypothesisFunction(initial_data.x[j])) * initial_data.x[j];
            }
            return -2 * sum / initial_data.x.length;
        }

        public double deriveTheta0(){
            double sum = 0;

            for (int j=0; j&amp;lt;initial_data.x.length; j++) {
                    sum += initial_data.y[j] - hypothesisFunction(initial_data.x[j]);
            }
            return -2 * sum / initial_data.x.length;

        }
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
Now, the &lt;strong&gt;Gradient Descent Algorithm&lt;/strong&gt; &lt;em&gt;per se&lt;/em&gt;, making use of the functions above:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;java hljs&quot;&gt;
    public void execute(){
            do {

                    this.theta1 -= alpha * deriveTheta1();
                    this.theta0 -= alpha * deriveTheta0();

                    //used for plotting
                    tplot[iters] = iters;
                    theta0plot[iters] = theta0;
                    theta1plot[iters] = theta1;
                    iters++;

                    if (iters % dispiter == 0){
                            addTrendLine(plot, true);

                    }

                    if (iters &amp;gt; maxiter) break;
            } while (Math.abs(theta1) &amp;gt; tol || Math.abs(theta0) &amp;gt; tol);
            plot.addScatterPlot(&quot;X-Y&quot;, initial_data.x, initial_data.y);
            System.out.println(&quot;theta0 = &quot; + this.theta0 + &quot; and theta1 = &quot; + this.theta1);
        }
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Note that it does &lt;em&gt;almost&lt;/em&gt; exactly the same as the mathematical model of the Gradient Descent demonstrated above, the difference is only a few details, such as the &lt;em&gt;if&lt;/em&gt; and &lt;em&gt;while&lt;/em&gt; to verify convergence or divergence &lt;em&gt;(which is, if it reached the iteration’s limit)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Next we have the &lt;strong&gt;addTrendLine&lt;/strong&gt; function, used to keep plotting our straight line as it will become more updated.&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;java hljs&quot;&gt;
    public void addTrendLine(Plot2DPanel plot, boolean removePrev){
            if (removePrev){
                    plot.removePlot(trendline);
            }
            double[] yEnd = new double[initial_data.x.length];
            for (int i=0; i&amp;lt;initial_data.x.length; i++)
                    yEnd[i] = hypothesisFunction(initial_data.x[i]);
            trendline = plot.addLinePlot(&quot;final&quot;, initial_data.x, yEnd);
        }
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;And now we have an extra, it’s a function to store and plot the convergence history of both \( \Theta_{0} \) and \( \Theta_{1} \), so we can see how it happened.&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;java hljs&quot;&gt;
    public void printConvergence(){

          double[] theta0plot2 = new double[iters];
          double[] theta1plot2 = new double[iters];
          double[] tplot2 = new double[iters];
          System.arraycopy(theta0plot, 0, theta0plot2, 0, iters);
          System.arraycopy(theta1plot, 0, theta1plot2, 0, iters);
          System.arraycopy(tplot, 0, tplot2, 0, iters);

          // Plot the convergence of data
          Plot2DPanel convPlot = new Plot2DPanel();

          // add a line plot to the PlotPanel
          convPlot.addLinePlot(&quot;theta0&quot;, tplot2, theta0plot2);
          convPlot.addLinePlot(&quot;theta1&quot;, tplot2, theta1plot2);

          // put the PlotPanel in a JFrame, as a JPanel
          JFrame frame2 = new JFrame(&quot;Convergence of parameters over time&quot;);
          frame2.setContentPane(convPlot);
          frame2.setSize(600, 600);
          frame2.setVisible(true);
        }
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Finally, we have our &lt;strong&gt;Test Class&lt;/strong&gt;, that will execute everything:&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;java hljs&quot;&gt;
    import javax.swing.JFrame;

    import org.math.plot.*;

    public class TestGradDescent {
            public static void main(String[] args ){
                    InitialData id = new InitialData();
                    id.plotData();
                    GradientDescent gd = new GradientDescent(id);
                    gd.execute();
                    gd.printConvergence();

            }
    }
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Now, executing the code, the output will be, at first, the initial data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/example-plot-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Executing the algorithm, it learns and generate its prediction based on its initial data:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/example-plot-line-e1420763780646.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;magical-right&quot;&gt;Magical, right?&lt;/h3&gt;

&lt;p&gt;And then, we can see the &lt;strong&gt;convergence&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2015/06/example-convergence.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And you can see in the terminal the final values of \( \Theta_{0} \) and \( \Theta_{1} \) that minimized the Cost Function.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If it wasn’t Science, probably would be black magic. heh.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;a-few-considerations&quot;&gt;A few considerations&lt;/h3&gt;

&lt;p&gt;You can download the complete code &lt;a href=&quot;https://github.com/digorithm/ArtificialIntelligenceAlgorithms&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have any questions/suggestion, drop me an email so we can talk!&lt;/p&gt;

&lt;p&gt;If you need further details of the mathematical model, I ultra advice to watch Andrew’s Ng Videos at Stanford@Coursera. &lt;strong&gt;His skills to teach it is something unbelievable awesome&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks for reading!&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Jan 2015 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/articles/2015-01/Making-Your-Machine-Think-Learn-And-Predict-Gradient-Descent-Algorithm-in-Java</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2015-01/Making-Your-Machine-Think-Learn-And-Predict-Gradient-Descent-Algorithm-in-Java</guid>
        
        <category>machine learning</category>
        
        <category>artificial intelligence</category>
        
        <category>java</category>
        
        <category>programming</category>
        
        <category>engineering</category>
        
        <category>tutorial</category>
        
        
        <category>machine-learning</category>
        
        <category>java</category>
        
        <category>algorithm</category>
        
      </item>
    
  </channel>
</rss>
