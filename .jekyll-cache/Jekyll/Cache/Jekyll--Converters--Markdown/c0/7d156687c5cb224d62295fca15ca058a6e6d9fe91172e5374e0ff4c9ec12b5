I"òE<p>Artificial Neurons is one of the most beautiful ways to simulate a biological behavior through computation, despite the fact that it‚Äôs not very close to the level of details of a real neuron. But it captured the core of what a neuron is doing.</p>

<p>And I find that trying to understand this mathematical method by first understanding the concept through a metaphor <em>(which, in this case, I think that the metaphor is our mathematical side, and not the biological one, the biological is just, you know, the real thing)</em> is really valuable to gain an intuition on this topic.</p>

<!--more-->

<p>If you were about to make a decision, given many variables, how would you make this decision? It may not look obvious, due the fact that, on normal occasions, we‚Äôre not thinking about our thinking process, but the truth is, we are collecting those variables that affect the future decision, and we‚Äôre giving weights to them. Weights? how? Simple, some variables are more crucial than other while making decision, isn‚Äôt it? the word <em>‚Äòcrucial‚Äô</em>, in this case, means a huge weight on this variable, and it will strongly affect the final decision.</p>

<p>Suppose a scenario: we‚Äôre deciding whether we should go to beach or not. Let‚Äôs put a few binary variables on table:</p>

<ol>
  <li>Are we in the mood to go to the beach?</li>
  <li>Is it raining?</li>
  <li>Our other friends are going?</li>
  <li>Do we have money?</li>
</ol>

<p>So, depending on those variables, it will be more likely that we will go to the beach‚Ä¶ or not. Let‚Äôs think about the variable <em>‚Äòis it raining?‚Äô</em>, if it‚Äôs raining, we can say that it‚Äôs a deal breaker, so we can conclude that this variable has more weights than the other. The fact is that we already trained those weights inside our brain, long time ago.</p>

<p>That‚Äôs what an Artificial Neuron try to do, the AN try to learn the weights of things so it can make decisions. So, drawing back to the mathematical and computational aspect of it, the perceptron is a single unit that will receive external inputs<em>(variables)</em>, it will have a weight for each input, it will do some computation, send this result to a decision function, and, finally, output the final decision.</p>

<p>Those inputs are the variables we‚Äôre talking before, and like our process to make a decision, the Perceptron will weight each input to make a decision.</p>

<p>Now, how the Perceptron knows how the weights should be for each input? Well‚Ä¶ it doesn‚Äôt. At least, not from the beginning of the learning process, just like you and me, when trying to learn something new.</p>

<p>That‚Äôs why this case is a case of supervised learning, what the Perceptron will do is: start with a random small weights, take one previously trained example <em>(e.g: (1) yes to mood to beach, (2) not raining, (3) yes to friends going to beach and (4) yes to money, the output: 1 - yes, we shall go to the beach)</em> do some computation taking in consideration the random weights we defined before, send it to a decision function, output something <em>(1 - yes, 0 - no)</em> and check if this output is equal to the expected <em>(we‚Äôre using a trained example, remember?)</em>, of course that at the first try, it won‚Äôt be equal. So the Perceptron calculate the error rate and update its weights‚Ä¶ after this, guess what? it repeat the process of trying to predict, but now, with the updated weights, after a few tries, the error rate tends to reduce and it starts predicting correctly. So it learned to predict a behavior, by training with previously trained examples.</p>

<p>So, a nice learning behavior would be something like this:</p>

<div class="imgcenter">
	<img src="/content/images/images/perceptron_0.1.gif" />
</div>

<p>Which means, at every iteration <em>(we call it epochs)</em>, the error rate tend to decrease and, as consequence, the Perceptron starts to predict correctly!</p>

<p>As you may have noted, I omitted a few details of the process so you could see the whole picture of the process: we take an example, we practice on it, we see what we missed, we try again, we learn. That‚Äôs the core process.</p>

<p>Now, to the details:</p>

<h3 id="the-computation">The computation</h3>

<p>The first step that the Perceptron does it‚Äôs a computation that I referred as <em>‚Äúsome computation‚Äù</em>, this is a simple computation, it‚Äôs just a simple Linear Combination of the feature vector <em>(the variables)</em> and the weight vector, which is just the sum of the products between feature/input and its respective weight:</p>

<script type="math/tex; mode=display">x_{1}w_{1}+x_{2}w_{2}+x_{3}w_{3}+ \cdots +x_{n}w_{n}</script>

<p>¬†</p>

<h3 id="the-decision-function">The Decision Function</h3>
<p>So, after the Linear Combination, we send the result of it to a decision function, which will, somehow, based on some threshold <em>(or without a threshold as we‚Äôre going to see)</em>, give us the output, that will be checked with the correct output, in case if it‚Äôs correct, fine, keep the weights like this, otherwise, it will calculate the error rate, adjust the weights and restart the process.</p>

<p>¬†</p>

<h4 id="binary-output-with-threshold">Binary output with threshold</h4>

<p>This technique is very simple, after the linear combination, if the output is bigger than some threshold, it outputs 1 and we say that the neuron was activated, otherwise, output 0.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
    X=
    \begin{cases}
      1, & \text{if}\ Linear\,Combination>0 \\
      0, & \text{otherwise}
    \end{cases}
  \end{equation} %]]></script>

<p>¬†</p>

<h4 id="using-sigmoid-function">Using sigmoid function</h4>

<p>Now, that a interesting one, it won‚Äôt use a threshold anymore, we‚Äôll send the output of the Linear Combination to a sigmoid function</p>

<script type="math/tex; mode=display">S(\vec{w}\vec{x}) = \dfrac{1}{1+e^{-\vec{w}\vec{x}}}</script>

<p>which will, then, smooth the output, making it in the range of 0 and 1. Which is the most used in many Machine Learning Algorithms.</p>

<div class="imgcenter">
	<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png" />
</div>

<h3 id="adjusting-the-weights-and-learning">Adjusting the weights and learning</h3>

<p>After the perceptron fails to predict correctly, it‚Äôs time to adjust the weight, using some rule, the classic perceptron has 2 rules:</p>

<p>¬†</p>

<h4 id="perceptron-learning-rule">Perceptron Learning Rule</h4>
<p>This is very simple, when the perceptron has the incorrect output, it update its weights following this:</p>

<script type="math/tex; mode=display">w_{i} = w_{i} + \eta (t_{i} - o_{i})x_{i}</script>

<p>Which is simply multiplying a learning rate <em>(usually 0.001)</em> by the difference between the correct output and the wrong output and then multiplying it by its original input, after this, we add this value to the previous weight and then we have the value of the new weight. 
It‚Äôs fair simple, isn‚Äôt it? The problem arises when the data isn‚Äôt linearly separable, like this:</p>

<div class="imgcenter">
	<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/DBSCAN-density-data.svg/2000px-DBSCAN-density-data.svg.png" width="400" />
</div>

<p>¬†</p>

<p>With this scenario, the result just won‚Äôt converge. So we adopt another rule!</p>

<p>¬†</p>

<h4 id="delta-rule">Delta Rule</h4>

<p>Now we must find a way to have a non-linear output, and what‚Äôs the best for this if not the classic Gradient Descent algorithm? GD will simply search through hypothesis spaces and try to minimize the cost function, I wrote about this <a href="http://rodrigoaraujo.me/general/setup/demo/2015/01/09/Making-Your-Machine-Think-Learn-And-Predict--Gradient-Descent-Algorithm-in-Java.html">here</a>.</p>

<div class="imgcenter">
	<img src="http://www.yaldex.com/game-development/FILES/17fig06.gif" />
</div>

<p>So, it‚Äôs just a technique to solve our previous problem, but still, the weights will be updated <em>(now, using the GD)</em> and then the process start over!</p>

<p>¬†</p>

<h3 id="code-of-a-perceptron">Code of a Perceptron</h3>

<p>Now, here‚Äôs a code of a Perceptron that will behave like a simple Boolean function. I didn‚Äôt use the Delta Dule (Gradient Descent) to optimize the weights, as this problem (binary Boolean functions) is linearly separable.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="kn">import</span> <span class="nn">math</span>
  <span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="n">array</span>
  <span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">choice</span>
  <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

  <span class="k">class</span> <span class="nc">Perceptron</span><span class="p">():</span>
      
      <span class="s">"""
      this is the thereshold to activate the unit 
      """</span>
      <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

      <span class="k">def</span> <span class="nf">linear_combination</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
          <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)):</span>
              <span class="n">total</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
          <span class="k">return</span> <span class="n">total</span>
      
      <span class="k">def</span> <span class="nf">unit_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
          <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_combination</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

      <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
          
          <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.5</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>

          <span class="c1"># initializing weights
</span>          <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>
          
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
              <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">choice</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span>
              <span class="c1"># &gt; calculate output with current weight
</span>              <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
              <span class="c1"># &gt; calculate error rate (t - o)
</span>              <span class="n">error_rate</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">output</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error_rate</span><span class="p">)</span>
              <span class="k">print</span> <span class="s">'the X: '</span><span class="p">,</span> <span class="n">X</span>
              <span class="k">print</span> <span class="s">'output: '</span><span class="p">,</span><span class="n">output</span>
              <span class="k">print</span> <span class="s">'correct target: '</span><span class="p">,</span> <span class="n">y</span>
              <span class="c1"># &gt; apply learning rule which will update weights
</span>              <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">error_rate</span> <span class="o">*</span> <span class="n">X</span>

      <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
          <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">y</span>
                  

          <span class="n">training_data</span> <span class="o">=</span> <span class="p">[</span>
                  <span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">1</span><span class="p">),</span>
                  
          <span class="p">]</span>

  <span class="n">p</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">()</span>

  <span class="n">p</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
  <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">errors</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'errors'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epochs'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<h3 id="conclusion-and-next-steps">Conclusion and next steps</h3>

<p>With this we conclude what a simple single Perceptron is doing! Of course there are many improvements on it and we can connect many of these Artificial Neurons in many layers‚Ä¶ that‚Äôs what is called Artificial Neural Network, I‚Äôll be writing about it soon!</p>

:ET