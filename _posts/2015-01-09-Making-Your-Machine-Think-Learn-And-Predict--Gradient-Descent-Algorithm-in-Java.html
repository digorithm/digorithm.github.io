---                                                                                         
layout: blog_post                                                                           
title: Making your machine think, learn and predict - Gradient Descent Algorithm in Java
excerpt: The first thought I had when I started to study Design Patterns was “damn, is all this really necessary?" I mean, so many patterns, so many details to do things that look simple to do without any further detailed thoughts. And I started to question myself if this was really a productive thing.
comments: true
--- 

<html>

<h2>How can we make a machine learn from data?</h2>

<p>Then, how can we make the machine predicts things based on that learned data? Those are the question answered by one of the most classic Machine Learning Algorithms, the <strong>Gradient Descent Algorithm</strong>, from a Mathematical-Statistical side it’s called <strong>Univariate Linear Regression</strong>.</p>


<p>This is one of the tools of the Machine Learning toolbox, and what it tries to do is to model a relationship between a scalar dependent variable Y and a explanatory variable X.</p>
&nbsp;

<h2>in Layman’s term…</h2>

<p>Let’s suppose you have a few points distributed in a Graph, so you already know that in a point A you have a well defined X and Y, which means, if you input X, your output will be Y, and in a point B you have a well defined X’ and Y’ as well. But, thing is, if a point emerge between A and B, and you only have the X…  what will be the Y <em>(the output)</em>?</p>

<p>What this algorithm does is: <strong>It tries to predict this Y value, based on the previous data</strong>! Amazing, right?</p>

<p>At the end of the execution, you’ll have a full trend line that you can use to predict values! just like the image below.</p>
<div align=center>
<img src="/images/linearRegression.png" width=500 height=400/>
</div>

<h2>The Theory Behind It</h2>

<p>I’ll cover a few theories about this algorithm here, but, it won’t be complete, as this demand a great coverage of mathematical material that if I would write it all here, It would be a <em>long, long</em>, <strong>very</strong> <em>long</em> post. So I’m assuming that you’re already familiar with Calculus <em>(Sums, Partial Derivatives)</em>, Statistics and Discrete Mathematics.</p>

<p>So, our goal here is to <strong>fit the best straight line in our initial data</strong>, right?</p>

<p>Thus, we need something to represent this straight line, which will be our hypothesis function:</p>
<span style="font-size:169%;">
$$

h_{\Theta}(X) = \Theta_{0} + \Theta_{1}x

$$
</span>

<p>Where this \( \Theta_{0} \) and \( \Theta_{1} \) are the parameters of the function, and <strong>finding the best parameters for this function is what is going to give us the correct straight line to plot on our data</strong>.</p>

<p>Here’s an example of what we're trying to do, which is, fit the best straight line in the data:</p>

<div align=center>
        <img src="/images/Linear-regression.svg"/>
</div>

<p>So what we want to do is to find a \( \Theta_{0} \) and \( \Theta_{1} \) so our Hypothesis outputs can be very close to the real Y output.</p>
<p>Formally we want:<p>
<span style="font-size:160%;">
$$
\underset{\Theta_{0}\Theta_{1}}{min}(h_{\Theta}(x) - y)^2
$$
</span>

<p>So we want <strong>minimize</strong> \( \Theta_{0} \) and \( \Theta_{1} \) so the difference between the <strong>Hypothesis</strong> and the <strong>real output</strong> is minimal.</p>

<p><strong>But we want it for every point in our data</strong>, which is \(x(i)\) <em>(which is the i-th x of our data)</em>, so we want the <strong>sum of this average</strong>, which is, formally:</p>

<span style="font-size:199%;">
$$
\underset{\Theta_{0}\Theta_{1}}{min}\,\frac{1}{2m} \sum_{i=1}^{m} (h_{0}(x^{(i)}) - y^{(i)})^2
$$
</span>

<p>And we’re going to call this function <strong>Cost Function</strong>, with the following notation:</p>
<span style="font-size:160%;">
$$
J(\Theta_{0}, \Theta_{1}) = \,\frac{1}{2m} \sum_{i=1}^{m} (h_{0}(x^{(i)}) - y^{(i)})^2
$$
</span>

<p>So, our goals is to <strong>minimize this cost function</strong>:</p>

<span style="font-size:160%;">
$$
\underset{\Theta_{0}\Theta_{1}}{min}\,\, J(\Theta_{0}, \Theta_{1})
$$
</span>


<p>This cost function is also called <a href="http://en.wikipedia.org/wiki/Mean_squared_error">Square Error Function</a>.</p>

<p><strong>Now, the gradient descent algorithm</strong></p>

<p>With our cost function built, we need to “keep” finding values for \( \Theta_{0} \) and \( \Theta_{1} \) so we can reach our ideal trend line. So, basically:</p>

<p>1. We start with some \( \Theta_{0} \) and \( \Theta_{1} \)</p>
<p>2. keep changing \( \Theta_{0} \) and \( \Theta_{1} \) to reduce our cost function \( J(\Theta_{0}, \Theta_{1}) \), until we find the minimum.</p>

<p>That’s quite simple and intuitive, right? There’s a lot of intuitive explanation and more visual examples of what this algorithm is doing in the <a href="https://class.coursera.org/ml-007/">machine learning course taught by Andrews Ng (From Stanford)</a>.</p>


<p>What this algorithm will be doing is: partially derive our cost function for \( \Theta_{0} \) and \( \Theta_{1} \) simultaneously, so we can find the minimum value for them, with every iteration updating our \( \Theta_{0} \) and \( \Theta_{1} \) with their new value!</p>

<p><em>- Meh, talk is cheap show me the math!</em></p>

<p>Formally, the algorithm is:</p>

<span style="font-size:160%;">
$$
repeat \, until  \, convergence\,\{
\\
\,\,\,\,\,\,\,\,\,\,\,\, \Theta_{j} := \Theta_{j} - \alpha \frac{\partial }{\partial \Theta_{j}} \,\, J(\Theta_{0}, \Theta_{1})

\\ \} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,

$$
</span>

<p><em><strong>Make sure that this will run for j = 0 and j = 1.</em></strong></p>

<p>But, pay attention, this is the generic version, the \( \Theta_{j} \) represent both \( \Theta_{0} \) and \( \Theta_{1} \).</p>

<p>What the algorithm is saying is that we’ll be doing this procedure to \( \Theta_{0} \) and \( \Theta_{1} \) at the same time!</p>

<p>So, we can put it in this way:</p>
<span style="font-size:160%;">
$$
repeat \, until  \, convergence\,\{
\\
\Theta_{0} := \Theta_{0} - \alpha \frac{\partial }{\partial \Theta_{0}} \,\, J(\Theta_{0}, \Theta_{1})
\\
\Theta_{1} := \Theta_{1} - \alpha \frac{\partial }{\partial \Theta_{1}} \,\, J(\Theta_{0}, \Theta_{1})

\\ \} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\


$$
</span>

<p>Also, we can expand the <strong>Cost Function</strong> that is being derived, doing this, it will be exactly what I’ll be putting into code soon, so, our <strong>final algorithm</strong> is:</p>
<span style="font-size:160%;">
$$
repeat \, until  \, convergence\,\{
\\
\Theta_{0} := \Theta_{0} - \alpha \frac{\partial }{\partial \Theta_{0}} \,\, \Big (\frac{1}{2m} \sum_{i=1}^{m} (h_{\Theta}(x^{(i)}) - y^{(i)})^2 \Big) 
\\
\Theta_{1} := \Theta_{1} - \alpha \frac{\partial }{\partial \Theta_{1}} \,\, \Big (\frac{1}{2m} \sum_{i=1}^{m} (h_{\Theta}(x^{(i)}) - y^{(i)})^2 \Big)

\\ \} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\

$$
</span>


<p><strong>Now it’s time to code all of it!</strong></p>

<p>First things first, we’ll be using <a href="http://code.google.com/p/jmathplot/">Google’s JMathPlot</a> to plot graphs using Java and Swing to use its JFrame, we shall start with our class to represent our <strong>Initial Data</strong>, which will be the <strong>Training Set</strong>.</p>


<pre>
<code class="hljs java">

import javax.swing.JFrame;
import org.math.plot.*;

public class InitialData{
        public static double[] x = {2, 4, 6, 8};
        public static double[] y = {2, 5, 5, 8};


        public void plotData(){
                Plot2DPanel plot = new Plot2DPanel();
                plot.addScatterPlot("X-Y", this.x, this.y);
                JFrame frame = new JFrame("Original X-Y Data");
                frame.setContentPane(plot);
                frame.setSize(600, 600);
                frame.setVisible(true);
        }
}

</code>
</pre>

<p>To try out the data plotting, create a main class and call <em>plotData()</em> from <strong>InitialData.java</strong>, so we can have this:</p>
<div align=center>
<img src="/images/example-plot.png" width=400 height=400/>
</div>

<p>Now we’re going to take our first steps on writing the <strong>GradientDescent.java</strong>, we must be very careful here.</p>

<p>Let’s start with the main settings and parameters of it:</p>

<pre>
<code class="hljs java">

import javax.swing.JFrame;
import org.math.plot.*;

public class GradientDescent{
        private double theta0;
        private double theta1;

        private int trendline;

        // Algorithm settings
    double alpha = 0.01;  // learning rate
    double tol = 1e-11;   // tolerance to determine convergence
    int maxiter = 9000;   // maximum number of iterations in case convergence is not reached
    int dispiter = 100;   // interval for displaying results during iterations
    int iters = 0;

    //track of results
    double[] theta0plot = new double[maxiter+1];
    double[] theta1plot = new double[maxiter+1];
    double[] tplot = new double[maxiter+1];

    InitialData initial_data;

    Plot2DPanel plot;

    public GradientDescent(InitialData id){
                //initial guesses
        this.theta0 = 0;
        this.theta1 = 0;
        this.initial_data = id;

        plot = new Plot2DPanel();
        plot.addScatterPlot("X-Y", initial_data.x, initial_data.y);
        JFrame frame = new JFrame("Final X-Y Data");
        frame.setContentPane(plot);
        frame.setSize(600, 600);
        frame.setVisible(true);
    }
[...]

</code>
</pre>



<p>Now let me explain a few details of this part:</p>

<p><strong>Alpha</strong> is the <strong>Learning Rate</strong>, it’s a <em>dangerous variable</em>, it’s used to set the size of the step that the algorithm will take while trying to find the \( \Theta_{0} \) and \( \Theta_{1} \), that’s the learning rate of the algorithm. If Alpha is <strong>too low</strong>, the algorithm can be very slow, although very precise, if Alpha is <strong>higher</strong>, it will be taking <strong>larger steps</strong>, which can be <strong>faster</strong>, or <strong><style="font-color:red;">dangerous</style></strong>, causing the algorithm to <strong>DIVERGE</strong>, which, <em>trust me</em>, you don’t want this! <em>(Andrew Ng explain this part very well in its course)</em></p>


<p>The variable <strong>TrendLine</strong> is what we’ll use to plot the straight line which is our main goal.</p>

<p>The <strong>tol</strong> variable is our safe move in case of a dangerous convergence, which means, in case of convergence, it will stop the execution.</p>

<p>The other variables and objects in this part are very intuitive to understand, it’s auto explainable! <em>(forgive if i’m wrong, just say something and I’ll put more detail on that)</em>.</p>

<p>About the Constructor, we’re saying that our initial guesses for \( \Theta_{0} \) and \( \Theta_{1} \) is 0. The rest is just data plotting.</p>

<p><strong>next: our Hypothesis Function</strong> <em>(that will be doing exactly as the model that I did show above)</em></p>


<pre>
<code class="hljs java">
public double hypothesisFunction(double x){
        return this.theta1*x + theta0;
    }
</code>
</pre>

<p>Now, our two function to derive our \( \Theta \), again, it will be doing exactly the same as the mathematical model, there’s no magic!</p>

<pre>
<code class="hljs java">
public double deriveTheta1(){
        double sum = 0;

        for (int j=0; j&lt;initial_data.x.length; j++){
                sum += (initial_data.y[j] - hypothesisFunction(initial_data.x[j])) * initial_data.x[j];
        }
        return -2 * sum / initial_data.x.length;
    }

    public double deriveTheta0(){
        double sum = 0;

        for (int j=0; j&lt;initial_data.x.length; j++) {
                sum += initial_data.y[j] - hypothesisFunction(initial_data.x[j]);
        }
        return -2 * sum / initial_data.x.length;

    }
</code>
</pre>

<p>Now, the <strong>Gradient Descent Algorithm</strong> <em>per se</em>, making use of the functions above:</p>

<pre>
<code class="hljs java">

public void execute(){
        do {

                this.theta1 -= alpha * deriveTheta1();
                this.theta0 -= alpha * deriveTheta0();

                //used for plotting
                tplot[iters] = iters;
                theta0plot[iters] = theta0;
                theta1plot[iters] = theta1;
                iters++;

                if (iters % dispiter == 0){
                        addTrendLine(plot, true);

                }

                if (iters &gt; maxiter) break;
        } while (Math.abs(theta1) &gt; tol || Math.abs(theta0) &gt; tol);
        plot.addScatterPlot("X-Y", initial_data.x, initial_data.y);
        System.out.println("theta0 = " + this.theta0 + " and theta1 = " + this.theta1);
    }

</code>
</pre>

<p>Note that it does <em>almost</em> exactly the same as the mathematical model of the Gradient Descent demonstrated above, the difference is only a few details, such as the <em>if</em> and <em>while</em> to verify convergence or divergence <em>(which is, if it reached the iteration’s limit)</em></p>

<p>Next we have the <strong>addTrendLine</strong> function, used to keep plotting our straight line as it will become more updated.</p>

<pre>
<code class="hljs java">
public void addTrendLine(Plot2DPanel plot, boolean removePrev){
        if (removePrev){
                plot.removePlot(trendline);
        }
        double[] yEnd = new double[initial_data.x.length];
        for (int i=0; i&lt;initial_data.x.length; i++)
                yEnd[i] = hypothesisFunction(initial_data.x[i]);
        trendline = plot.addLinePlot("final", initial_data.x, yEnd);
    }
</code>
</pre>

<p>And now we have an extra, it’s a function to store and plot the convergence history of both \( \Theta_{0} \) and \( \Theta_{1} \), so we can see how it happened.</p>
<pre>
<code class="hljs java">
public void printConvergence(){

      double[] theta0plot2 = new double[iters];
      double[] theta1plot2 = new double[iters];
      double[] tplot2 = new double[iters];
      System.arraycopy(theta0plot, 0, theta0plot2, 0, iters);
      System.arraycopy(theta1plot, 0, theta1plot2, 0, iters);
      System.arraycopy(tplot, 0, tplot2, 0, iters);
                
      // Plot the convergence of data
      Plot2DPanel convPlot = new Plot2DPanel();
 
      // add a line plot to the PlotPanel
      convPlot.addLinePlot("theta0", tplot2, theta0plot2);
      convPlot.addLinePlot("theta1", tplot2, theta1plot2);
 
      // put the PlotPanel in a JFrame, as a JPanel
      JFrame frame2 = new JFrame("Convergence of parameters over time");
      frame2.setContentPane(convPlot);
      frame2.setSize(600, 600);
      frame2.setVisible(true);
    }

</code>
</pre>

<p>Finally, we have our <strong>Test Class</strong>, that will execute everything:</p>

<pre>
<code class="hljs java">

import javax.swing.JFrame;

import org.math.plot.*;

public class TestGradDescent {
        public static void main(String[] args ){
                InitialData id = new InitialData();
                id.plotData();
                GradientDescent gd = new GradientDescent(id);
                gd.execute();
                gd.printConvergence();

        }
}

</code>
</pre>

<p>Now, executing the code, the output will be, at first, the initial data:</p>
<div align=center>
<img src="/images/example-plot-2.png"/>
</div>
<p><strong>Executing the algorithm, it learns and generate its prediction based on its initial data:</strong></p>

<h2>Magical, right?</h2>

<p>And then, we can see the <strong>convergence</strong>:</p>

<div align=center>
<img src="/images/example-convergence.png"/>
</div>

<p>And you can see in the terminal the final values of \( \Theta_{0} \) and \( \Theta_{1} \) that minimized the Cost Function.</p>

<p><em>If it wasn’t science, probably would be black magic. heh.</em></p>

<h2>A few considerations</h2>

<p>You can download the complete code <a href="https://github.com/digorithm/ArtificialIntelligenceAlgorithms">here</a>.</p>

<p>If you have any questions/suggestion, drop me an email so we can talk!</p>

<p>If you need further details of the mathematical model, I ultra advice to watch Andrew’s Ng Videos at Stanford@Coursera. <strong>His skills to teach it is something unbelievable awesome</strong>.</p>

<p><em>Thanks for the reading!</em></p>


</html>


